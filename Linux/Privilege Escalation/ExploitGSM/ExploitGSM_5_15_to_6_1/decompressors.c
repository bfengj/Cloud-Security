#include <malloc.h>
#include <stdbool.h>
#include <stdint.h>
#include <memory.h>
#include <linux/byteorder/little_endian.h>
#include <assert.h>

#define likely(x)       (x)
#define unlikely(x)     (x)

// XZ BEGIN
#define XZ_DEC_BCJ
#define XZ_DEC_X86
// xz.h start
#include <stddef.h>
#include <stdint.h>

typedef __u32 u32;
typedef __s32 s32;

typedef __u16 u16;
typedef __s16 s16;

typedef __u8  u8;
typedef __s8  s8;

#ifdef __cplusplus
extern "C" {
#endif

/* In Linux, this is used to make extern functions static when needed. */
#ifndef XZ_EXTERN
#	define XZ_EXTERN extern
#endif

/**
 * enum xz_mode - Operation mode
 *
 * @XZ_SINGLE:              Single-call mode. This uses less RAM than
 *                          than multi-call modes, because the LZMA2
 *                          dictionary doesn't need to be allocated as
 *                          part of the decoder state. All required data
 *                          structures are allocated at initialization,
 *                          so xz_dec_run() cannot return XZ_MEM_ERROR.
 * @XZ_PREALLOC:            Multi-call mode with preallocated LZMA2
 *                          dictionary buffer. All data structures are
 *                          allocated at initialization, so xz_dec_run()
 *                          cannot return XZ_MEM_ERROR.
 * @XZ_DYNALLOC:            Multi-call mode. The LZMA2 dictionary is
 *                          allocated once the required size has been
 *                          parsed from the stream headers. If the
 *                          allocation fails, xz_dec_run() will return
 *                          XZ_MEM_ERROR.
 *
 * It is possible to enable support only for a subset of the above
 * modes at compile time by defining XZ_DEC_SINGLE, XZ_DEC_PREALLOC,
 * or XZ_DEC_DYNALLOC. The xz_dec kernel module is always compiled
 * with support for all operation modes, but the preboot code may
 * be built with fewer features to minimize code size.
 */
enum xz_mode {
    XZ_SINGLE,
    XZ_PREALLOC,
    XZ_DYNALLOC
};

/**
 * enum xz_ret - Return codes
 * @XZ_OK:                  Everything is OK so far. More input or more
 *                          output space is required to continue. This
 *                          return code is possible only in multi-call mode
 *                          (XZ_PREALLOC or XZ_DYNALLOC).
 * @XZ_STREAM_END:          Operation finished successfully.
 * @XZ_UNSUPPORTED_CHECK:   Integrity check type is not supported. Decoding
 *                          is still possible in multi-call mode by simply
 *                          calling xz_dec_run() again.
 *                          Note that this return value is used only if
 *                          XZ_DEC_ANY_CHECK was defined at build time,
 *                          which is not used in the kernel. Unsupported
 *                          check types return XZ_OPTIONS_ERROR if
 *                          XZ_DEC_ANY_CHECK was not defined at build time.
 * @XZ_MEM_ERROR:           Allocating memory failed. This return code is
 *                          possible only if the decoder was initialized
 *                          with XZ_DYNALLOC. The amount of memory that was
 *                          tried to be allocated was no more than the
 *                          dict_max argument given to xz_dec_init().
 * @XZ_MEMLIMIT_ERROR:      A bigger LZMA2 dictionary would be needed than
 *                          allowed by the dict_max argument given to
 *                          xz_dec_init(). This return value is possible
 *                          only in multi-call mode (XZ_PREALLOC or
 *                          XZ_DYNALLOC); the single-call mode (XZ_SINGLE)
 *                          ignores the dict_max argument.
 * @XZ_FORMAT_ERROR:        File format was not recognized (wrong magic
 *                          bytes).
 * @XZ_OPTIONS_ERROR:       This implementation doesn't support the requested
 *                          compression options. In the decoder this means
 *                          that the header CRC32 matches, but the header
 *                          itself specifies something that we don't support.
 * @XZ_DATA_ERROR:          Compressed data is corrupt.
 * @XZ_BUF_ERROR:           Cannot make any progress. Details are slightly
 *                          different between multi-call and single-call
 *                          mode; more information below.
 *
 * In multi-call mode, XZ_BUF_ERROR is returned when two consecutive calls
 * to XZ code cannot consume any input and cannot produce any new output.
 * This happens when there is no new input available, or the output buffer
 * is full while at least one output byte is still pending. Assuming your
 * code is not buggy, you can get this error only when decoding a compressed
 * stream that is truncated or otherwise corrupt.
 *
 * In single-call mode, XZ_BUF_ERROR is returned only when the output buffer
 * is too small or the compressed input is corrupt in a way that makes the
 * decoder produce more output than the caller expected. When it is
 * (relatively) clear that the compressed input is truncated, XZ_DATA_ERROR
 * is used instead of XZ_BUF_ERROR.
 */
enum xz_ret {
    XZ_OK,
    XZ_STREAM_END,
    XZ_UNSUPPORTED_CHECK,
    XZ_MEM_ERROR,
    XZ_MEMLIMIT_ERROR,
    XZ_FORMAT_ERROR,
    XZ_OPTIONS_ERROR,
    XZ_DATA_ERROR,
    XZ_BUF_ERROR
};

/**
 * struct xz_buf - Passing input and output buffers to XZ code
 * @in:         Beginning of the input buffer. This may be NULL if and only
 *              if in_pos is equal to in_size.
 * @in_pos:     Current position in the input buffer. This must not exceed
 *              in_size.
 * @in_size:    Size of the input buffer
 * @out:        Beginning of the output buffer. This may be NULL if and only
 *              if out_pos is equal to out_size.
 * @out_pos:    Current position in the output buffer. This must not exceed
 *              out_size.
 * @out_size:   Size of the output buffer
 *
 * Only the contents of the output buffer from out[out_pos] onward, and
 * the variables in_pos and out_pos are modified by the XZ code.
 */
struct xz_buf {
    const uint8_t *in;
    size_t in_pos;
    size_t in_size;

    uint8_t *out;
    size_t out_pos;
    size_t out_size;
};

/**
 * struct xz_dec - Opaque type to hold the XZ decoder state
 */
struct xz_dec;

/**
 * xz_dec_init() - Allocate and initialize a XZ decoder state
 * @mode:       Operation mode
 * @dict_max:   Maximum size of the LZMA2 dictionary (history buffer) for
 *              multi-call decoding. This is ignored in single-call mode
 *              (mode == XZ_SINGLE). LZMA2 dictionary is always 2^n bytes
 *              or 2^n + 2^(n-1) bytes (the latter sizes are less common
 *              in practice), so other values for dict_max don't make sense.
 *              In the kernel, dictionary sizes of 64 KiB, 128 KiB, 256 KiB,
 *              512 KiB, and 1 MiB are probably the only reasonable values,
 *              except for kernel and initramfs images where a bigger
 *              dictionary can be fine and useful.
 *
 * Single-call mode (XZ_SINGLE): xz_dec_run() decodes the whole stream at
 * once. The caller must provide enough output space or the decoding will
 * fail. The output space is used as the dictionary buffer, which is why
 * there is no need to allocate the dictionary as part of the decoder's
 * internal state.
 *
 * Because the output buffer is used as the workspace, streams encoded using
 * a big dictionary are not a problem in single-call mode. It is enough that
 * the output buffer is big enough to hold the actual uncompressed data; it
 * can be smaller than the dictionary size stored in the stream headers.
 *
 * Multi-call mode with preallocated dictionary (XZ_PREALLOC): dict_max bytes
 * of memory is preallocated for the LZMA2 dictionary. This way there is no
 * risk that xz_dec_run() could run out of memory, since xz_dec_run() will
 * never allocate any memory. Instead, if the preallocated dictionary is too
 * small for decoding the given input stream, xz_dec_run() will return
 * XZ_MEMLIMIT_ERROR. Thus, it is important to know what kind of data will be
 * decoded to avoid allocating excessive amount of memory for the dictionary.
 *
 * Multi-call mode with dynamically allocated dictionary (XZ_DYNALLOC):
 * dict_max specifies the maximum allowed dictionary size that xz_dec_run()
 * may allocate once it has parsed the dictionary size from the stream
 * headers. This way excessive allocations can be avoided while still
 * limiting the maximum memory usage to a sane value to prevent running the
 * system out of memory when decompressing streams from untrusted sources.
 *
 * On success, xz_dec_init() returns a pointer to struct xz_dec, which is
 * ready to be used with xz_dec_run(). If memory allocation fails,
 * xz_dec_init() returns NULL.
 */
XZ_EXTERN struct xz_dec *xz_dec_init(enum xz_mode mode, uint32_t dict_max);

/**
 * xz_dec_run() - Run the XZ decoder
 * @s:          Decoder state allocated using xz_dec_init()
 * @b:          Input and output buffers
 *
 * The possible return values depend on build options and operation mode.
 * See enum xz_ret for details.
 *
 * Note that if an error occurs in single-call mode (return value is not
 * XZ_STREAM_END), b->in_pos and b->out_pos are not modified and the
 * contents of the output buffer from b->out[b->out_pos] onward are
 * undefined. This is true even after XZ_BUF_ERROR, because with some filter
 * chains, there may be a second pass over the output buffer, and this pass
 * cannot be properly done if the output buffer is truncated. Thus, you
 * cannot give the single-call decoder a too small buffer and then expect to
 * get that amount valid data from the beginning of the stream. You must use
 * the multi-call decoder if you don't want to uncompress the whole stream.
 */
XZ_EXTERN enum xz_ret xz_dec_run(struct xz_dec *s, struct xz_buf *b);

/**
 * xz_dec_reset() - Reset an already allocated decoder state
 * @s:          Decoder state allocated using xz_dec_init()
 *
 * This function can be used to reset the multi-call decoder state without
 * freeing and reallocating memory with xz_dec_end() and xz_dec_init().
 *
 * In single-call mode, xz_dec_reset() is always called in the beginning of
 * xz_dec_run(). Thus, explicit call to xz_dec_reset() is useful only in
 * multi-call mode.
 */
XZ_EXTERN void xz_dec_reset(struct xz_dec *s);

/**
 * xz_dec_end() - Free the memory allocated for the decoder state
 * @s:          Decoder state allocated using xz_dec_init(). If s is NULL,
 *              this function does nothing.
 */
XZ_EXTERN void xz_dec_end(struct xz_dec *s);

/*
 * Standalone build (userspace build or in-kernel build for boot time use)
 * needs a CRC32 implementation. For normal in-kernel use, kernel's own
 * CRC32 module is used instead, and users of this module don't need to
 * care about the functions below.
 */
#ifndef XZ_INTERNAL_CRC32
#	ifdef __KERNEL__
#		define XZ_INTERNAL_CRC32 0
#	else
#		define XZ_INTERNAL_CRC32 1
#	endif
#endif

#if XZ_INTERNAL_CRC32
/*
 * This must be called before any other xz_* function to initialize
 * the CRC32 lookup table.
 */
XZ_EXTERN void xz_crc32_init(void);

/*
 * Update CRC32 value using the polynomial from IEEE-802.3. To start a new
 * calculation, the third argument must be zero. To continue the calculation,
 * the previously returned value is passed as the third argument.
 */
XZ_EXTERN uint32_t xz_crc32(const uint8_t *buf, size_t size, uint32_t crc);
#endif

#ifdef __cplusplus
}
#endif
// xz.h end

// xz_config.h begin
#include <stdbool.h>
#include <stdlib.h>
#include <string.h>

#define kmalloc(size, flags) malloc(size)
#define kfree(ptr) free(ptr)
#define vmalloc(size) malloc(size)
#define vfree(ptr) free(ptr)

#define memeq(a, b, size) (memcmp(a, b, size) == 0)
#define memzero(buf, size) memset(buf, 0, size)

#ifndef min
#	define min(x, y) ((x) < (y) ? (x) : (y))
#endif
#define min_t(type, x, y) min(x, y)

/*
 * Some functions have been marked with __always_inline to keep the
 * performance reasonable even when the compiler is optimizing for
 * small code size. You may be able to save a few bytes by #defining
 * __always_inline to plain inline, but don't complain if the code
 * becomes slow.
 *
 * NOTE: System headers on GNU/Linux may #define this macro already,
 * so if you want to change it, you need to #undef it first.
 */
#ifndef __always_inline
#	ifdef __GNUC__
#		define __always_inline \
            inline __attribute__((__always_inline__))
#	else
#		define __always_inline inline
#	endif
#endif

/* Inline functions to access unaligned unsigned 32-bit integers */
#ifndef get_unaligned_le32
static inline uint32_t get_unaligned_le32(const uint8_t *buf)
{
    return (uint32_t)buf[0]
            | ((uint32_t)buf[1] << 8)
            | ((uint32_t)buf[2] << 16)
            | ((uint32_t)buf[3] << 24);
}
#endif

#ifndef get_unaligned_be32
static inline uint32_t get_unaligned_be32(const uint8_t *buf)
{
    return (uint32_t)(buf[0] << 24)
            | ((uint32_t)buf[1] << 16)
            | ((uint32_t)buf[2] << 8)
            | (uint32_t)buf[3];
}
#endif

#ifndef put_unaligned_le32
static inline void put_unaligned_le32(uint32_t val, uint8_t *buf)
{
    buf[0] = (uint8_t)val;
    buf[1] = (uint8_t)(val >> 8);
    buf[2] = (uint8_t)(val >> 16);
    buf[3] = (uint8_t)(val >> 24);
}
#endif

#ifndef put_unaligned_be32
static inline void put_unaligned_be32(uint32_t val, uint8_t *buf)
{
    buf[0] = (uint8_t)(val >> 24);
    buf[1] = (uint8_t)(val >> 16);
    buf[2] = (uint8_t)(val >> 8);
    buf[3] = (uint8_t)val;
}
#endif

//xz_config.h
/*
 * Use get_unaligned_le32() also for aligned access for simplicity. On
 * little endian systems, #define get_le32(ptr) (*(const uint32_t *)(ptr))
 * could save a few bytes in code size.
 */
#ifndef get_le32
#	define get_le32 get_unaligned_le32
#endif

// xz_stream.h start
#if defined(__KERNEL__) && !XZ_INTERNAL_CRC32
#	include <linux/crc32.h>
#	undef crc32
#	define xz_crc32(buf, size, crc) \
        (~crc32_le(~(uint32_t)(crc), buf, size))
#endif

/*
 * See the .xz file format specification at
 * http://tukaani.org/xz/xz-file-format.txt
 * to understand the container format.
 */

#define STREAM_HEADER_SIZE 12

#define HEADER_MAGIC "\3757zXZ"
#define HEADER_MAGIC_SIZE 6

#define FOOTER_MAGIC "YZ"
#define FOOTER_MAGIC_SIZE 2

/*
 * Variable-length integer can hold a 63-bit unsigned integer or a special
 * value indicating that the value is unknown.
 *
 * Experimental: vli_type can be defined to uint32_t to save a few bytes
 * in code size (no effect on speed). Doing so limits the uncompressed and
 * compressed size of the file to less than 256 MiB and may also weaken
 * error detection slightly.
 */
typedef uint64_t vli_type;

#define VLI_MAX ((vli_type)-1 / 2)
#define VLI_UNKNOWN ((vli_type)-1)

/* Maximum encoded size of a VLI */
#define VLI_BYTES_MAX (sizeof(vli_type) * 8 / 7)

/* Integrity Check types */
enum xz_check {
    XZ_CHECK_NONE = 0,
    XZ_CHECK_CRC32 = 1,
    XZ_CHECK_CRC64 = 4,
    XZ_CHECK_SHA256 = 10
};

/* Maximum possible Check ID */
#define XZ_CHECK_MAX 15

// xz_stream.h end

//xz_private.h start
   /*
     * For userspace builds, use a separate header to define the required
     * macros and functions. This makes it easier to adapt the code into
     * different environments and avoids clutter in the Linux kernel tree.
     */
//#	include "xz_config.h"

/* If no specific decoding mode is requested, enable support for all modes. */
#if !defined(XZ_DEC_SINGLE) && !defined(XZ_DEC_PREALLOC) \
        && !defined(XZ_DEC_DYNALLOC)
#	define XZ_DEC_SINGLE
#	define XZ_DEC_PREALLOC
#	define XZ_DEC_DYNALLOC
#endif

/*
 * The DEC_IS_foo(mode) macros are used in "if" statements. If only some
 * of the supported modes are enabled, these macros will evaluate to true or
 * false at compile time and thus allow the compiler to omit unneeded code.
 */
#ifdef XZ_DEC_SINGLE
#	define DEC_IS_SINGLE(mode) ((mode) == XZ_SINGLE)
#else
#	define DEC_IS_SINGLE(mode) (false)
#endif

#ifdef XZ_DEC_PREALLOC
#	define DEC_IS_PREALLOC(mode) ((mode) == XZ_PREALLOC)
#else
#	define DEC_IS_PREALLOC(mode) (false)
#endif

#ifdef XZ_DEC_DYNALLOC
#	define DEC_IS_DYNALLOC(mode) ((mode) == XZ_DYNALLOC)
#else
#	define DEC_IS_DYNALLOC(mode) (false)
#endif

#if !defined(XZ_DEC_SINGLE)
#	define DEC_IS_MULTI(mode) (true)
#elif defined(XZ_DEC_PREALLOC) || defined(XZ_DEC_DYNALLOC)
#	define DEC_IS_MULTI(mode) ((mode) != XZ_SINGLE)
#else
#	define DEC_IS_MULTI(mode) (false)
#endif

/*
 * If any of the BCJ filter decoders are wanted, define XZ_DEC_BCJ.
 * XZ_DEC_BCJ is used to enable generic support for BCJ decoders.
 */
#ifndef XZ_DEC_BCJ
#	if defined(XZ_DEC_X86) || defined(XZ_DEC_POWERPC) \
            || defined(XZ_DEC_IA64) || defined(XZ_DEC_ARM) \
            || defined(XZ_DEC_ARM) || defined(XZ_DEC_ARMTHUMB) \
            || defined(XZ_DEC_SPARC)
#		define XZ_DEC_BCJ
#	endif
#endif

/*
 * Allocate memory for LZMA2 decoder. xz_dec_lzma2_reset() must be used
 * before calling xz_dec_lzma2_run().
 */
XZ_EXTERN struct xz_dec_lzma2 *xz_dec_lzma2_create(enum xz_mode mode,
                           uint32_t dict_max);

/*
 * Decode the LZMA2 properties (one byte) and reset the decoder. Return
 * XZ_OK on success, XZ_MEMLIMIT_ERROR if the preallocated dictionary is not
 * big enough, and XZ_OPTIONS_ERROR if props indicates something that this
 * decoder doesn't support.
 */
XZ_EXTERN enum xz_ret xz_dec_lzma2_reset(struct xz_dec_lzma2 *s,
                     uint8_t props);

/* Decode raw LZMA2 stream from b->in to b->out. */
XZ_EXTERN enum xz_ret xz_dec_lzma2_run(struct xz_dec_lzma2 *s,
                       struct xz_buf *b);

/* Free the memory allocated for the LZMA2 decoder. */
XZ_EXTERN void xz_dec_lzma2_end(struct xz_dec_lzma2 *s);

#ifdef XZ_DEC_BCJ
/*
 * Allocate memory for BCJ decoders. xz_dec_bcj_reset() must be used before
 * calling xz_dec_bcj_run().
 */
XZ_EXTERN struct xz_dec_bcj *xz_dec_bcj_create(bool single_call);

/*
 * Decode the Filter ID of a BCJ filter. This implementation doesn't
 * support custom start offsets, so no decoding of Filter Properties
 * is needed. Returns XZ_OK if the given Filter ID is supported.
 * Otherwise XZ_OPTIONS_ERROR is returned.
 */
XZ_EXTERN enum xz_ret xz_dec_bcj_reset(struct xz_dec_bcj *s, uint8_t id);

/*
 * Decode raw BCJ + LZMA2 stream. This must be used only if there actually is
 * a BCJ filter in the chain. If the chain has only LZMA2, xz_dec_lzma2_run()
 * must be called directly.
 */
XZ_EXTERN enum xz_ret xz_dec_bcj_run(struct xz_dec_bcj *s,
                     struct xz_dec_lzma2 *lzma2,
                     struct xz_buf *b);

/* Free the memory allocated for the BCJ filters. */
#define xz_dec_bcj_end(s) kfree(s)
#endif

//xz_private.h end

//xz_lzma2.h start

/* Range coder constants */
#define RC_SHIFT_BITS 8
#define RC_TOP_BITS 24
#define RC_TOP_VALUE (1 << RC_TOP_BITS)
#define RC_BIT_MODEL_TOTAL_BITS 11
#define RC_BIT_MODEL_TOTAL (1 << RC_BIT_MODEL_TOTAL_BITS)
#define RC_MOVE_BITS 5

/*
 * Maximum number of position states. A position state is the lowest pb
 * number of bits of the current uncompressed offset. In some places there
 * are different sets of probabilities for different position states.
 */
#define POS_STATES_MAX (1 << 4)

/*
 * This enum is used to track which LZMA symbols have occurred most recently
 * and in which order. This information is used to predict the next symbol.
 *
 * Symbols:
 *  - Literal: One 8-bit byte
 *  - Match: Repeat a chunk of data at some distance
 *  - Long repeat: Multi-byte match at a recently seen distance
 *  - Short repeat: One-byte repeat at a recently seen distance
 *
 * The symbol names are in from STATE_oldest_older_previous. REP means
 * either short or long repeated match, and NONLIT means any non-literal.
 */
enum lzma_state {
    STATE_LIT_LIT,
    STATE_MATCH_LIT_LIT,
    STATE_REP_LIT_LIT,
    STATE_SHORTREP_LIT_LIT,
    STATE_MATCH_LIT,
    STATE_REP_LIT,
    STATE_SHORTREP_LIT,
    STATE_LIT_MATCH,
    STATE_LIT_LONGREP,
    STATE_LIT_SHORTREP,
    STATE_NONLIT_MATCH,
    STATE_NONLIT_REP
};

/* Total number of states */
#define STATES 12

/* The lowest 7 states indicate that the previous state was a literal. */
#define LIT_STATES 7

/* Indicate that the latest symbol was a literal. */
static inline void lzma_state_literal(enum lzma_state *state)
{
    if (*state <= STATE_SHORTREP_LIT_LIT)
        *state = STATE_LIT_LIT;
    else if (*state <= STATE_LIT_SHORTREP)
        *state -= 3;
    else
        *state -= 6;
}

/* Indicate that the latest symbol was a match. */
static inline void lzma_state_match(enum lzma_state *state)
{
    *state = *state < LIT_STATES ? STATE_LIT_MATCH : STATE_NONLIT_MATCH;
}

/* Indicate that the latest state was a long repeated match. */
static inline void lzma_state_long_rep(enum lzma_state *state)
{
    *state = *state < LIT_STATES ? STATE_LIT_LONGREP : STATE_NONLIT_REP;
}

/* Indicate that the latest symbol was a short match. */
static inline void lzma_state_short_rep(enum lzma_state *state)
{
    *state = *state < LIT_STATES ? STATE_LIT_SHORTREP : STATE_NONLIT_REP;
}

/* Test if the previous symbol was a literal. */
static inline bool lzma_state_is_literal(enum lzma_state state)
{
    return state < LIT_STATES;
}

/* Each literal coder is divided in three sections:
 *   - 0x001-0x0FF: Without match byte
 *   - 0x101-0x1FF: With match byte; match bit is 0
 *   - 0x201-0x2FF: With match byte; match bit is 1
 *
 * Match byte is used when the previous LZMA symbol was something else than
 * a literal (that is, it was some kind of match).
 */
#define LITERAL_CODER_SIZE 0x300

/* Maximum number of literal coders */
#define LITERAL_CODERS_MAX (1 << 4)

/* Minimum length of a match is two bytes. */
#define MATCH_LEN_MIN 2

/* Match length is encoded with 4, 5, or 10 bits.
 *
 * Length   Bits
 *  2-9      4 = Choice=0 + 3 bits
 * 10-17     5 = Choice=1 + Choice2=0 + 3 bits
 * 18-273   10 = Choice=1 + Choice2=1 + 8 bits
 */
#define LEN_LOW_BITS 3
#define LEN_LOW_SYMBOLS (1 << LEN_LOW_BITS)
#define LEN_MID_BITS 3
#define LEN_MID_SYMBOLS (1 << LEN_MID_BITS)
#define LEN_HIGH_BITS 8
#define LEN_HIGH_SYMBOLS (1 << LEN_HIGH_BITS)
#define LEN_SYMBOLS (LEN_LOW_SYMBOLS + LEN_MID_SYMBOLS + LEN_HIGH_SYMBOLS)

/*
 * Maximum length of a match is 273 which is a result of the encoding
 * described above.
 */
#define MATCH_LEN_MAX (MATCH_LEN_MIN + LEN_SYMBOLS - 1)

/*
 * Different sets of probabilities are used for match distances that have
 * very short match length: Lengths of 2, 3, and 4 bytes have a separate
 * set of probabilities for each length. The matches with longer length
 * use a shared set of probabilities.
 */
#define DIST_STATES 4

/*
 * Get the index of the appropriate probability array for decoding
 * the distance slot.
 */
static inline uint32_t lzma_get_dist_state(uint32_t len)
{
    return len < DIST_STATES + MATCH_LEN_MIN
            ? len - MATCH_LEN_MIN : DIST_STATES - 1;
}

/*
 * The highest two bits of a 32-bit match distance are encoded using six bits.
 * This six-bit value is called a distance slot. This way encoding a 32-bit
 * value takes 6-36 bits, larger values taking more bits.
 */
#define DIST_SLOT_BITS 6
#define DIST_SLOTS (1 << DIST_SLOT_BITS)

/* Match distances up to 127 are fully encoded using probabilities. Since
 * the highest two bits (distance slot) are always encoded using six bits,
 * the distances 0-3 don't need any additional bits to encode, since the
 * distance slot itself is the same as the actual distance. DIST_MODEL_START
 * indicates the first distance slot where at least one additional bit is
 * needed.
 */
#define DIST_MODEL_START 4

/*
 * Match distances greater than 127 are encoded in three pieces:
 *   - distance slot: the highest two bits
 *   - direct bits: 2-26 bits below the highest two bits
 *   - alignment bits: four lowest bits
 *
 * Direct bits don't use any probabilities.
 *
 * The distance slot value of 14 is for distances 128-191.
 */
#define DIST_MODEL_END 14

/* Distance slots that indicate a distance <= 127. */
#define FULL_DISTANCES_BITS (DIST_MODEL_END / 2)
#define FULL_DISTANCES (1 << FULL_DISTANCES_BITS)

/*
 * For match distances greater than 127, only the highest two bits and the
 * lowest four bits (alignment) is encoded using probabilities.
 */
#define ALIGN_BITS 4
#define ALIGN_SIZE (1 << ALIGN_BITS)
#define ALIGN_MASK (ALIGN_SIZE - 1)

/* Total number of all probability variables */
#define PROBS_TOTAL (1846 + LITERAL_CODERS_MAX * LITERAL_CODER_SIZE)

/*
 * LZMA remembers the four most recent match distances. Reusing these
 * distances tends to take less space than re-encoding the actual
 * distance value.
 */
#define REPS 4
//xz_lzma.h end

//xz_crc32.c start
#ifndef STATIC_RW_DATA
#	define STATIC_RW_DATA static
#endif

STATIC_RW_DATA uint32_t xz_crc32_table[256];

XZ_EXTERN void xz_crc32_init(void)
{
    const uint32_t poly = 0xEDB88320;

    uint32_t i;
    uint32_t j;
    uint32_t r;

    for (i = 0; i < 256; ++i) {
        r = i;
        for (j = 0; j < 8; ++j)
            r = (r >> 1) ^ (poly & ~((r & 1) - 1));

        xz_crc32_table[i] = r;
    }

    return;
}

XZ_EXTERN uint32_t xz_crc32(const uint8_t *buf, size_t size, uint32_t crc)
{
    crc = ~crc;

    while (size != 0) {
        crc = xz_crc32_table[*buf++ ^ (crc & 0xFF)] ^ (crc >> 8);
        --size;
    }

    return ~crc;
}
//xz_crc32.c end

//xz_dec_bcj.c start
#ifdef XZ_DEC_BCJ

struct xz_dec_bcj {
    /* Type of the BCJ filter being used */
    enum {
        BCJ_X86 = 4,        /* x86 or x86-64 */
        BCJ_POWERPC = 5,    /* Big endian only */
        BCJ_IA64 = 6,       /* Big or little endian */
        BCJ_ARM = 7,        /* Little endian only */
        BCJ_ARMTHUMB = 8,   /* Little endian only */
        BCJ_SPARC = 9       /* Big or little endian */
    } type;

    /*
     * Return value of the next filter in the chain. We need to preserve
     * this information across calls, because we must not call the next
     * filter anymore once it has returned XZ_STREAM_END.
     */
    enum xz_ret ret;

    /* True if we are operating in single-call mode. */
    bool single_call;

    /*
     * Absolute position relative to the beginning of the uncompressed
     * data (in a single .xz Block). We care only about the lowest 32
     * bits so this doesn't need to be uint64_t even with big files.
     */
    uint32_t pos;

    /* x86 filter state */
    uint32_t x86_prev_mask;

    /* Temporary space to hold the variables from struct xz_buf */
    uint8_t *out;
    size_t out_pos;
    size_t out_size;

    struct {
        /* Amount of already filtered data in the beginning of buf */
        size_t filtered;

        /* Total amount of data currently stored in buf  */
        size_t size;

        /*
         * Buffer to hold a mix of filtered and unfiltered data. This
         * needs to be big enough to hold Alignment + 2 * Look-ahead:
         *
         * Type         Alignment   Look-ahead
         * x86              1           4
         * PowerPC          4           0
         * IA-64           16           0
         * ARM              4           0
         * ARM-Thumb        2           2
         * SPARC            4           0
         */
        uint8_t buf[16];
    } temp;
};

#ifdef XZ_DEC_X86
/*
 * This is used to test the most significant byte of a memory address
 * in an x86 instruction.
 */
static inline int bcj_x86_test_msbyte(uint8_t b)
{
    return b == 0x00 || b == 0xFF;
}

static size_t bcj_x86(struct xz_dec_bcj *s, uint8_t *buf, size_t size)
{
    static const bool mask_to_allowed_status[8]
        = { true, true, true, false, true, false, false, false };

    static const uint8_t mask_to_bit_num[8] = { 0, 1, 2, 2, 3, 3, 3, 3 };

    size_t i;
    size_t prev_pos = (size_t)-1;
    uint32_t prev_mask = s->x86_prev_mask;
    uint32_t src;
    uint32_t dest;
    uint32_t j;
    uint8_t b;

    if (size <= 4)
        return 0;

    size -= 4;
    for (i = 0; i < size; ++i) {
        if ((buf[i] & 0xFE) != 0xE8)
            continue;

        prev_pos = i - prev_pos;
        if (prev_pos > 3) {
            prev_mask = 0;
        } else {
            prev_mask = (prev_mask << (prev_pos - 1)) & 7;
            if (prev_mask != 0) {
                b = buf[i + 4 - mask_to_bit_num[prev_mask]];
                if (!mask_to_allowed_status[prev_mask]
                        || bcj_x86_test_msbyte(b)) {
                    prev_pos = i;
                    prev_mask = (prev_mask << 1) | 1;
                    continue;
                }
            }
        }

        prev_pos = i;

        if (bcj_x86_test_msbyte(buf[i + 4])) {
            src = get_unaligned_le32(buf + i + 1);
            while (true) {
                dest = src - (s->pos + (uint32_t)i + 5);
                if (prev_mask == 0)
                    break;

                j = mask_to_bit_num[prev_mask] * 8;
                b = (uint8_t)(dest >> (24 - j));
                if (!bcj_x86_test_msbyte(b))
                    break;

                src = dest ^ (((uint32_t)1 << (32 - j)) - 1);
            }

            dest &= 0x01FFFFFF;
            dest |= (uint32_t)0 - (dest & 0x01000000);
            put_unaligned_le32(dest, buf + i + 1);
            i += 4;
        } else {
            prev_mask = (prev_mask << 1) | 1;
        }
    }

    prev_pos = i - prev_pos;
    s->x86_prev_mask = prev_pos > 3 ? 0 : prev_mask << (prev_pos - 1);
    return i;
}
#endif

#ifdef XZ_DEC_POWERPC
static size_t bcj_powerpc(struct xz_dec_bcj *s, uint8_t *buf, size_t size)
{
    size_t i;
    uint32_t instr;

    for (i = 0; i + 4 <= size; i += 4) {
        instr = get_unaligned_be32(buf + i);
        if ((instr & 0xFC000003) == 0x48000001) {
            instr &= 0x03FFFFFC;
            instr -= s->pos + (uint32_t)i;
            instr &= 0x03FFFFFC;
            instr |= 0x48000001;
            put_unaligned_be32(instr, buf + i);
        }
    }

    return i;
}
#endif

#ifdef XZ_DEC_IA64
static size_t bcj_ia64(struct xz_dec_bcj *s, uint8_t *buf, size_t size)
{
    static const uint8_t branch_table[32] = {
        0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0,
        4, 4, 6, 6, 0, 0, 7, 7,
        4, 4, 0, 0, 4, 4, 0, 0
    };

    /*
     * The local variables take a little bit stack space, but it's less
     * than what LZMA2 decoder takes, so it doesn't make sense to reduce
     * stack usage here without doing that for the LZMA2 decoder too.
     */

    /* Loop counters */
    size_t i;
    size_t j;

    /* Instruction slot (0, 1, or 2) in the 128-bit instruction word */
    uint32_t slot;

    /* Bitwise offset of the instruction indicated by slot */
    uint32_t bit_pos;

    /* bit_pos split into byte and bit parts */
    uint32_t byte_pos;
    uint32_t bit_res;

    /* Address part of an instruction */
    uint32_t addr;

    /* Mask used to detect which instructions to convert */
    uint32_t mask;

    /* 41-bit instruction stored somewhere in the lowest 48 bits */
    uint64_t instr;

    /* Instruction normalized with bit_res for easier manipulation */
    uint64_t norm;

    for (i = 0; i + 16 <= size; i += 16) {
        mask = branch_table[buf[i] & 0x1F];
        for (slot = 0, bit_pos = 5; slot < 3; ++slot, bit_pos += 41) {
            if (((mask >> slot) & 1) == 0)
                continue;

            byte_pos = bit_pos >> 3;
            bit_res = bit_pos & 7;
            instr = 0;
            for (j = 0; j < 6; ++j)
                instr |= (uint64_t)(buf[i + j + byte_pos])
                        << (8 * j);

            norm = instr >> bit_res;

            if (((norm >> 37) & 0x0F) == 0x05
                    && ((norm >> 9) & 0x07) == 0) {
                addr = (norm >> 13) & 0x0FFFFF;
                addr |= ((uint32_t)(norm >> 36) & 1) << 20;
                addr <<= 4;
                addr -= s->pos + (uint32_t)i;
                addr >>= 4;

                norm &= ~((uint64_t)0x8FFFFF << 13);
                norm |= (uint64_t)(addr & 0x0FFFFF) << 13;
                norm |= (uint64_t)(addr & 0x100000)
                        << (36 - 20);

                instr &= (1 << bit_res) - 1;
                instr |= norm << bit_res;

                for (j = 0; j < 6; j++)
                    buf[i + j + byte_pos]
                        = (uint8_t)(instr >> (8 * j));
            }
        }
    }

    return i;
}
#endif

#ifdef XZ_DEC_ARM
static size_t bcj_arm(struct xz_dec_bcj *s, uint8_t *buf, size_t size)
{
    size_t i;
    uint32_t addr;

    for (i = 0; i + 4 <= size; i += 4) {
        if (buf[i + 3] == 0xEB) {
            addr = (uint32_t)buf[i] | ((uint32_t)buf[i + 1] << 8)
                    | ((uint32_t)buf[i + 2] << 16);
            addr <<= 2;
            addr -= s->pos + (uint32_t)i + 8;
            addr >>= 2;
            buf[i] = (uint8_t)addr;
            buf[i + 1] = (uint8_t)(addr >> 8);
            buf[i + 2] = (uint8_t)(addr >> 16);
        }
    }

    return i;
}
#endif

#ifdef XZ_DEC_ARMTHUMB
static size_t bcj_armthumb(struct xz_dec_bcj *s, uint8_t *buf, size_t size)
{
    size_t i;
    uint32_t addr;

    for (i = 0; i + 4 <= size; i += 2) {
        if ((buf[i + 1] & 0xF8) == 0xF0
                && (buf[i + 3] & 0xF8) == 0xF8) {
            addr = (((uint32_t)buf[i + 1] & 0x07) << 19)
                    | ((uint32_t)buf[i] << 11)
                    | (((uint32_t)buf[i + 3] & 0x07) << 8)
                    | (uint32_t)buf[i + 2];
            addr <<= 1;
            addr -= s->pos + (uint32_t)i + 4;
            addr >>= 1;
            buf[i + 1] = (uint8_t)(0xF0 | ((addr >> 19) & 0x07));
            buf[i] = (uint8_t)(addr >> 11);
            buf[i + 3] = (uint8_t)(0xF8 | ((addr >> 8) & 0x07));
            buf[i + 2] = (uint8_t)addr;
            i += 2;
        }
    }

    return i;
}
#endif

#ifdef XZ_DEC_SPARC
static size_t bcj_sparc(struct xz_dec_bcj *s, uint8_t *buf, size_t size)
{
    size_t i;
    uint32_t instr;

    for (i = 0; i + 4 <= size; i += 4) {
        instr = get_unaligned_be32(buf + i);
        if ((instr >> 22) == 0x100 || (instr >> 22) == 0x1FF) {
            instr <<= 2;
            instr -= s->pos + (uint32_t)i;
            instr >>= 2;
            instr = ((uint32_t)0x40000000 - (instr & 0x400000))
                    | 0x40000000 | (instr & 0x3FFFFF);
            put_unaligned_be32(instr, buf + i);
        }
    }

    return i;
}
#endif

/*
 * Apply the selected BCJ filter. Update *pos and s->pos to match the amount
 * of data that got filtered.
 *
 * NOTE: This is implemented as a switch statement to avoid using function
 * pointers, which could be problematic in the kernel boot code, which must
 * avoid pointers to static data (at least on x86).
 */
static void bcj_apply(struct xz_dec_bcj *s,
              uint8_t *buf, size_t *pos, size_t size)
{
    size_t filtered;

    buf += *pos;
    size -= *pos;

    switch (s->type) {
#ifdef XZ_DEC_X86
    case BCJ_X86:
        filtered = bcj_x86(s, buf, size);
        break;
#endif
#ifdef XZ_DEC_POWERPC
    case BCJ_POWERPC:
        filtered = bcj_powerpc(s, buf, size);
        break;
#endif
#ifdef XZ_DEC_IA64
    case BCJ_IA64:
        filtered = bcj_ia64(s, buf, size);
        break;
#endif
#ifdef XZ_DEC_ARM
    case BCJ_ARM:
        filtered = bcj_arm(s, buf, size);
        break;
#endif
#ifdef XZ_DEC_ARMTHUMB
    case BCJ_ARMTHUMB:
        filtered = bcj_armthumb(s, buf, size);
        break;
#endif
#ifdef XZ_DEC_SPARC
    case BCJ_SPARC:
        filtered = bcj_sparc(s, buf, size);
        break;
#endif
    default:
        /* Never reached but silence compiler warnings. */
        filtered = 0;
        break;
    }

    *pos += filtered;
    s->pos += filtered;
}

/*
 * Flush pending filtered data from temp to the output buffer.
 * Move the remaining mixture of possibly filtered and unfiltered
 * data to the beginning of temp.
 */
static void bcj_flush(struct xz_dec_bcj *s, struct xz_buf *b)
{
    size_t copy_size;

    copy_size = min_t(size_t, s->temp.filtered, b->out_size - b->out_pos);
    memcpy(b->out + b->out_pos, s->temp.buf, copy_size);
    b->out_pos += copy_size;

    s->temp.filtered -= copy_size;
    s->temp.size -= copy_size;
    memmove(s->temp.buf, s->temp.buf + copy_size, s->temp.size);
}

/*
 * The BCJ filter functions are primitive in sense that they process the
 * data in chunks of 1-16 bytes. To hide this issue, this function does
 * some buffering.
 */
XZ_EXTERN enum xz_ret xz_dec_bcj_run(struct xz_dec_bcj *s,
                     struct xz_dec_lzma2 *lzma2,
                     struct xz_buf *b)
{
    size_t out_start;

    /*
     * Flush pending already filtered data to the output buffer. Return
     * immediatelly if we couldn't flush everything, or if the next
     * filter in the chain had already returned XZ_STREAM_END.
     */
    if (s->temp.filtered > 0) {
        bcj_flush(s, b);
        if (s->temp.filtered > 0)
            return XZ_OK;

        if (s->ret == XZ_STREAM_END)
            return XZ_STREAM_END;
    }

    /*
     * If we have more output space than what is currently pending in
     * temp, copy the unfiltered data from temp to the output buffer
     * and try to fill the output buffer by decoding more data from the
     * next filter in the chain. Apply the BCJ filter on the new data
     * in the output buffer. If everything cannot be filtered, copy it
     * to temp and rewind the output buffer position accordingly.
     */
    if (s->temp.size < b->out_size - b->out_pos) {
        out_start = b->out_pos;
        memcpy(b->out + b->out_pos, s->temp.buf, s->temp.size);
        b->out_pos += s->temp.size;

        s->ret = xz_dec_lzma2_run(lzma2, b);
        if (s->ret != XZ_STREAM_END
                && (s->ret != XZ_OK || s->single_call))
            return s->ret;

        bcj_apply(s, b->out, &out_start, b->out_pos);

        /*
         * As an exception, if the next filter returned XZ_STREAM_END,
         * we can do that too, since the last few bytes that remain
         * unfiltered are meant to remain unfiltered.
         */
        if (s->ret == XZ_STREAM_END)
            return XZ_STREAM_END;

        s->temp.size = b->out_pos - out_start;
        b->out_pos -= s->temp.size;
        memcpy(s->temp.buf, b->out + b->out_pos, s->temp.size);
    }

    /*
     * If we have unfiltered data in temp, try to fill by decoding more
     * data from the next filter. Apply the BCJ filter on temp. Then we
     * hopefully can fill the actual output buffer by copying filtered
     * data from temp. A mix of filtered and unfiltered data may be left
     * in temp; it will be taken care on the next call to this function.
     */
    if (s->temp.size > 0) {
        /* Make b->out{,_pos,_size} temporarily point to s->temp. */
        s->out = b->out;
        s->out_pos = b->out_pos;
        s->out_size = b->out_size;
        b->out = s->temp.buf;
        b->out_pos = s->temp.size;
        b->out_size = sizeof(s->temp.buf);

        s->ret = xz_dec_lzma2_run(lzma2, b);

        s->temp.size = b->out_pos;
        b->out = s->out;
        b->out_pos = s->out_pos;
        b->out_size = s->out_size;

        if (s->ret != XZ_OK && s->ret != XZ_STREAM_END)
            return s->ret;

        bcj_apply(s, s->temp.buf, &s->temp.filtered, s->temp.size);

        /*
         * If the next filter returned XZ_STREAM_END, we mark that
         * everything is filtered, since the last unfiltered bytes
         * of the stream are meant to be left as is.
         */
        if (s->ret == XZ_STREAM_END)
            s->temp.filtered = s->temp.size;

        bcj_flush(s, b);
        if (s->temp.filtered > 0)
            return XZ_OK;
    }

    return s->ret;
}

XZ_EXTERN struct xz_dec_bcj *xz_dec_bcj_create(bool single_call)
{
    struct xz_dec_bcj *s = kmalloc(sizeof(*s), GFP_KERNEL);
    if (s != NULL)
        s->single_call = single_call;

    return s;
}

XZ_EXTERN enum xz_ret xz_dec_bcj_reset(struct xz_dec_bcj *s, uint8_t id)
{
    switch (id) {
#ifdef XZ_DEC_X86
    case BCJ_X86:
#endif
#ifdef XZ_DEC_POWERPC
    case BCJ_POWERPC:
#endif
#ifdef XZ_DEC_IA64
    case BCJ_IA64:
#endif
#ifdef XZ_DEC_ARM
    case BCJ_ARM:
#endif
#ifdef XZ_DEC_ARMTHUMB
    case BCJ_ARMTHUMB:
#endif
#ifdef XZ_DEC_SPARC
    case BCJ_SPARC:
#endif
        break;

    default:
        /* Unsupported Filter ID */
        return XZ_OPTIONS_ERROR;
    }

    s->type = id;
    s->ret = XZ_OK;
    s->pos = 0;
    s->x86_prev_mask = 0;
    s->temp.filtered = 0;
    s->temp.size = 0;

    return XZ_OK;
}

#endif

//xz_dec_bcj.c end

//xz_dec_lzma2.c begin
#define RC_INIT_BYTES 5

/*
 * Minimum number of usable input buffer to safely decode one LZMA symbol.
 * The worst case is that we decode 22 bits using probabilities and 26
 * direct bits. This may decode at maximum of 20 bytes of input. However,
 * lzma_main() does an extra normalization before returning, thus we
 * need to put 21 here.
 */
#define LZMA_IN_REQUIRED 21

/*
 * Dictionary (history buffer)
 *
 * These are always true:
 *    start <= pos <= full <= end
 *    pos <= limit <= end
 *
 * In multi-call mode, also these are true:
 *    end == size
 *    size <= size_max
 *    allocated <= size
 *
 * Most of these variables are size_t to support single-call mode,
 * in which the dictionary variables address the actual output
 * buffer directly.
 */
struct dictionary {
    /* Beginning of the history buffer */
    uint8_t *buf;

    /* Old position in buf (before decoding more data) */
    size_t start;

    /* Position in buf */
    size_t pos;

    /*
     * How full dictionary is. This is used to detect corrupt input that
     * would read beyond the beginning of the uncompressed stream.
     */
    size_t full;

    /* Write limit; we don't write to buf[limit] or later bytes. */
    size_t limit;

    /*
     * End of the dictionary buffer. In multi-call mode, this is
     * the same as the dictionary size. In single-call mode, this
     * indicates the size of the output buffer.
     */
    size_t end;

    /*
     * Size of the dictionary as specified in Block Header. This is used
     * together with "full" to detect corrupt input that would make us
     * read beyond the beginning of the uncompressed stream.
     */
    uint32_t size;

    /*
     * Maximum allowed dictionary size in multi-call mode.
     * This is ignored in single-call mode.
     */
    uint32_t size_max;

    /*
     * Amount of memory currently allocated for the dictionary.
     * This is used only with XZ_DYNALLOC. (With XZ_PREALLOC,
     * size_max is always the same as the allocated size.)
     */
    uint32_t allocated;

    /* Operation mode */
    enum xz_mode mode;
};

/* Range decoder */
struct rc_dec {
    uint32_t range;
    uint32_t code;

    /*
     * Number of initializing bytes remaining to be read
     * by rc_read_init().
     */
    uint32_t init_bytes_left;

    /*
     * Buffer from which we read our input. It can be either
     * temp.buf or the caller-provided input buffer.
     */
    const uint8_t *in;
    size_t in_pos;
    size_t in_limit;
};

/* Probabilities for a length decoder. */
struct lzma_len_dec {
    /* Probability of match length being at least 10 */
    uint16_t choice;

    /* Probability of match length being at least 18 */
    uint16_t choice2;

    /* Probabilities for match lengths 2-9 */
    uint16_t low[POS_STATES_MAX][LEN_LOW_SYMBOLS];

    /* Probabilities for match lengths 10-17 */
    uint16_t mid[POS_STATES_MAX][LEN_MID_SYMBOLS];

    /* Probabilities for match lengths 18-273 */
    uint16_t high[LEN_HIGH_SYMBOLS];
};

struct lzma_dec {
    /* Distances of latest four matches */
    uint32_t rep0;
    uint32_t rep1;
    uint32_t rep2;
    uint32_t rep3;

    /* Types of the most recently seen LZMA symbols */
    enum lzma_state state;

    /*
     * Length of a match. This is updated so that dict_repeat can
     * be called again to finish repeating the whole match.
     */
    uint32_t len;

    /*
     * LZMA properties or related bit masks (number of literal
     * context bits, a mask dervied from the number of literal
     * position bits, and a mask dervied from the number
     * position bits)
     */
    uint32_t lc;
    uint32_t literal_pos_mask; /* (1 << lp) - 1 */
    uint32_t pos_mask;         /* (1 << pb) - 1 */

    /* If 1, it's a match. Otherwise it's a single 8-bit literal. */
    uint16_t is_match[STATES][POS_STATES_MAX];

    /* If 1, it's a repeated match. The distance is one of rep0 .. rep3. */
    uint16_t is_rep[STATES];

    /*
     * If 0, distance of a repeated match is rep0.
     * Otherwise check is_rep1.
     */
    uint16_t is_rep0[STATES];

    /*
     * If 0, distance of a repeated match is rep1.
     * Otherwise check is_rep2.
     */
    uint16_t is_rep1[STATES];

    /* If 0, distance of a repeated match is rep2. Otherwise it is rep3. */
    uint16_t is_rep2[STATES];

    /*
     * If 1, the repeated match has length of one byte. Otherwise
     * the length is decoded from rep_len_decoder.
     */
    uint16_t is_rep0_long[STATES][POS_STATES_MAX];

    /*
     * Probability tree for the highest two bits of the match
     * distance. There is a separate probability tree for match
     * lengths of 2 (i.e. MATCH_LEN_MIN), 3, 4, and [5, 273].
     */
    uint16_t dist_slot[DIST_STATES][DIST_SLOTS];

    /*
     * Probility trees for additional bits for match distance
     * when the distance is in the range [4, 127].
     */
    uint16_t dist_special[FULL_DISTANCES - DIST_MODEL_END];

    /*
     * Probability tree for the lowest four bits of a match
     * distance that is equal to or greater than 128.
     */
    uint16_t dist_align[ALIGN_SIZE];

    /* Length of a normal match */
    struct lzma_len_dec match_len_dec;

    /* Length of a repeated match */
    struct lzma_len_dec rep_len_dec;

    /* Probabilities of literals */
    uint16_t literal[LITERAL_CODERS_MAX][LITERAL_CODER_SIZE];
};

struct lzma2_dec {
    /* Position in xz_dec_lzma2_run(). */
    enum lzma2_seq {
        SEQ_CONTROL,
        SEQ_UNCOMPRESSED_1,
        SEQ_UNCOMPRESSED_2,
        SEQ_COMPRESSED_0,
        SEQ_COMPRESSED_1,
        SEQ_PROPERTIES,
        SEQ_LZMA_PREPARE,
        SEQ_LZMA_RUN,
        SEQ_COPY
    } sequence;

    /* Next position after decoding the compressed size of the chunk. */
    enum lzma2_seq next_sequence;

    /* Uncompressed size of LZMA chunk (2 MiB at maximum) */
    uint32_t uncompressed;

    /*
     * Compressed size of LZMA chunk or compressed/uncompressed
     * size of uncompressed chunk (64 KiB at maximum)
     */
    uint32_t compressed;

    /*
     * True if dictionary reset is needed. This is false before
     * the first chunk (LZMA or uncompressed).
     */
    bool need_dict_reset;

    /*
     * True if new LZMA properties are needed. This is false
     * before the first LZMA chunk.
     */
    bool need_props;
};

struct xz_dec_lzma2 {
    /*
     * The order below is important on x86 to reduce code size and
     * it shouldn't hurt on other platforms. Everything up to and
     * including lzma.pos_mask are in the first 128 bytes on x86-32,
     * which allows using smaller instructions to access those
     * variables. On x86-64, fewer variables fit into the first 128
     * bytes, but this is still the best order without sacrificing
     * the readability by splitting the structures.
     */
    struct rc_dec rc;
    struct dictionary dict;
    struct lzma2_dec lzma2;
    struct lzma_dec lzma;

    /*
     * Temporary buffer which holds small number of input bytes between
     * decoder calls. See lzma2_lzma() for details.
     */
    struct {
        uint32_t size;
        uint8_t buf[3 * LZMA_IN_REQUIRED];
    } temp;
};

/**************
 * Dictionary *
 **************/

/*
 * Reset the dictionary state. When in single-call mode, set up the beginning
 * of the dictionary to point to the actual output buffer.
 */
static void dict_reset(struct dictionary *dict, struct xz_buf *b)
{
    if (DEC_IS_SINGLE(dict->mode)) {
        dict->buf = b->out + b->out_pos;
        dict->end = b->out_size - b->out_pos;
    }

    dict->start = 0;
    dict->pos = 0;
    dict->limit = 0;
    dict->full = 0;
}

/* Set dictionary write limit */
static void dict_limit(struct dictionary *dict, size_t out_max)
{
    if (dict->end - dict->pos <= out_max)
        dict->limit = dict->end;
    else
        dict->limit = dict->pos + out_max;
}

/* Return true if at least one byte can be written into the dictionary. */
static inline bool dict_has_space(const struct dictionary *dict)
{
    return dict->pos < dict->limit;
}

/*
 * Get a byte from the dictionary at the given distance. The distance is
 * assumed to valid, or as a special case, zero when the dictionary is
 * still empty. This special case is needed for single-call decoding to
 * avoid writing a '\0' to the end of the destination buffer.
 */
static inline uint32_t dict_get(const struct dictionary *dict, uint32_t dist)
{
    size_t offset = dict->pos - dist - 1;

    if (dist >= dict->pos)
        offset += dict->end;

    return dict->full > 0 ? dict->buf[offset] : 0;
}

/*
 * Put one byte into the dictionary. It is assumed that there is space for it.
 */
static inline void dict_put(struct dictionary *dict, uint8_t byte)
{
    dict->buf[dict->pos++] = byte;

    if (dict->full < dict->pos)
        dict->full = dict->pos;
}

/*
 * Repeat given number of bytes from the given distance. If the distance is
 * invalid, false is returned. On success, true is returned and *len is
 * updated to indicate how many bytes were left to be repeated.
 */
static bool dict_repeat(struct dictionary *dict, uint32_t *len, uint32_t dist)
{
    size_t back;
    uint32_t left;

    if (dist >= dict->full || dist >= dict->size)
        return false;

    left = min_t(size_t, dict->limit - dict->pos, *len);
    *len -= left;

    back = dict->pos - dist - 1;
    if (dist >= dict->pos)
        back += dict->end;

    do {
        dict->buf[dict->pos++] = dict->buf[back++];
        if (back == dict->end)
            back = 0;
    } while (--left > 0);

    if (dict->full < dict->pos)
        dict->full = dict->pos;

    return true;
}

/* Copy uncompressed data as is from input to dictionary and output buffers. */
static void dict_uncompressed(struct dictionary *dict, struct xz_buf *b,
                  uint32_t *left)
{
    size_t copy_size;

    while (*left > 0 && b->in_pos < b->in_size
            && b->out_pos < b->out_size) {
        copy_size = min(b->in_size - b->in_pos,
                b->out_size - b->out_pos);
        if (copy_size > dict->end - dict->pos)
            copy_size = dict->end - dict->pos;
        if (copy_size > *left)
            copy_size = *left;

        *left -= copy_size;

        memcpy(dict->buf + dict->pos, b->in + b->in_pos, copy_size);
        dict->pos += copy_size;

        if (dict->full < dict->pos)
            dict->full = dict->pos;

        if (DEC_IS_MULTI(dict->mode)) {
            if (dict->pos == dict->end)
                dict->pos = 0;

            memcpy(b->out + b->out_pos, b->in + b->in_pos,
                    copy_size);
        }

        dict->start = dict->pos;

        b->out_pos += copy_size;
        b->in_pos += copy_size;
    }
}

/*
 * Flush pending data from dictionary to b->out. It is assumed that there is
 * enough space in b->out. This is guaranteed because caller uses dict_limit()
 * before decoding data into the dictionary.
 */
static uint32_t dict_flush(struct dictionary *dict, struct xz_buf *b)
{
    size_t copy_size = dict->pos - dict->start;

    if (DEC_IS_MULTI(dict->mode)) {
        if (dict->pos == dict->end)
            dict->pos = 0;

        memcpy(b->out + b->out_pos, dict->buf + dict->start,
                copy_size);
    }

    dict->start = dict->pos;
    b->out_pos += copy_size;
    return copy_size;
}

/*****************
 * Range decoder *
 *****************/

/* Reset the range decoder. */
static void rc_reset(struct rc_dec *rc)
{
    rc->range = (uint32_t)-1;
    rc->code = 0;
    rc->init_bytes_left = RC_INIT_BYTES;
}

/*
 * Read the first five initial bytes into rc->code if they haven't been
 * read already. (Yes, the first byte gets completely ignored.)
 */
static bool rc_read_init(struct rc_dec *rc, struct xz_buf *b)
{
    while (rc->init_bytes_left > 0) {
        if (b->in_pos == b->in_size)
            return false;

        rc->code = (rc->code << 8) + b->in[b->in_pos++];
        --rc->init_bytes_left;
    }

    return true;
}

/* Return true if there may not be enough input for the next decoding loop. */
static inline bool rc_limit_exceeded(const struct rc_dec *rc)
{
    return rc->in_pos > rc->in_limit;
}

/*
 * Return true if it is possible (from point of view of range decoder) that
 * we have reached the end of the LZMA chunk.
 */
static inline bool rc_is_finished(const struct rc_dec *rc)
{
    return rc->code == 0;
}

/* Read the next input byte if needed. */
static __always_inline void rc_normalize(struct rc_dec *rc)
{
    if (rc->range < RC_TOP_VALUE) {
        rc->range <<= RC_SHIFT_BITS;
        rc->code = (rc->code << RC_SHIFT_BITS) + rc->in[rc->in_pos++];
    }
}

/*
 * Decode one bit. In some versions, this function has been splitted in three
 * functions so that the compiler is supposed to be able to more easily avoid
 * an extra branch. In this particular version of the LZMA decoder, this
 * doesn't seem to be a good idea (tested with GCC 3.3.6, 3.4.6, and 4.3.3
 * on x86). Using a non-splitted version results in nicer looking code too.
 *
 * NOTE: This must return an int. Do not make it return a bool or the speed
 * of the code generated by GCC 3.x decreases 10-15 %. (GCC 4.3 doesn't care,
 * and it generates 10-20 % faster code than GCC 3.x from this file anyway.)
 */
static __always_inline int rc_bit(struct rc_dec *rc, uint16_t *prob)
{
    uint32_t bound;
    int bit;

    rc_normalize(rc);
    bound = (rc->range >> RC_BIT_MODEL_TOTAL_BITS) * *prob;
    if (rc->code < bound) {
        rc->range = bound;
        *prob += (RC_BIT_MODEL_TOTAL - *prob) >> RC_MOVE_BITS;
        bit = 0;
    } else {
        rc->range -= bound;
        rc->code -= bound;
        *prob -= *prob >> RC_MOVE_BITS;
        bit = 1;
    }

    return bit;
}

/* Decode a bittree starting from the most significant bit. */
static __always_inline uint32_t rc_bittree(struct rc_dec *rc,
                       uint16_t *probs, uint32_t limit)
{
    uint32_t symbol = 1;

    do {
        if (rc_bit(rc, &probs[symbol]))
            symbol = (symbol << 1) + 1;
        else
            symbol <<= 1;
    } while (symbol < limit);

    return symbol;
}

/* Decode a bittree starting from the least significant bit. */
static __always_inline void rc_bittree_reverse(struct rc_dec *rc,
                           uint16_t *probs,
                           uint32_t *dest, uint32_t limit)
{
    uint32_t symbol = 1;
    uint32_t i = 0;

    do {
        if (rc_bit(rc, &probs[symbol])) {
            symbol = (symbol << 1) + 1;
            *dest += 1 << i;
        } else {
            symbol <<= 1;
        }
    } while (++i < limit);
}

/* Decode direct bits (fixed fifty-fifty probability) */
static inline void rc_direct(struct rc_dec *rc, uint32_t *dest, uint32_t limit)
{
    uint32_t mask;

    do {
        rc_normalize(rc);
        rc->range >>= 1;
        rc->code -= rc->range;
        mask = (uint32_t)0 - (rc->code >> 31);
        rc->code += rc->range & mask;
        *dest = (*dest << 1) + (mask + 1);
    } while (--limit > 0);
}

/********
 * LZMA *
 ********/

/* Get pointer to literal coder probability array. */
static uint16_t *lzma_literal_probs(struct xz_dec_lzma2 *s)
{
    uint32_t prev_byte = dict_get(&s->dict, 0);
    uint32_t low = prev_byte >> (8 - s->lzma.lc);
    uint32_t high = (s->dict.pos & s->lzma.literal_pos_mask) << s->lzma.lc;
    return s->lzma.literal[low + high];
}

/* Decode a literal (one 8-bit byte) */
static void lzma_literal(struct xz_dec_lzma2 *s)
{
    uint16_t *probs;
    uint32_t symbol;
    uint32_t match_byte;
    uint32_t match_bit;
    uint32_t offset;
    uint32_t i;

    probs = lzma_literal_probs(s);

    if (lzma_state_is_literal(s->lzma.state)) {
        symbol = rc_bittree(&s->rc, probs, 0x100);
    } else {
        symbol = 1;
        match_byte = dict_get(&s->dict, s->lzma.rep0) << 1;
        offset = 0x100;

        do {
            match_bit = match_byte & offset;
            match_byte <<= 1;
            i = offset + match_bit + symbol;

            if (rc_bit(&s->rc, &probs[i])) {
                symbol = (symbol << 1) + 1;
                offset &= match_bit;
            } else {
                symbol <<= 1;
                offset &= ~match_bit;
            }
        } while (symbol < 0x100);
    }

    dict_put(&s->dict, (uint8_t)symbol);
    lzma_state_literal(&s->lzma.state);
}

/* Decode the length of the match into s->lzma.len. */
static void lzma_len(struct xz_dec_lzma2 *s, struct lzma_len_dec *l,
             uint32_t pos_state)
{
    uint16_t *probs;
    uint32_t limit;

    if (!rc_bit(&s->rc, &l->choice)) {
        probs = l->low[pos_state];
        limit = LEN_LOW_SYMBOLS;
        s->lzma.len = MATCH_LEN_MIN;
    } else {
        if (!rc_bit(&s->rc, &l->choice2)) {
            probs = l->mid[pos_state];
            limit = LEN_MID_SYMBOLS;
            s->lzma.len = MATCH_LEN_MIN + LEN_LOW_SYMBOLS;
        } else {
            probs = l->high;
            limit = LEN_HIGH_SYMBOLS;
            s->lzma.len = MATCH_LEN_MIN + LEN_LOW_SYMBOLS
                    + LEN_MID_SYMBOLS;
        }
    }

    s->lzma.len += rc_bittree(&s->rc, probs, limit) - limit;
}

/* Decode a match. The distance will be stored in s->lzma.rep0. */
static void lzma_match(struct xz_dec_lzma2 *s, uint32_t pos_state)
{
    uint16_t *probs;
    uint32_t dist_slot;
    uint32_t limit;

    lzma_state_match(&s->lzma.state);

    s->lzma.rep3 = s->lzma.rep2;
    s->lzma.rep2 = s->lzma.rep1;
    s->lzma.rep1 = s->lzma.rep0;

    lzma_len(s, &s->lzma.match_len_dec, pos_state);

    probs = s->lzma.dist_slot[lzma_get_dist_state(s->lzma.len)];
    dist_slot = rc_bittree(&s->rc, probs, DIST_SLOTS) - DIST_SLOTS;

    if (dist_slot < DIST_MODEL_START) {
        s->lzma.rep0 = dist_slot;
    } else {
        limit = (dist_slot >> 1) - 1;
        s->lzma.rep0 = 2 + (dist_slot & 1);

        if (dist_slot < DIST_MODEL_END) {
            s->lzma.rep0 <<= limit;
            probs = s->lzma.dist_special + s->lzma.rep0
                    - dist_slot - 1;
            rc_bittree_reverse(&s->rc, probs,
                    &s->lzma.rep0, limit);
        } else {
            rc_direct(&s->rc, &s->lzma.rep0, limit - ALIGN_BITS);
            s->lzma.rep0 <<= ALIGN_BITS;
            rc_bittree_reverse(&s->rc, s->lzma.dist_align,
                    &s->lzma.rep0, ALIGN_BITS);
        }
    }
}

/*
 * Decode a repeated match. The distance is one of the four most recently
 * seen matches. The distance will be stored in s->lzma.rep0.
 */
static void lzma_rep_match(struct xz_dec_lzma2 *s, uint32_t pos_state)
{
    uint32_t tmp;

    if (!rc_bit(&s->rc, &s->lzma.is_rep0[s->lzma.state])) {
        if (!rc_bit(&s->rc, &s->lzma.is_rep0_long[
                s->lzma.state][pos_state])) {
            lzma_state_short_rep(&s->lzma.state);
            s->lzma.len = 1;
            return;
        }
    } else {
        if (!rc_bit(&s->rc, &s->lzma.is_rep1[s->lzma.state])) {
            tmp = s->lzma.rep1;
        } else {
            if (!rc_bit(&s->rc, &s->lzma.is_rep2[s->lzma.state])) {
                tmp = s->lzma.rep2;
            } else {
                tmp = s->lzma.rep3;
                s->lzma.rep3 = s->lzma.rep2;
            }

            s->lzma.rep2 = s->lzma.rep1;
        }

        s->lzma.rep1 = s->lzma.rep0;
        s->lzma.rep0 = tmp;
    }

    lzma_state_long_rep(&s->lzma.state);
    lzma_len(s, &s->lzma.rep_len_dec, pos_state);
}

/* LZMA decoder core */
static bool lzma_main(struct xz_dec_lzma2 *s)
{
    uint32_t pos_state;

    /*
     * If the dictionary was reached during the previous call, try to
     * finish the possibly pending repeat in the dictionary.
     */
    if (dict_has_space(&s->dict) && s->lzma.len > 0)
        dict_repeat(&s->dict, &s->lzma.len, s->lzma.rep0);

    /*
     * Decode more LZMA symbols. One iteration may consume up to
     * LZMA_IN_REQUIRED - 1 bytes.
     */
    while (dict_has_space(&s->dict) && !rc_limit_exceeded(&s->rc)) {
        pos_state = s->dict.pos & s->lzma.pos_mask;

        if (!rc_bit(&s->rc, &s->lzma.is_match[
                s->lzma.state][pos_state])) {
            lzma_literal(s);
        } else {
            if (rc_bit(&s->rc, &s->lzma.is_rep[s->lzma.state]))
                lzma_rep_match(s, pos_state);
            else
                lzma_match(s, pos_state);

            if (!dict_repeat(&s->dict, &s->lzma.len, s->lzma.rep0))
                return false;
        }
    }

    /*
     * Having the range decoder always normalized when we are outside
     * this function makes it easier to correctly handle end of the chunk.
     */
    rc_normalize(&s->rc);

    return true;
}

/*
 * Reset the LZMA decoder and range decoder state. Dictionary is nore reset
 * here, because LZMA state may be reset without resetting the dictionary.
 */
static void lzma_reset(struct xz_dec_lzma2 *s)
{
    uint16_t *probs;
    size_t i;

    s->lzma.state = STATE_LIT_LIT;
    s->lzma.rep0 = 0;
    s->lzma.rep1 = 0;
    s->lzma.rep2 = 0;
    s->lzma.rep3 = 0;

    /*
     * All probabilities are initialized to the same value. This hack
     * makes the code smaller by avoiding a separate loop for each
     * probability array.
     *
     * This could be optimized so that only that part of literal
     * probabilities that are actually required. In the common case
     * we would write 12 KiB less.
     */
    probs = s->lzma.is_match[0];
    for (i = 0; i < PROBS_TOTAL; ++i)
        probs[i] = RC_BIT_MODEL_TOTAL / 2;

    rc_reset(&s->rc);
}

/*
 * Decode and validate LZMA properties (lc/lp/pb) and calculate the bit masks
 * from the decoded lp and pb values. On success, the LZMA decoder state is
 * reset and true is returned.
 */
static bool lzma_props(struct xz_dec_lzma2 *s, uint8_t props)
{
    if (props > (4 * 5 + 4) * 9 + 8)
        return false;

    s->lzma.pos_mask = 0;
    while (props >= 9 * 5) {
        props -= 9 * 5;
        ++s->lzma.pos_mask;
    }

    s->lzma.pos_mask = (1 << s->lzma.pos_mask) - 1;

    s->lzma.literal_pos_mask = 0;
    while (props >= 9) {
        props -= 9;
        ++s->lzma.literal_pos_mask;
    }

    s->lzma.lc = props;

    if (s->lzma.lc + s->lzma.literal_pos_mask > 4)
        return false;

    s->lzma.literal_pos_mask = (1 << s->lzma.literal_pos_mask) - 1;

    lzma_reset(s);

    return true;
}

/*********
 * LZMA2 *
 *********/

/*
 * The LZMA decoder assumes that if the input limit (s->rc.in_limit) hasn't
 * been exceeded, it is safe to read up to LZMA_IN_REQUIRED bytes. This
 * wrapper function takes care of making the LZMA decoder's assumption safe.
 *
 * As long as there is plenty of input left to be decoded in the current LZMA
 * chunk, we decode directly from the caller-supplied input buffer until
 * there's LZMA_IN_REQUIRED bytes left. Those remaining bytes are copied into
 * s->temp.buf, which (hopefully) gets filled on the next call to this
 * function. We decode a few bytes from the temporary buffer so that we can
 * continue decoding from the caller-supplied input buffer again.
 */
static bool lzma2_lzma(struct xz_dec_lzma2 *s, struct xz_buf *b)
{
    size_t in_avail;
    uint32_t tmp;

    in_avail = b->in_size - b->in_pos;
    if (s->temp.size > 0 || s->lzma2.compressed == 0) {
        tmp = 2 * LZMA_IN_REQUIRED - s->temp.size;
        if (tmp > s->lzma2.compressed - s->temp.size)
            tmp = s->lzma2.compressed - s->temp.size;
        if (tmp > in_avail)
            tmp = in_avail;

        memcpy(s->temp.buf + s->temp.size, b->in + b->in_pos, tmp);

        if (s->temp.size + tmp == s->lzma2.compressed) {
            memzero(s->temp.buf + s->temp.size + tmp,
                    sizeof(s->temp.buf)
                        - s->temp.size - tmp);
            s->rc.in_limit = s->temp.size + tmp;
        } else if (s->temp.size + tmp < LZMA_IN_REQUIRED) {
            s->temp.size += tmp;
            b->in_pos += tmp;
            return true;
        } else {
            s->rc.in_limit = s->temp.size + tmp - LZMA_IN_REQUIRED;
        }

        s->rc.in = s->temp.buf;
        s->rc.in_pos = 0;

        if (!lzma_main(s) || s->rc.in_pos > s->temp.size + tmp)
            return false;

        s->lzma2.compressed -= s->rc.in_pos;

        if (s->rc.in_pos < s->temp.size) {
            s->temp.size -= s->rc.in_pos;
            memmove(s->temp.buf, s->temp.buf + s->rc.in_pos,
                    s->temp.size);
            return true;
        }

        b->in_pos += s->rc.in_pos - s->temp.size;
        s->temp.size = 0;
    }

    in_avail = b->in_size - b->in_pos;
    if (in_avail >= LZMA_IN_REQUIRED) {
        s->rc.in = b->in;
        s->rc.in_pos = b->in_pos;

        if (in_avail >= s->lzma2.compressed + LZMA_IN_REQUIRED)
            s->rc.in_limit = b->in_pos + s->lzma2.compressed;
        else
            s->rc.in_limit = b->in_size - LZMA_IN_REQUIRED;

        if (!lzma_main(s))
            return false;

        in_avail = s->rc.in_pos - b->in_pos;
        if (in_avail > s->lzma2.compressed)
            return false;

        s->lzma2.compressed -= in_avail;
        b->in_pos = s->rc.in_pos;
    }

    in_avail = b->in_size - b->in_pos;
    if (in_avail < LZMA_IN_REQUIRED) {
        if (in_avail > s->lzma2.compressed)
            in_avail = s->lzma2.compressed;

        memcpy(s->temp.buf, b->in + b->in_pos, in_avail);
        s->temp.size = in_avail;
        b->in_pos += in_avail;
    }

    return true;
}

/*
 * Take care of the LZMA2 control layer, and forward the job of actual LZMA
 * decoding or copying of uncompressed chunks to other functions.
 */
XZ_EXTERN enum xz_ret xz_dec_lzma2_run(struct xz_dec_lzma2 *s,
                       struct xz_buf *b)
{
    uint32_t tmp;

    while (b->in_pos < b->in_size || s->lzma2.sequence == SEQ_LZMA_RUN) {
        switch (s->lzma2.sequence) {
        case SEQ_CONTROL:
            /*
             * LZMA2 control byte
             *
             * Exact values:
             *   0x00   End marker
             *   0x01   Dictionary reset followed by
             *          an uncompressed chunk
             *   0x02   Uncompressed chunk (no dictionary reset)
             *
             * Highest three bits (s->control & 0xE0):
             *   0xE0   Dictionary reset, new properties and state
             *          reset, followed by LZMA compressed chunk
             *   0xC0   New properties and state reset, followed
             *          by LZMA compressed chunk (no dictionary
             *          reset)
             *   0xA0   State reset using old properties,
             *          followed by LZMA compressed chunk (no
             *          dictionary reset)
             *   0x80   LZMA chunk (no dictionary or state reset)
             *
             * For LZMA compressed chunks, the lowest five bits
             * (s->control & 1F) are the highest bits of the
             * uncompressed size (bits 16-20).
             *
             * A new LZMA2 stream must begin with a dictionary
             * reset. The first LZMA chunk must set new
             * properties and reset the LZMA state.
             *
             * Values that don't match anything described above
             * are invalid and we return XZ_DATA_ERROR.
             */
            tmp = b->in[b->in_pos++];

            if (tmp >= 0xE0 || tmp == 0x01) {
                s->lzma2.need_props = true;
                s->lzma2.need_dict_reset = false;
                dict_reset(&s->dict, b);
            } else if (s->lzma2.need_dict_reset) {
                return XZ_DATA_ERROR;
            }

            if (tmp >= 0x80) {
                s->lzma2.uncompressed = (tmp & 0x1F) << 16;
                s->lzma2.sequence = SEQ_UNCOMPRESSED_1;

                if (tmp >= 0xC0) {
                    /*
                     * When there are new properties,
                     * state reset is done at
                     * SEQ_PROPERTIES.
                     */
                    s->lzma2.need_props = false;
                    s->lzma2.next_sequence
                            = SEQ_PROPERTIES;

                } else if (s->lzma2.need_props) {
                    return XZ_DATA_ERROR;

                } else {
                    s->lzma2.next_sequence
                            = SEQ_LZMA_PREPARE;
                    if (tmp >= 0xA0)
                        lzma_reset(s);
                }
            } else {
                if (tmp == 0x00)
                    return XZ_STREAM_END;

                if (tmp > 0x02)
                    return XZ_DATA_ERROR;

                s->lzma2.sequence = SEQ_COMPRESSED_0;
                s->lzma2.next_sequence = SEQ_COPY;
            }

            break;

        case SEQ_UNCOMPRESSED_1:
            s->lzma2.uncompressed
                    += (uint32_t)b->in[b->in_pos++] << 8;
            s->lzma2.sequence = SEQ_UNCOMPRESSED_2;
            break;

        case SEQ_UNCOMPRESSED_2:
            s->lzma2.uncompressed
                    += (uint32_t)b->in[b->in_pos++] + 1;
            s->lzma2.sequence = SEQ_COMPRESSED_0;
            break;

        case SEQ_COMPRESSED_0:
            s->lzma2.compressed
                    = (uint32_t)b->in[b->in_pos++] << 8;
            s->lzma2.sequence = SEQ_COMPRESSED_1;
            break;

        case SEQ_COMPRESSED_1:
            s->lzma2.compressed
                    += (uint32_t)b->in[b->in_pos++] + 1;
            s->lzma2.sequence = s->lzma2.next_sequence;
            break;

        case SEQ_PROPERTIES:
            if (!lzma_props(s, b->in[b->in_pos++]))
                return XZ_DATA_ERROR;

            s->lzma2.sequence = SEQ_LZMA_PREPARE;
            __attribute__((fallthrough));
        case SEQ_LZMA_PREPARE:
            if (s->lzma2.compressed < RC_INIT_BYTES)
                return XZ_DATA_ERROR;

            if (!rc_read_init(&s->rc, b))
                return XZ_OK;

            s->lzma2.compressed -= RC_INIT_BYTES;
            s->lzma2.sequence = SEQ_LZMA_RUN;
        __attribute__((fallthrough));
        case SEQ_LZMA_RUN:
            /*
             * Set dictionary limit to indicate how much we want
             * to be encoded at maximum. Decode new data into the
             * dictionary. Flush the new data from dictionary to
             * b->out. Check if we finished decoding this chunk.
             * In case the dictionary got full but we didn't fill
             * the output buffer yet, we may run this loop
             * multiple times without changing s->lzma2.sequence.
             */
            dict_limit(&s->dict, min_t(size_t,
                    b->out_size - b->out_pos,
                    s->lzma2.uncompressed));
            if (!lzma2_lzma(s, b))
                return XZ_DATA_ERROR;

            s->lzma2.uncompressed -= dict_flush(&s->dict, b);

            if (s->lzma2.uncompressed == 0) {
                if (s->lzma2.compressed > 0 || s->lzma.len > 0
                        || !rc_is_finished(&s->rc))
                    return XZ_DATA_ERROR;

                rc_reset(&s->rc);
                s->lzma2.sequence = SEQ_CONTROL;

            } else if (b->out_pos == b->out_size
                    || (b->in_pos == b->in_size
                        && s->temp.size
                        < s->lzma2.compressed)) {
                return XZ_OK;
            }

            break;

        case SEQ_COPY:
            dict_uncompressed(&s->dict, b, &s->lzma2.compressed);
            if (s->lzma2.compressed > 0)
                return XZ_OK;

            s->lzma2.sequence = SEQ_CONTROL;
            break;
        }
    }

    return XZ_OK;
}

XZ_EXTERN struct xz_dec_lzma2 *xz_dec_lzma2_create(enum xz_mode mode,
                           uint32_t dict_max)
{
    struct xz_dec_lzma2 *s = kmalloc(sizeof(*s), GFP_KERNEL);
    if (s == NULL)
        return NULL;

    s->dict.mode = mode;
    s->dict.size_max = dict_max;

    if (DEC_IS_PREALLOC(mode)) {
        s->dict.buf = vmalloc(dict_max);
        if (s->dict.buf == NULL) {
            kfree(s);
            return NULL;
        }
    } else if (DEC_IS_DYNALLOC(mode)) {
        s->dict.buf = NULL;
        s->dict.allocated = 0;
    }

    return s;
}

XZ_EXTERN enum xz_ret xz_dec_lzma2_reset(struct xz_dec_lzma2 *s, uint8_t props)
{
    /* This limits dictionary size to 3 GiB to keep parsing simpler. */
    if (props > 39)
        return XZ_OPTIONS_ERROR;

    s->dict.size = 2 + (props & 1);
    s->dict.size <<= (props >> 1) + 11;

    if (DEC_IS_MULTI(s->dict.mode)) {
        if (s->dict.size > s->dict.size_max)
            return XZ_MEMLIMIT_ERROR;

        s->dict.end = s->dict.size;

        if (DEC_IS_DYNALLOC(s->dict.mode)) {
            if (s->dict.allocated < s->dict.size) {
                vfree(s->dict.buf);
                s->dict.buf = vmalloc(s->dict.size);
                if (s->dict.buf == NULL) {
                    s->dict.allocated = 0;
                    return XZ_MEM_ERROR;
                }
            }
        }
    }

    s->lzma.len = 0;

    s->lzma2.sequence = SEQ_CONTROL;
    s->lzma2.need_dict_reset = true;

    s->temp.size = 0;

    return XZ_OK;
}

XZ_EXTERN void xz_dec_lzma2_end(struct xz_dec_lzma2 *s)
{
    if (DEC_IS_MULTI(s->dict.mode))
        vfree(s->dict.buf);

    kfree(s);
}
//xz_dec_lzma2.c end

//xz_dec_stream.c start
struct xz_dec_hash {
    vli_type unpadded;
    vli_type uncompressed;
    uint32_t crc32;
};

struct xz_dec {
    /* Position in dec_main() */
    enum {
        SEQ_STREAM_HEADER,
        SEQ_BLOCK_START,
        SEQ_BLOCK_HEADER,
        SEQ_BLOCK_UNCOMPRESS,
        SEQ_BLOCK_PADDING,
        SEQ_BLOCK_CHECK,
        SEQ_INDEX,
        SEQ_INDEX_PADDING,
        SEQ_INDEX_CRC32,
        SEQ_STREAM_FOOTER
    } sequence;

    /* Position in variable-length integers and Check fields */
    uint32_t pos;

    /* Variable-length integer decoded by dec_vli() */
    vli_type vli;

    /* Saved in_pos and out_pos */
    size_t in_start;
    size_t out_start;

    /* CRC32 value in Block or Index */
    uint32_t crc32;

    /* Type of the integrity check calculated from uncompressed data */
    enum xz_check check_type;

    /* Operation mode */
    enum xz_mode mode;

    /*
     * True if the next call to xz_dec_run() is allowed to return
     * XZ_BUF_ERROR.
     */
    bool allow_buf_error;

    /* Information stored in Block Header */
    struct {
        /*
         * Value stored in the Compressed Size field, or
         * VLI_UNKNOWN if Compressed Size is not present.
         */
        vli_type compressed;

        /*
         * Value stored in the Uncompressed Size field, or
         * VLI_UNKNOWN if Uncompressed Size is not present.
         */
        vli_type uncompressed;

        /* Size of the Block Header field */
        uint32_t size;
    } block_header;

    /* Information collected when decoding Blocks */
    struct {
        /* Observed compressed size of the current Block */
        vli_type compressed;

        /* Observed uncompressed size of the current Block */
        vli_type uncompressed;

        /* Number of Blocks decoded so far */
        vli_type count;

        /*
         * Hash calculated from the Block sizes. This is used to
         * validate the Index field.
         */
        struct xz_dec_hash hash;
    } block;

    /* Variables needed when verifying the Index field */
    struct {
        /* Position in dec_index() */
        enum {
            SEQ_INDEX_COUNT,
            SEQ_INDEX_UNPADDED,
            SEQ_INDEX_UNCOMPRESSED
        } sequence;

        /* Size of the Index in bytes */
        vli_type size;

        /* Number of Records (matches block.count in valid files) */
        vli_type count;

        /*
         * Hash calculated from the Records (matches block.hash in
         * valid files).
         */
        struct xz_dec_hash hash;
    } index;

    /*
     * Temporary buffer needed to hold Stream Header, Block Header,
     * and Stream Footer. The Block Header is the biggest (1 KiB)
     * so we reserve space according to that. buf[] has to be aligned
     * to a multiple of four bytes; the size_t variables before it
     * should guarantee this.
     */
    struct {
        size_t pos;
        size_t size;
        uint8_t buf[1024];
    } temp;

    struct xz_dec_lzma2 *lzma2;

#ifdef XZ_DEC_BCJ
    struct xz_dec_bcj *bcj;
    bool bcj_active;
#endif
};

#ifdef XZ_DEC_ANY_CHECK
/* Sizes of the Check field with different Check IDs */
static const uint8_t check_sizes[16] = {
    0,
    4, 4, 4,
    8, 8, 8,
    16, 16, 16,
    32, 32, 32,
    64, 64, 64
};
#endif

/*
 * Fill s->temp by copying data starting from b->in[b->in_pos]. Caller
 * must have set s->temp.pos to indicate how much data we are supposed
 * to copy into s->temp.buf. Return true once s->temp.pos has reached
 * s->temp.size.
 */
static bool fill_temp(struct xz_dec *s, struct xz_buf *b)
{
    size_t copy_size = min_t(size_t,
            b->in_size - b->in_pos, s->temp.size - s->temp.pos);

    memcpy(s->temp.buf + s->temp.pos, b->in + b->in_pos, copy_size);
    b->in_pos += copy_size;
    s->temp.pos += copy_size;

    if (s->temp.pos == s->temp.size) {
        s->temp.pos = 0;
        return true;
    }

    return false;
}

/* Decode a variable-length integer (little-endian base-128 encoding) */
static enum xz_ret dec_vli(struct xz_dec *s, const uint8_t *in,
               size_t *in_pos, size_t in_size)
{
    uint8_t byte;

    if (s->pos == 0)
        s->vli = 0;

    while (*in_pos < in_size) {
        byte = in[*in_pos];
        ++*in_pos;

        s->vli |= (vli_type)(byte & 0x7F) << s->pos;

        if ((byte & 0x80) == 0) {
            /* Don't allow non-minimal encodings. */
            if (byte == 0 && s->pos != 0)
                return XZ_DATA_ERROR;

            s->pos = 0;
            return XZ_STREAM_END;
        }

        s->pos += 7;
        if (s->pos == 7 * VLI_BYTES_MAX)
            return XZ_DATA_ERROR;
    }

    return XZ_OK;
}

/*
 * Decode the Compressed Data field from a Block. Update and validate
 * the observed compressed and uncompressed sizes of the Block so that
 * they don't exceed the values possibly stored in the Block Header
 * (validation assumes that no integer overflow occurs, since vli_type
 * is normally uint64_t). Update the CRC32 if presence of the CRC32
 * field was indicated in Stream Header.
 *
 * Once the decoding is finished, validate that the observed sizes match
 * the sizes possibly stored in the Block Header. Update the hash and
 * Block count, which are later used to validate the Index field.
 */
static enum xz_ret dec_block(struct xz_dec *s, struct xz_buf *b)
{
    enum xz_ret ret;

    s->in_start = b->in_pos;
    s->out_start = b->out_pos;

#ifdef XZ_DEC_BCJ
    if (s->bcj_active)
        ret = xz_dec_bcj_run(s->bcj, s->lzma2, b);
    else
#endif
        ret = xz_dec_lzma2_run(s->lzma2, b);

    s->block.compressed += b->in_pos - s->in_start;
    s->block.uncompressed += b->out_pos - s->out_start;

    /*
     * There is no need to separately check for VLI_UNKNOWN, since
     * the observed sizes are always smaller than VLI_UNKNOWN.
     */
    if (s->block.compressed > s->block_header.compressed
            || s->block.uncompressed
                > s->block_header.uncompressed)
        return XZ_DATA_ERROR;

    if (s->check_type == XZ_CHECK_CRC32)
        s->crc32 = xz_crc32(b->out + s->out_start,
                b->out_pos - s->out_start, s->crc32);

    if (ret == XZ_STREAM_END) {
        if (s->block_header.compressed != VLI_UNKNOWN
                && s->block_header.compressed
                    != s->block.compressed)
            return XZ_DATA_ERROR;

        if (s->block_header.uncompressed != VLI_UNKNOWN
                && s->block_header.uncompressed
                    != s->block.uncompressed)
            return XZ_DATA_ERROR;

        s->block.hash.unpadded += s->block_header.size
                + s->block.compressed;

#ifdef XZ_DEC_ANY_CHECK
        s->block.hash.unpadded += check_sizes[s->check_type];
#else
        if (s->check_type == XZ_CHECK_CRC32)
            s->block.hash.unpadded += 4;
#endif

        s->block.hash.uncompressed += s->block.uncompressed;
        s->block.hash.crc32 = xz_crc32(
                (const uint8_t *)&s->block.hash,
                sizeof(s->block.hash), s->block.hash.crc32);

        ++s->block.count;
    }

    return ret;
}

/* Update the Index size and the CRC32 value. */
static void index_update(struct xz_dec *s, const struct xz_buf *b)
{
    size_t in_used = b->in_pos - s->in_start;
    s->index.size += in_used;
    s->crc32 = xz_crc32(b->in + s->in_start, in_used, s->crc32);
}

/*
 * Decode the Number of Records, Unpadded Size, and Uncompressed Size
 * fields from the Index field. That is, Index Padding and CRC32 are not
 * decoded by this function.
 *
 * This can return XZ_OK (more input needed), XZ_STREAM_END (everything
 * successfully decoded), or XZ_DATA_ERROR (input is corrupt).
 */
static enum xz_ret dec_index(struct xz_dec *s, struct xz_buf *b)
{
    enum xz_ret ret;

    do {
        ret = dec_vli(s, b->in, &b->in_pos, b->in_size);
        if (ret != XZ_STREAM_END) {
            index_update(s, b);
            return ret;
        }

        switch (s->index.sequence) {
        case SEQ_INDEX_COUNT:
            s->index.count = s->vli;

            /*
             * Validate that the Number of Records field
             * indicates the same number of Records as
             * there were Blocks in the Stream.
             */
            if (s->index.count != s->block.count)
                return XZ_DATA_ERROR;

            s->index.sequence = SEQ_INDEX_UNPADDED;
            break;

        case SEQ_INDEX_UNPADDED:
            s->index.hash.unpadded += s->vli;
            s->index.sequence = SEQ_INDEX_UNCOMPRESSED;
            break;

        case SEQ_INDEX_UNCOMPRESSED:
            s->index.hash.uncompressed += s->vli;
            s->index.hash.crc32 = xz_crc32(
                    (const uint8_t *)&s->index.hash,
                    sizeof(s->index.hash),
                    s->index.hash.crc32);
            --s->index.count;
            s->index.sequence = SEQ_INDEX_UNPADDED;
            break;
        }
    } while (s->index.count > 0);

    return XZ_STREAM_END;
}

/*
 * Validate that the next four input bytes match the value of s->crc32.
 * s->pos must be zero when starting to validate the first byte.
 */
static enum xz_ret crc32_validate(struct xz_dec *s, struct xz_buf *b)
{
    do {
        if (b->in_pos == b->in_size)
            return XZ_OK;

        if (((s->crc32 >> s->pos) & 0xFF) != b->in[b->in_pos++])
            return XZ_DATA_ERROR;

        s->pos += 8;

    } while (s->pos < 32);

    s->crc32 = 0;
    s->pos = 0;

    return XZ_STREAM_END;
}

#ifdef XZ_DEC_ANY_CHECK
/*
 * Skip over the Check field when the Check ID is not supported.
 * Returns true once the whole Check field has been skipped over.
 */
static bool check_skip(struct xz_dec *s, struct xz_buf *b)
{
    while (s->pos < check_sizes[s->check_type]) {
        if (b->in_pos == b->in_size)
            return false;

        ++b->in_pos;
        ++s->pos;
    }

    s->pos = 0;

    return true;
}
#endif

/* Decode the Stream Header field (the first 12 bytes of the .xz Stream). */
static enum xz_ret dec_stream_header(struct xz_dec *s)
{
    if (!memeq(s->temp.buf, HEADER_MAGIC, HEADER_MAGIC_SIZE))
        return XZ_FORMAT_ERROR;

    if (xz_crc32(s->temp.buf + HEADER_MAGIC_SIZE, 2, 0)
            != get_le32(s->temp.buf + HEADER_MAGIC_SIZE + 2))
        return XZ_DATA_ERROR;

    if (s->temp.buf[HEADER_MAGIC_SIZE] != 0)
        return XZ_OPTIONS_ERROR;

    /*
     * Of integrity checks, we support only none (Check ID = 0) and
     * CRC32 (Check ID = 1). However, if XZ_DEC_ANY_CHECK is defined,
     * we will accept other check types too, but then the check won't
     * be verified and a warning (XZ_UNSUPPORTED_CHECK) will be given.
     */
    s->check_type = s->temp.buf[HEADER_MAGIC_SIZE + 1];

#ifdef XZ_DEC_ANY_CHECK
    if (s->check_type > XZ_CHECK_MAX)
        return XZ_OPTIONS_ERROR;

    if (s->check_type > XZ_CHECK_CRC32)
        return XZ_UNSUPPORTED_CHECK;
#else
    if (s->check_type > XZ_CHECK_CRC32)
        return XZ_OPTIONS_ERROR;
#endif

    return XZ_OK;
}

/* Decode the Stream Footer field (the last 12 bytes of the .xz Stream) */
static enum xz_ret dec_stream_footer(struct xz_dec *s)
{
    if (!memeq(s->temp.buf + 10, FOOTER_MAGIC, FOOTER_MAGIC_SIZE))
        return XZ_DATA_ERROR;

    if (xz_crc32(s->temp.buf + 4, 6, 0) != get_le32(s->temp.buf))
        return XZ_DATA_ERROR;

    /*
     * Validate Backward Size. Note that we never added the size of the
     * Index CRC32 field to s->index.size, thus we use s->index.size / 4
     * instead of s->index.size / 4 - 1.
     */
    if ((s->index.size >> 2) != get_le32(s->temp.buf + 4))
        return XZ_DATA_ERROR;

    if (s->temp.buf[8] != 0 || s->temp.buf[9] != s->check_type)
        return XZ_DATA_ERROR;

    /*
     * Use XZ_STREAM_END instead of XZ_OK to be more convenient
     * for the caller.
     */
    return XZ_STREAM_END;
}

/* Decode the Block Header and initialize the filter chain. */
static enum xz_ret dec_block_header(struct xz_dec *s)
{
    enum xz_ret ret;

    /*
     * Validate the CRC32. We know that the temp buffer is at least
     * eight bytes so this is safe.
     */
    s->temp.size -= 4;
    if (xz_crc32(s->temp.buf, s->temp.size, 0)
            != get_le32(s->temp.buf + s->temp.size))
        return XZ_DATA_ERROR;

    s->temp.pos = 2;

    /*
     * Catch unsupported Block Flags. We support only one or two filters
     * in the chain, so we catch that with the same test.
     */
#ifdef XZ_DEC_BCJ
    if (s->temp.buf[1] & 0x3E)
#else
    if (s->temp.buf[1] & 0x3F)
#endif
        return XZ_OPTIONS_ERROR;

    /* Compressed Size */
    if (s->temp.buf[1] & 0x40) {
        if (dec_vli(s, s->temp.buf, &s->temp.pos, s->temp.size)
                    != XZ_STREAM_END)
            return XZ_DATA_ERROR;

        s->block_header.compressed = s->vli;
    } else {
        s->block_header.compressed = VLI_UNKNOWN;
    }

    /* Uncompressed Size */
    if (s->temp.buf[1] & 0x80) {
        if (dec_vli(s, s->temp.buf, &s->temp.pos, s->temp.size)
                != XZ_STREAM_END)
            return XZ_DATA_ERROR;

        s->block_header.uncompressed = s->vli;
    } else {
        s->block_header.uncompressed = VLI_UNKNOWN;
    }

#ifdef XZ_DEC_BCJ
    /* If there are two filters, the first one must be a BCJ filter. */
    s->bcj_active = s->temp.buf[1] & 0x01;
    if (s->bcj_active) {
        if (s->temp.size - s->temp.pos < 2)
            return XZ_OPTIONS_ERROR;

        ret = xz_dec_bcj_reset(s->bcj, s->temp.buf[s->temp.pos++]);
        if (ret != XZ_OK)
            return ret;

        /*
         * We don't support custom start offset,
         * so Size of Properties must be zero.
         */
        if (s->temp.buf[s->temp.pos++] != 0x00)
            return XZ_OPTIONS_ERROR;
    }
#endif

    /* Valid Filter Flags always take at least two bytes. */
    if (s->temp.size - s->temp.pos < 2)
        return XZ_DATA_ERROR;

    /* Filter ID = LZMA2 */
    if (s->temp.buf[s->temp.pos++] != 0x21)
        return XZ_OPTIONS_ERROR;

    /* Size of Properties = 1-byte Filter Properties */
    if (s->temp.buf[s->temp.pos++] != 0x01)
        return XZ_OPTIONS_ERROR;

    /* Filter Properties contains LZMA2 dictionary size. */
    if (s->temp.size - s->temp.pos < 1)
        return XZ_DATA_ERROR;

    ret = xz_dec_lzma2_reset(s->lzma2, s->temp.buf[s->temp.pos++]);
    if (ret != XZ_OK)
        return ret;

    /* The rest must be Header Padding. */
    while (s->temp.pos < s->temp.size)
        if (s->temp.buf[s->temp.pos++] != 0x00)
            return XZ_OPTIONS_ERROR;

    s->temp.pos = 0;
    s->block.compressed = 0;
    s->block.uncompressed = 0;

    return XZ_OK;
}

static enum xz_ret dec_main(struct xz_dec *s, struct xz_buf *b)
{
    enum xz_ret ret;

    /*
     * Store the start position for the case when we are in the middle
     * of the Index field.
     */
    s->in_start = b->in_pos;

    while (true) {
        switch (s->sequence) {
        case SEQ_STREAM_HEADER:
            /*
             * Stream Header is copied to s->temp, and then
             * decoded from there. This way if the caller
             * gives us only little input at a time, we can
             * still keep the Stream Header decoding code
             * simple. Similar approach is used in many places
             * in this file.
             */
            if (!fill_temp(s, b))
                return XZ_OK;

            /*
             * If dec_stream_header() returns
             * XZ_UNSUPPORTED_CHECK, it is still possible
             * to continue decoding if working in multi-call
             * mode. Thus, update s->sequence before calling
             * dec_stream_header().
             */
            s->sequence = SEQ_BLOCK_START;

            ret = dec_stream_header(s);
            if (ret != XZ_OK)
                return ret;
        __attribute__((fallthrough));
        case SEQ_BLOCK_START:
            /* We need one byte of input to continue. */
            if (b->in_pos == b->in_size)
                return XZ_OK;

            /* See if this is the beginning of the Index field. */
            if (b->in[b->in_pos] == 0) {
                s->in_start = b->in_pos++;
                s->sequence = SEQ_INDEX;
                break;
            }

            /*
             * Calculate the size of the Block Header and
             * prepare to decode it.
             */
            s->block_header.size
                = ((uint32_t)b->in[b->in_pos] + 1) * 4;

            s->temp.size = s->block_header.size;
            s->temp.pos = 0;
            s->sequence = SEQ_BLOCK_HEADER;
        __attribute__((fallthrough));
        case SEQ_BLOCK_HEADER:
            if (!fill_temp(s, b))
                return XZ_OK;

            ret = dec_block_header(s);
            if (ret != XZ_OK)
                return ret;

            s->sequence = SEQ_BLOCK_UNCOMPRESS;
        __attribute__((fallthrough));
        case SEQ_BLOCK_UNCOMPRESS:
            ret = dec_block(s, b);
            if (ret != XZ_STREAM_END)
                return ret;

            s->sequence = SEQ_BLOCK_PADDING;

        case SEQ_BLOCK_PADDING:
            /*
             * Size of Compressed Data + Block Padding
             * must be a multiple of four. We don't need
             * s->block.compressed for anything else
             * anymore, so we use it here to test the size
             * of the Block Padding field.
             */
            while (s->block.compressed & 3) {
                if (b->in_pos == b->in_size)
                    return XZ_OK;

                if (b->in[b->in_pos++] != 0)
                    return XZ_DATA_ERROR;

                ++s->block.compressed;
            }

            s->sequence = SEQ_BLOCK_CHECK;
        __attribute__((fallthrough));
        case SEQ_BLOCK_CHECK:
            if (s->check_type == XZ_CHECK_CRC32) {
                ret = crc32_validate(s, b);
                if (ret != XZ_STREAM_END)
                    return ret;
            }
#ifdef XZ_DEC_ANY_CHECK
            else if (!check_skip(s, b)) {
                return XZ_OK;
            }
#endif

            s->sequence = SEQ_BLOCK_START;
            break;

        case SEQ_INDEX:
            ret = dec_index(s, b);
            if (ret != XZ_STREAM_END)
                return ret;

            s->sequence = SEQ_INDEX_PADDING;

        case SEQ_INDEX_PADDING:
            while ((s->index.size + (b->in_pos - s->in_start))
                    & 3) {
                if (b->in_pos == b->in_size) {
                    index_update(s, b);
                    return XZ_OK;
                }

                if (b->in[b->in_pos++] != 0)
                    return XZ_DATA_ERROR;
            }

            /* Finish the CRC32 value and Index size. */
            index_update(s, b);

            /* Compare the hashes to validate the Index field. */
            if (!memeq(&s->block.hash, &s->index.hash,
                    sizeof(s->block.hash)))
                return XZ_DATA_ERROR;

            s->sequence = SEQ_INDEX_CRC32;
        __attribute__((fallthrough));
        case SEQ_INDEX_CRC32:
            ret = crc32_validate(s, b);
            if (ret != XZ_STREAM_END)
                return ret;

            s->temp.size = STREAM_HEADER_SIZE;
            s->sequence = SEQ_STREAM_FOOTER;
        __attribute__((fallthrough));
        case SEQ_STREAM_FOOTER:
            if (!fill_temp(s, b))
                return XZ_OK;

            return dec_stream_footer(s);
        }
    }

    /* Never reached */
}

/*
 * xz_dec_run() is a wrapper for dec_main() to handle some special cases in
 * multi-call and single-call decoding.
 *
 * In multi-call mode, we must return XZ_BUF_ERROR when it seems clear that we
 * are not going to make any progress anymore. This is to prevent the caller
 * from calling us infinitely when the input file is truncated or otherwise
 * corrupt. Since zlib-style API allows that the caller fills the input buffer
 * only when the decoder doesn't produce any new output, we have to be careful
 * to avoid returning XZ_BUF_ERROR too easily: XZ_BUF_ERROR is returned only
 * after the second consecutive call to xz_dec_run() that makes no progress.
 *
 * In single-call mode, if we couldn't decode everything and no error
 * occurred, either the input is truncated or the output buffer is too small.
 * Since we know that the last input byte never produces any output, we know
 * that if all the input was consumed and decoding wasn't finished, the file
 * must be corrupt. Otherwise the output buffer has to be too small or the
 * file is corrupt in a way that decoding it produces too big output.
 *
 * If single-call decoding fails, we reset b->in_pos and b->out_pos back to
 * their original values. This is because with some filter chains there won't
 * be any valid uncompressed data in the output buffer unless the decoding
 * actually succeeds (that's the price to pay of using the output buffer as
 * the workspace).
 */
XZ_EXTERN enum xz_ret xz_dec_run(struct xz_dec *s, struct xz_buf *b)
{
    size_t in_start;
    size_t out_start;
    enum xz_ret ret;

    if (DEC_IS_SINGLE(s->mode))
        xz_dec_reset(s);

    in_start = b->in_pos;
    out_start = b->out_pos;
    ret = dec_main(s, b);

    if (DEC_IS_SINGLE(s->mode)) {
        if (ret == XZ_OK)
            ret = b->in_pos == b->in_size
                    ? XZ_DATA_ERROR : XZ_BUF_ERROR;

        if (ret != XZ_STREAM_END) {
            b->in_pos = in_start;
            b->out_pos = out_start;
        }

    } else if (ret == XZ_OK && in_start == b->in_pos
            && out_start == b->out_pos) {
        if (s->allow_buf_error)
            ret = XZ_BUF_ERROR;

        s->allow_buf_error = true;
    } else {
        s->allow_buf_error = false;
    }

    return ret;
}

XZ_EXTERN struct xz_dec *xz_dec_init(enum xz_mode mode, uint32_t dict_max)
{
    struct xz_dec *s = kmalloc(sizeof(*s), GFP_KERNEL);
    if (s == NULL)
        return NULL;

    s->mode = mode;

#ifdef XZ_DEC_BCJ
    s->bcj = xz_dec_bcj_create(DEC_IS_SINGLE(mode));
    if (s->bcj == NULL)
        goto error_bcj;
#endif

    s->lzma2 = xz_dec_lzma2_create(mode, dict_max);
    if (s->lzma2 == NULL)
        goto error_lzma2;

    xz_dec_reset(s);
    return s;

error_lzma2:
#ifdef XZ_DEC_BCJ
    xz_dec_bcj_end(s->bcj);
error_bcj:
#endif
    kfree(s);
    return NULL;
}

XZ_EXTERN void xz_dec_reset(struct xz_dec *s)
{
    s->sequence = SEQ_STREAM_HEADER;
    s->allow_buf_error = false;
    s->pos = 0;
    s->crc32 = 0;
    memzero(&s->block, sizeof(s->block));
    memzero(&s->index, sizeof(s->index));
    s->temp.pos = 0;
    s->temp.size = STREAM_HEADER_SIZE;
}

XZ_EXTERN void xz_dec_end(struct xz_dec *s)
{
    if (s != NULL) {
        xz_dec_lzma2_end(s->lzma2);
#ifdef XZ_DEC_BCJ
        xz_dec_bcj_end(s->bcj);
#endif
        kfree(s);
    }
}
//xz_dec_stream.c end

//decompress_unxz.c start
#define XZ_INTERNAL_CRC32 1

/*
 * For boot time use, we enable only the BCJ filter of the current
 * architecture or none if no BCJ filter is available for the architecture.
 */
#ifdef CONFIG_X86
#	define XZ_DEC_X86
#endif
#ifdef CONFIG_PPC
#	define XZ_DEC_POWERPC
#endif
#ifdef CONFIG_ARM
#	define XZ_DEC_ARM
#endif
#ifdef CONFIG_IA64
#	define XZ_DEC_IA64
#endif
#ifdef CONFIG_SPARC
#	define XZ_DEC_SPARC
#endif

/*
 * This will get the basic headers so that memeq() and others
 * can be defined.
 */

/*
 * Replace the normal allocation functions with the versions from
 * <linux/decompress/mm.h>. vfree() needs to support vfree(NULL)
 * when XZ_DYNALLOC is used, but the pre-boot free() doesn't support it.
 * Workaround it here because the other decompressors don't need it.
 */
#undef kmalloc
#undef kfree
#undef vmalloc
#undef vfree
#define kmalloc(size, flags) malloc(size)
#define kfree(ptr) free(ptr)
#define vmalloc(size) malloc(size)
#define vfree(ptr) do { if (ptr != NULL) free(ptr); } while (0)

/*
 * FIXME: Not all basic memory functions are provided in architecture-specific
 * files (yet). We define our own versions here for now, but this should be
 * only a temporary solution.
 *
 * memeq and memzero are not used much and any remotely sane implementation
 * is fast enough. memcpy/memmove speed matters in multi-call mode, but
 * the kernel image is decompressed in single-call mode, in which only
 * memcpy speed can matter and only if there is a lot of uncompressible data
 * (LZMA2 stores uncompressible chunks in uncompressed form). Thus, the
 * functions below should just be kept small; it's probably not worth
 * optimizing for speed.
 */

#ifndef memeq
static bool memeq(const void *a, const void *b, size_t size)
{
    const uint8_t *x = a;
    const uint8_t *y = b;
    size_t i;

    for (i = 0; i < size; ++i)
        if (x[i] != y[i])
            return false;

    return true;
}
#endif

#ifndef memzero
static void memzero(void *buf, size_t size)
{
    uint8_t *b = buf;
    uint8_t *e = b + size;

    while (b != e)
        *b++ = '\0';
}
#endif

#ifndef memmove
/* Not static to avoid a conflict with the prototype in the Linux headers. */
void *memmove(void *dest, const void *src, size_t size)
{
    uint8_t *d = dest;
    const uint8_t *s = src;
    size_t i;

    if (d < s) {
        for (i = 0; i < size; ++i)
            d[i] = s[i];
    } else if (d > s) {
        i = size;
        while (i-- > 0)
            d[i] = s[i];
    }

    return dest;
}
#endif

/*
 * Since we need memmove anyway, would use it as memcpy too.
 * Commented out for now to avoid breaking things.
 */
/*
#ifndef memcpy
#	define memcpy memmove
#endif
*/




/* Size of the input and output buffers in multi-call mode */
#define XZ_IOBUF_SIZE 4096

/*
 * This function implements the API defined in <linux/decompress/generic.h>.
 *
 * This wrapper will automatically choose single-call or multi-call mode
 * of the native XZ decoder API. The single-call mode can be used only when
 * both input and output buffers are available as a single chunk, i.e. when
 * fill() and flush() won't be used.
 */
int unxz(unsigned char *in, int in_size,
             int (*fill)(void *dest, unsigned int size),
             int (*flush)(void *src, unsigned int size),
             unsigned char *out, int *in_used,
             void (*error)(char *x))
{
    struct xz_buf b;
    struct xz_dec *s;
    enum xz_ret ret;
    bool must_free_in = false;

#if XZ_INTERNAL_CRC32
    xz_crc32_init();
#endif

    if (in_used != NULL)
        *in_used = 0;

    if (fill == NULL && flush == NULL)
        s = xz_dec_init(XZ_SINGLE, 0);
    else
        s = xz_dec_init(XZ_DYNALLOC, (uint32_t)-1);

    if (s == NULL)
        goto error_alloc_state;

    if (flush == NULL) {
        b.out = out;
        b.out_size = (size_t)-1;
    } else {
        b.out_size = XZ_IOBUF_SIZE;
        b.out = malloc(XZ_IOBUF_SIZE);
        if (b.out == NULL)
            goto error_alloc_out;
    }

    if (in == NULL) {
        must_free_in = true;
        in = malloc(XZ_IOBUF_SIZE);
        if (in == NULL)
            goto error_alloc_in;
    }

    b.in = in;
    b.in_pos = 0;
    b.in_size = in_size;
    b.out_pos = 0;

    if (fill == NULL && flush == NULL) {
        ret = xz_dec_run(s, &b);
    } else {
        do {
            if (b.in_pos == b.in_size && fill != NULL) {
                if (in_used != NULL)
                    *in_used += b.in_pos;

                b.in_pos = 0;

                in_size = fill(in, XZ_IOBUF_SIZE);
                if (in_size < 0) {
                    /*
                     * This isn't an optimal error code
                     * but it probably isn't worth making
                     * a new one either.
                     */
                    ret = XZ_BUF_ERROR;
                    break;
                }

                b.in_size = in_size;
            }

            ret = xz_dec_run(s, &b);

            if (flush != NULL && (b.out_pos == b.out_size
                    || (ret != XZ_OK && b.out_pos > 0))) {
                /*
                 * Setting ret here may hide an error
                 * returned by xz_dec_run(), but probably
                 * it's not too bad.
                 */
                if (flush(b.out, b.out_pos) != (int)b.out_pos)
                    ret = XZ_BUF_ERROR;

                b.out_pos = 0;
            }
        } while (ret == XZ_OK);

        if (must_free_in)
            free(in);

        if (flush != NULL)
            free(b.out);
    }

    if (in_used != NULL)
        *in_used += b.in_pos;

    xz_dec_end(s);

    switch (ret) {
    case XZ_STREAM_END:
        return 0;

    case XZ_MEM_ERROR:
        /* This can occur only in multi-call mode. */
        error("XZ decompressor ran out of memory");
        break;

    case XZ_FORMAT_ERROR:
        error("Input is not in the XZ format (wrong magic bytes)");
        break;

    case XZ_OPTIONS_ERROR:
        error("Input was encoded with settings that are not "
                "supported by this XZ decoder");
        break;

    case XZ_DATA_ERROR:
    case XZ_BUF_ERROR:
        error("XZ-compressed data is corrupt");
        break;

    default:
        error("Bug in the XZ decompressor");
        break;
    }

    return -1;

error_alloc_in:
    if (flush != NULL)
        free(b.out);

error_alloc_out:
    xz_dec_end(s);

error_alloc_state:
    error("XZ decompressor ran out of memory");
    return -1;
}

//decompress_unxz.c end

// XZ END


// LZO BEGIN
#define LZO1X_1_MEM_COMPRESS	(8192 * sizeof(unsigned short))
#define LZO1X_MEM_COMPRESS	LZO1X_1_MEM_COMPRESS

#define lzo1x_worst_compress(x) ((x) + ((x) / 16) + 64 + 3 + 2)

/* This requires 'wrkmem' of size LZO1X_1_MEM_COMPRESS */
int lzo1x_1_compress(const unsigned char *src, size_t src_len,
             unsigned char *dst, size_t *dst_len, void *wrkmem);

/* This requires 'wrkmem' of size LZO1X_1_MEM_COMPRESS */
int lzorle1x_1_compress(const unsigned char *src, size_t src_len,
             unsigned char *dst, size_t *dst_len, void *wrkmem);

/* safe decompression with overrun testing */
int lzo1x_decompress_safe(const unsigned char *src, size_t src_len,
              unsigned char *dst, size_t *dst_len);

/*
 * Return values (< 0 = Error)
 */
#define LZO_E_OK			0
#define LZO_E_ERROR			(-1)
#define LZO_E_OUT_OF_MEMORY		(-2)
#define LZO_E_NOT_COMPRESSIBLE		(-3)
#define LZO_E_INPUT_OVERRUN		(-4)
#define LZO_E_OUTPUT_OVERRUN		(-5)
#define LZO_E_LOOKBEHIND_OVERRUN	(-6)
#define LZO_E_EOF_NOT_FOUND		(-7)
#define LZO_E_INPUT_NOT_CONSUMED	(-8)
#define LZO_E_NOT_YET_IMPLEMENTED	(-9)
#define LZO_E_INVALID_ARGUMENT		(-10)

static const unsigned char lzop_magic[] = {
    0x89, 0x4c, 0x5a, 0x4f, 0x00, 0x0d, 0x0a, 0x1a, 0x0a };

#define LZO_BLOCK_SIZE        (256*1024l)
#define HEADER_HAS_FILTER      0x00000800L
#define HEADER_SIZE_MIN       (9 + 7     + 4 + 8     + 1       + 4)
#define HEADER_SIZE_MAX       (9 + 7 + 1 + 8 + 8 + 4 + 1 + 255 + 4)


#define HAVE_IP(x)      ((size_t)(ip_end - ip) >= (size_t)(x))
#define HAVE_OP(x)      ((size_t)(op_end - op) >= (size_t)(x))
#define NEED_IP(x)      if (!HAVE_IP(x)) goto input_overrun
#define NEED_OP(x)      if (!HAVE_OP(x)) goto output_overrun
#define TEST_LB(m_pos)  if ((m_pos) < out) goto lookbehind_overrun

#define LZO_VERSION 1

#define COPY4(dst, src)	\
        put_unaligned(get_unaligned((const u32 *)(src)), (u32 *)(dst))
#if defined(CONFIG_X86_64) || defined(CONFIG_ARM64)
#define COPY8(dst, src)	\
        put_unaligned(get_unaligned((const u64 *)(src)), (u64 *)(dst))
#else
#define COPY8(dst, src)	\
        COPY4(dst, src); COPY4((dst) + 4, (src) + 4)
#endif

#define LZO_USE_CTZ64	1


#define M1_MAX_OFFSET	0x0400
#define M2_MAX_OFFSET	0x0800
#define M3_MAX_OFFSET	0x4000
#define M4_MAX_OFFSET_V0	0xbfff
#define M4_MAX_OFFSET_V1	0xbffe

#define M1_MIN_LEN	2
#define M1_MAX_LEN	2
#define M2_MIN_LEN	3
#define M2_MAX_LEN	8
#define M3_MIN_LEN	3
#define M3_MAX_LEN	33
#define M4_MIN_LEN	3
#define M4_MAX_LEN	9

#define M1_MARKER	0
#define M2_MARKER	64
#define M3_MARKER	32
#define M4_MARKER	16

#define MIN_ZERO_RUN_LENGTH	4
#define MAX_ZERO_RUN_LENGTH	(2047 + MIN_ZERO_RUN_LENGTH)

#define lzo_dict_t      unsigned short
#define D_BITS		13
#define D_SIZE		(1u << D_BITS)
#define D_MASK		(D_SIZE - 1)
#define D_HIGH		((D_MASK >> 1) + 1)

/* This MAX_255_COUNT is the maximum number of times we can add 255 to a base
 * count without overflowing an integer. The multiply will overflow when
 * multiplying 255 by more than MAXINT/255. The sum will overflow earlier
 * depending on the base count. Since the base count is taken from a u8
 * and a few bits, it is safe to assume that it will always be lower than
 * or equal to 2*255, thus we can always prevent any overflow by accepting
 * two less 255 steps. See Documentation/lzo.txt for more information.
 */
#define MAX_255_COUNT      ((((size_t)~0) / 255) - 2)


static inline uint16_t __get_unaligned_be16(const uint8_t *p)
{
    return p[0] << 8 | p[1];
}

static inline uint16_t get_unaligned_be16(const void *p)
{
    return __get_unaligned_be16((const uint8_t *)p);
}

static inline uint16_t __get_unaligned_le16(const uint8_t *p)
{
    return p[0] | p[1] << 8;
}

static inline uint16_t get_unaligned_le16(const void *p)
{
    return __get_unaligned_le16((const uint8_t *)p);
}

int lzo1x_decompress_safe(const unsigned char *in, size_t in_len,
              unsigned char *out, size_t *out_len)
{
    unsigned char *op;
    const unsigned char *ip;
    size_t t, next;
    size_t state = 0;
    const unsigned char *m_pos;
    const unsigned char * const ip_end = in + in_len;
    unsigned char * const op_end = out + *out_len;

    unsigned char bitstream_version;

    op = out;
    ip = in;

    if (in_len < 3)
        goto input_overrun;

    if (in_len >= 5 && *ip == 17) {
        bitstream_version = ip[1];
        ip += 2;
    } else {
        bitstream_version = 0;
    }

    if (*ip > 17) {
        t = *ip++ - 17;
        if (t < 4) {
            next = t;
            goto match_next;
        }
        goto copy_literal_run;
    }

    for (;;) {
        t = *ip++;
        if (t < 16) {
            if (state == 0) {
                if (t == 0) {
                    size_t offset;
                    const unsigned char *ip_last = ip;

                    while (*ip == 0) {
                        ip++;
                        NEED_IP(1);
                    }
                    offset = ip - ip_last;
                    if (offset > MAX_255_COUNT)
                        return LZO_E_ERROR;

                    offset = (offset << 8) - offset;
                    t += offset + 15 + *ip++;
                }
                t += 3;
copy_literal_run:
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
                if (likely(HAVE_IP(t + 15) && HAVE_OP(t + 15))) {
                    const unsigned char *ie = ip + t;
                    unsigned char *oe = op + t;
                    do {
                        COPY8(op, ip);
                        op += 8;
                        ip += 8;
                        COPY8(op, ip);
                        op += 8;
                        ip += 8;
                    } while (ip < ie);
                    ip = ie;
                    op = oe;
                } else
#endif
                {
                    NEED_OP(t);
                    NEED_IP(t + 3);
                    do {
                        *op++ = *ip++;
                    } while (--t > 0);
                }
                state = 4;
                continue;
            } else if (state != 4) {
                next = t & 3;
                m_pos = op - 1;
                m_pos -= t >> 2;
                m_pos -= *ip++ << 2;
                TEST_LB(m_pos);
                NEED_OP(2);
                op[0] = m_pos[0];
                op[1] = m_pos[1];
                op += 2;
                goto match_next;
            } else {
                next = t & 3;
                m_pos = op - (1 + M2_MAX_OFFSET);
                m_pos -= t >> 2;
                m_pos -= *ip++ << 2;
                t = 3;
            }
        } else if (t >= 64) {
            next = t & 3;
            m_pos = op - 1;
            m_pos -= (t >> 2) & 7;
            m_pos -= *ip++ << 3;
            t = (t >> 5) - 1 + (3 - 1);
        } else if (t >= 32) {
            t = (t & 31) + (3 - 1);
            if (t == 2) {
                size_t offset;
                const unsigned char *ip_last = ip;

                while (*ip == 0) {
                    ip++;
                    NEED_IP(1);
                }
                offset = ip - ip_last;
                if (offset > MAX_255_COUNT)
                    return LZO_E_ERROR;

                offset = (offset << 8) - offset;
                t += offset + 31 + *ip++;
                NEED_IP(2);
            }
            m_pos = op - 1;
            next = get_unaligned_le16(ip);
            ip += 2;
            m_pos -= next >> 2;
            next &= 3;
        } else {
            NEED_IP(2);
            next = get_unaligned_le16(ip);
            if (((next & 0xfffc) == 0xfffc) &&
                ((t & 0xf8) == 0x18) &&
                bitstream_version) {
                NEED_IP(3);
                t &= 7;
                t |= ip[2] << 3;
                t += MIN_ZERO_RUN_LENGTH;
                NEED_OP(t);
                memset(op, 0, t);
                op += t;
                next &= 3;
                ip += 3;
                goto match_next;
            } else {
                m_pos = op;
                m_pos -= (t & 8) << 11;
                t = (t & 7) + (3 - 1);
                if (t == 2) {
                    size_t offset;
                    const unsigned char *ip_last = ip;

                    while (*ip == 0) {
                        ip++;
                        NEED_IP(1);
                    }
                    offset = ip - ip_last;
                    if (offset > MAX_255_COUNT)
                        return LZO_E_ERROR;

                    offset = (offset << 8) - offset;
                    t += offset + 7 + *ip++;
                    NEED_IP(2);
                    next = get_unaligned_le16(ip);
                }
                ip += 2;
                m_pos -= next >> 2;
                next &= 3;
                if (m_pos == op)
                    goto eof_found;
                m_pos -= 0x4000;
            }
        }
        TEST_LB(m_pos);
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
        if (op - m_pos >= 8) {
            unsigned char *oe = op + t;
            if (likely(HAVE_OP(t + 15))) {
                do {
                    COPY8(op, m_pos);
                    op += 8;
                    m_pos += 8;
                    COPY8(op, m_pos);
                    op += 8;
                    m_pos += 8;
                } while (op < oe);
                op = oe;
                if (HAVE_IP(6)) {
                    state = next;
                    COPY4(op, ip);
                    op += next;
                    ip += next;
                    continue;
                }
            } else {
                NEED_OP(t);
                do {
                    *op++ = *m_pos++;
                } while (op < oe);
            }
        } else
#endif
        {
            unsigned char *oe = op + t;
            NEED_OP(t);
            op[0] = m_pos[0];
            op[1] = m_pos[1];
            op += 2;
            m_pos += 2;
            do {
                *op++ = *m_pos++;
            } while (op < oe);
        }
match_next:
        state = next;
        t = next;
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
        if (likely(HAVE_IP(6) && HAVE_OP(4))) {
            COPY4(op, ip);
            op += t;
            ip += t;
        } else
#endif
        {
            NEED_IP(t + 3);
            NEED_OP(t);
            while (t > 0) {
                *op++ = *ip++;
                t--;
            }
        }
    }

eof_found:
    *out_len = op - out;
    return (t != 3       ? LZO_E_ERROR :
        ip == ip_end ? LZO_E_OK :
        ip <  ip_end ? LZO_E_INPUT_NOT_CONSUMED : LZO_E_INPUT_OVERRUN);

input_overrun:
    *out_len = op - out;
    return LZO_E_INPUT_OVERRUN;

output_overrun:
    *out_len = op - out;
    return LZO_E_OUTPUT_OVERRUN;

lookbehind_overrun:
    *out_len = op - out;
    return LZO_E_LOOKBEHIND_OVERRUN;
}

static inline long parse_header(u8 *input, long *skip, long in_len)
{
    int l;
    u8 *parse = input;
    u8 *end = input + in_len;
    u16 version;

    /*
     * Check that there's enough input to possibly have a valid header.
     * Then it is possible to parse several fields until the minimum
     * size may have been used.
     */
    if (in_len < HEADER_SIZE_MIN)
        return 0;

    /* read magic: 9 first bits */
    for (l = 0; l < 9; l++) {
        if (*parse++ != lzop_magic[l])
            return 0;
    }
    /* get version (2bytes), skip library version (2),
     * 'need to be extracted' version (2) and
     * method (1) */
    version = get_unaligned_be16(parse);
    parse += 7;
    if (version >= 0x0940)
        parse++;
    if (get_unaligned_be32(parse) & HEADER_HAS_FILTER)
        parse += 8; /* flags + filter info */
    else
        parse += 4; /* flags */

    /*
     * At least mode, mtime_low, filename length, and checksum must
     * be left to be parsed. If also mtime_high is present, it's OK
     * because the next input buffer check is after reading the
     * filename length.
     */
    if (end - parse < 8 + 1 + 4)
        return 0;

    /* skip mode and mtime_low */
    parse += 8;
    if (version >= 0x0940)
        parse += 4;	/* skip mtime_high */

    l = *parse++;
    /* don't care about the file name, and skip checksum */
    if (end - parse < l + 4)
        return 0;
    parse += l + 4;

    *skip = parse - input;
    return 1;
}

int unlzo(u8 *input, long in_len,
                long (*fill)(void *, unsigned long),
                long (*flush)(void *, unsigned long),
                u8 *output, long *posp,
                void (*error) (char *x))
{
    u8 r = 0;
    long skip = 0;
    u32 src_len, dst_len;
    size_t tmp;
    u8 *in_buf, *in_buf_save, *out_buf;
    int ret = -1;

    if (output) {
        out_buf = output;
    } else if (!flush) {
        error("NULL output pointer and no flush function provided");
        goto exit;
    } else {
        out_buf = malloc(LZO_BLOCK_SIZE);
        if (!out_buf) {
            error("Could not allocate output buffer");
            goto exit;
        }
    }

    if (input && fill) {
        error("Both input pointer and fill function provided, don't know what to do");
        goto exit_1;
    } else if (input) {
        in_buf = input;
    } else if (!fill) {
        error("NULL input pointer and missing fill function");
        goto exit_1;
    } else {
        in_buf = malloc(lzo1x_worst_compress(LZO_BLOCK_SIZE));
        if (!in_buf) {
            error("Could not allocate input buffer");
            goto exit_1;
        }
    }
    in_buf_save = in_buf;

    if (posp)
        *posp = 0;

    if (fill) {
        /*
         * Start from in_buf + HEADER_SIZE_MAX to make it possible
         * to use memcpy() to copy the unused data to the beginning
         * of the buffer. This way memmove() isn't needed which
         * is missing from pre-boot environments of most archs.
         */
        in_buf += HEADER_SIZE_MAX;
        in_len = fill(in_buf, HEADER_SIZE_MAX);
    }

    if (!parse_header(in_buf, &skip, in_len)) {
        error("invalid header");
        goto exit_2;
    }
    in_buf += skip;
    in_len -= skip;

    if (fill) {
        /* Move the unused data to the beginning of the buffer. */
        memcpy(in_buf_save, in_buf, in_len);
        in_buf = in_buf_save;
    }

    if (posp)
        *posp = skip;

    for (;;) {
        /* read uncompressed block size */
        if (fill && in_len < 4) {
            skip = fill(in_buf + in_len, 4 - in_len);
            if (skip > 0)
                in_len += skip;
        }
        if (in_len < 4) {
            error("file corrupted");
            goto exit_2;
        }
        dst_len = get_unaligned_be32(in_buf);
        in_buf += 4;
        in_len -= 4;

        /* exit if last block */
        if (dst_len == 0) {
            if (posp)
                *posp += 4;
            break;
        }

        if (dst_len > LZO_BLOCK_SIZE) {
            error("dest len longer than block size");
            goto exit_2;
        }

        /* read compressed block size, and skip block checksum info */
        if (fill && in_len < 8) {
            skip = fill(in_buf + in_len, 8 - in_len);
            if (skip > 0)
                in_len += skip;
        }
        if (in_len < 8) {
            error("file corrupted");
            goto exit_2;
        }
        src_len = get_unaligned_be32(in_buf);
        in_buf += 8;
        in_len -= 8;

        if (src_len <= 0 || src_len > dst_len) {
            error("file corrupted");
            goto exit_2;
        }

        /* decompress */
        if (fill && in_len < src_len) {
            skip = fill(in_buf + in_len, src_len - in_len);
            if (skip > 0)
                in_len += skip;
        }
        if (in_len < src_len) {
            error("file corrupted");
            goto exit_2;
        }
        tmp = dst_len;

        /* When the input data is not compressed at all,
         * lzo1x_decompress_safe will fail, so call memcpy()
         * instead */
        if (dst_len == src_len)
            memcpy(out_buf, in_buf, src_len);
        else {
            r = lzo1x_decompress_safe((u8 *) in_buf, src_len,
                        out_buf, &tmp);

            if (r != LZO_E_OK || dst_len != tmp) {
                error("Compressed data violation");
                goto exit_2;
            }
        }

        if (flush && flush(out_buf, dst_len) != dst_len)
            goto exit_2;
        if (output)
            out_buf += dst_len;
        if (posp)
            *posp += src_len + 12;

        in_buf += src_len;
        in_len -= src_len;
        if (fill) {
            /*
             * If there happens to still be unused data left in
             * in_buf, move it to the beginning of the buffer.
             * Use a loop to avoid memmove() dependency.
             */
            if (in_len > 0)
                for (skip = 0; skip < in_len; ++skip)
                    in_buf_save[skip] = in_buf[skip];
            in_buf = in_buf_save;
        }
    }

    ret = 0;
exit_2:
    if (!input)
        free(in_buf_save);
exit_1:
    if (!output)
        free(out_buf);
exit:
    return ret;
}
// END LZO

// BEGIN BUNZIP2

#define CRC32_POLY_BE 0x04c11db7

#define large_malloc(a) malloc(a)
#define large_free(a) free(a)

#ifndef INT_MAX
#define INT_MAX 0x7fffffff
#endif

/* Constants for Huffman coding */
#define MAX_GROUPS		6
#define GROUP_SIZE   		50	/* 64 would have been more efficient */
#define MAX_HUFCODE_BITS 	20	/* Longest Huffman code allowed */
#define MAX_SYMBOLS 		258	/* 256 literals + RUNA + RUNB */
#define SYMBOL_RUNA		0
#define SYMBOL_RUNB		1

/* Status return values */
#define RETVAL_OK			0
#define RETVAL_LAST_BLOCK		(-1)
#define RETVAL_NOT_BZIP_DATA		(-2)
#define RETVAL_UNEXPECTED_INPUT_EOF	(-3)
#define RETVAL_UNEXPECTED_OUTPUT_EOF	(-4)
#define RETVAL_DATA_ERROR		(-5)
#define RETVAL_OUT_OF_MEMORY		(-6)
#define RETVAL_OBSOLETE_INPUT		(-7)

/* Other housekeeping constants */
#define BZIP2_IOBUF_SIZE		4096

/* This is what we know about each Huffman coding group */
struct group_data {
    /* We have an extra slot at the end of limit[] for a sentinal value. */
    int limit[MAX_HUFCODE_BITS+1];
    int base[MAX_HUFCODE_BITS];
    int permute[MAX_SYMBOLS];
    int minLen, maxLen;
};

/* Structure holding all the housekeeping data, including IO buffers and
   memory that persists between calls to bunzip */
struct bunzip_data {
    /* State for interrupting output loop */
    int writeCopies, writePos, writeRunCountdown, writeCount, writeCurrent;
    /* I/O tracking data (file handles, buffers, positions, etc.) */
    long (*fill)(void*, unsigned long);
    long inbufCount, inbufPos /*, outbufPos*/;
    unsigned char *inbuf /*,*outbuf*/;
    unsigned int inbufBitCount, inbufBits;
    /* The CRC values stored in the block header and calculated from the
    data */
    unsigned int crc32Table[256], headerCRC, totalCRC, writeCRC;
    /* Intermediate buffer and its size (in bytes) */
    unsigned int *dbuf, dbufSize;
    /* These things are a bit too big to go on the stack */
    unsigned char selectors[32768];		/* nSelectors = 15 bits */
    struct group_data groups[MAX_GROUPS];	/* Huffman coding tables */
    int io_error;			/* non-zero if we have IO error */
    int byteCount[256];
    unsigned char symToByte[256], mtfSymbol[256];
};


/* Return the next nnn bits of input.  All reads from the compressed input
   are done through this function.  All reads are big endian */
static unsigned int get_bits(struct bunzip_data *bd, char bits_wanted)
{
    unsigned int bits = 0;

    /* If we need to get more data from the byte buffer, do so.
       (Loop getting one byte at a time to enforce endianness and avoid
       unaligned access.) */
    while (bd->inbufBitCount < bits_wanted) {
        /* If we need to read more data from file into byte buffer, do
           so */
        if (bd->inbufPos == bd->inbufCount) {
            if (bd->io_error)
                return 0;
            bd->inbufCount = bd->fill(bd->inbuf, BZIP2_IOBUF_SIZE);
            if (bd->inbufCount <= 0) {
                bd->io_error = RETVAL_UNEXPECTED_INPUT_EOF;
                return 0;
            }
            bd->inbufPos = 0;
        }
        /* Avoid 32-bit overflow (dump bit buffer to top of output) */
        if (bd->inbufBitCount >= 24) {
            bits = bd->inbufBits&((1 << bd->inbufBitCount)-1);
            bits_wanted -= bd->inbufBitCount;
            bits <<= bits_wanted;
            bd->inbufBitCount = 0;
        }
        /* Grab next 8 bits of input from buffer. */
        bd->inbufBits = (bd->inbufBits << 8)|bd->inbuf[bd->inbufPos++];
        bd->inbufBitCount += 8;
    }
    /* Calculate result */
    bd->inbufBitCount -= bits_wanted;
    bits |= (bd->inbufBits >> bd->inbufBitCount)&((1 << bits_wanted)-1);

    return bits;
}

/* Unpacks the next block and sets up for the inverse burrows-wheeler step. */

static int get_next_block(struct bunzip_data *bd)
{
    struct group_data *hufGroup = NULL;
    int *base = NULL;
    int *limit = NULL;
    int dbufCount, nextSym, dbufSize, groupCount, selector,
        i, j, k, t, runPos, symCount, symTotal, nSelectors, *byteCount;
    unsigned char uc, *symToByte, *mtfSymbol, *selectors;
    unsigned int *dbuf, origPtr;

    dbuf = bd->dbuf;
    dbufSize = bd->dbufSize;
    selectors = bd->selectors;
    byteCount = bd->byteCount;
    symToByte = bd->symToByte;
    mtfSymbol = bd->mtfSymbol;

    /* Read in header signature and CRC, then validate signature.
       (last block signature means CRC is for whole file, return now) */
    i = get_bits(bd, 24);
    j = get_bits(bd, 24);
    bd->headerCRC = get_bits(bd, 32);
    if ((i == 0x177245) && (j == 0x385090))
        return RETVAL_LAST_BLOCK;
    if ((i != 0x314159) || (j != 0x265359))
        return RETVAL_NOT_BZIP_DATA;
    /* We can add support for blockRandomised if anybody complains.
       There was some code for this in busybox 1.0.0-pre3, but nobody ever
       noticed that it didn't actually work. */
    if (get_bits(bd, 1))
        return RETVAL_OBSOLETE_INPUT;
    origPtr = get_bits(bd, 24);
    if (origPtr >= dbufSize)
        return RETVAL_DATA_ERROR;
    /* mapping table: if some byte values are never used (encoding things
       like ascii text), the compression code removes the gaps to have fewer
       symbols to deal with, and writes a sparse bitfield indicating which
       values were present.  We make a translation table to convert the
       symbols back to the corresponding bytes. */
    t = get_bits(bd, 16);
    symTotal = 0;
    for (i = 0; i < 16; i++) {
        if (t&(1 << (15-i))) {
            k = get_bits(bd, 16);
            for (j = 0; j < 16; j++)
                if (k&(1 << (15-j)))
                    symToByte[symTotal++] = (16*i)+j;
        }
    }
    /* How many different Huffman coding groups does this block use? */
    groupCount = get_bits(bd, 3);
    if (groupCount < 2 || groupCount > MAX_GROUPS)
        return RETVAL_DATA_ERROR;
    /* nSelectors: Every GROUP_SIZE many symbols we select a new
       Huffman coding group.  Read in the group selector list,
       which is stored as MTF encoded bit runs.  (MTF = Move To
       Front, as each value is used it's moved to the start of the
       list.) */
    nSelectors = get_bits(bd, 15);
    if (!nSelectors)
        return RETVAL_DATA_ERROR;
    for (i = 0; i < groupCount; i++)
        mtfSymbol[i] = i;
    for (i = 0; i < nSelectors; i++) {
        /* Get next value */
        for (j = 0; get_bits(bd, 1); j++)
            if (j >= groupCount)
                return RETVAL_DATA_ERROR;
        /* Decode MTF to get the next selector */
        uc = mtfSymbol[j];
        for (; j; j--)
            mtfSymbol[j] = mtfSymbol[j-1];
        mtfSymbol[0] = selectors[i] = uc;
    }
    /* Read the Huffman coding tables for each group, which code
       for symTotal literal symbols, plus two run symbols (RUNA,
       RUNB) */
    symCount = symTotal+2;
    for (j = 0; j < groupCount; j++) {
        unsigned char length[MAX_SYMBOLS], temp[MAX_HUFCODE_BITS+1];
        int	minLen,	maxLen, pp;
        /* Read Huffman code lengths for each symbol.  They're
           stored in a way similar to mtf; record a starting
           value for the first symbol, and an offset from the
           previous value for everys symbol after that.
           (Subtracting 1 before the loop and then adding it
           back at the end is an optimization that makes the
           test inside the loop simpler: symbol length 0
           becomes negative, so an unsigned inequality catches
           it.) */
        t = get_bits(bd, 5)-1;
        for (i = 0; i < symCount; i++) {
            for (;;) {
                if (((unsigned)t) > (MAX_HUFCODE_BITS-1))
                    return RETVAL_DATA_ERROR;

                /* If first bit is 0, stop.  Else
                   second bit indicates whether to
                   increment or decrement the value.
                   Optimization: grab 2 bits and unget
                   the second if the first was 0. */

                k = get_bits(bd, 2);
                if (k < 2) {
                    bd->inbufBitCount++;
                    break;
                }
                /* Add one if second bit 1, else
                 * subtract 1.  Avoids if/else */
                t += (((k+1)&2)-1);
            }
            /* Correct for the initial -1, to get the
             * final symbol length */
            length[i] = t+1;
        }
        /* Find largest and smallest lengths in this group */
        minLen = maxLen = length[0];

        for (i = 1; i < symCount; i++) {
            if (length[i] > maxLen)
                maxLen = length[i];
            else if (length[i] < minLen)
                minLen = length[i];
        }

        /* Calculate permute[], base[], and limit[] tables from
         * length[].
         *
         * permute[] is the lookup table for converting
         * Huffman coded symbols into decoded symbols.  base[]
         * is the amount to subtract from the value of a
         * Huffman symbol of a given length when using
         * permute[].
         *
         * limit[] indicates the largest numerical value a
         * symbol with a given number of bits can have.  This
         * is how the Huffman codes can vary in length: each
         * code with a value > limit[length] needs another
         * bit.
         */
        hufGroup = bd->groups+j;
        hufGroup->minLen = minLen;
        hufGroup->maxLen = maxLen;
        /* Note that minLen can't be smaller than 1, so we
           adjust the base and limit array pointers so we're
           not always wasting the first entry.  We do this
           again when using them (during symbol decoding).*/
        base = hufGroup->base-1;
        limit = hufGroup->limit-1;
        /* Calculate permute[].  Concurrently, initialize
         * temp[] and limit[]. */
        pp = 0;
        for (i = minLen; i <= maxLen; i++) {
            temp[i] = limit[i] = 0;
            for (t = 0; t < symCount; t++)
                if (length[t] == i)
                    hufGroup->permute[pp++] = t;
        }
        /* Count symbols coded for at each bit length */
        for (i = 0; i < symCount; i++)
            temp[length[i]]++;
        /* Calculate limit[] (the largest symbol-coding value
         *at each bit length, which is (previous limit <<
         *1)+symbols at this level), and base[] (number of
         *symbols to ignore at each bit length, which is limit
         *minus the cumulative count of symbols coded for
         *already). */
        pp = t = 0;
        for (i = minLen; i < maxLen; i++) {
            pp += temp[i];
            /* We read the largest possible symbol size
               and then unget bits after determining how
               many we need, and those extra bits could be
               set to anything.  (They're noise from
               future symbols.)  At each level we're
               really only interested in the first few
               bits, so here we set all the trailing
               to-be-ignored bits to 1 so they don't
               affect the value > limit[length]
               comparison. */
            limit[i] = (pp << (maxLen - i)) - 1;
            pp <<= 1;
            base[i+1] = pp-(t += temp[i]);
        }
        limit[maxLen+1] = INT_MAX; /* Sentinal value for
                        * reading next sym. */
        limit[maxLen] = pp+temp[maxLen]-1;
        base[minLen] = 0;
    }
    /* We've finished reading and digesting the block header.  Now
       read this block's Huffman coded symbols from the file and
       undo the Huffman coding and run length encoding, saving the
       result into dbuf[dbufCount++] = uc */

    /* Initialize symbol occurrence counters and symbol Move To
     * Front table */
    for (i = 0; i < 256; i++) {
        byteCount[i] = 0;
        mtfSymbol[i] = (unsigned char)i;
    }
    /* Loop through compressed symbols. */
    runPos = dbufCount = symCount = selector = 0;
    for (;;) {
        /* Determine which Huffman coding group to use. */
        if (!(symCount--)) {
            symCount = GROUP_SIZE-1;
            if (selector >= nSelectors)
                return RETVAL_DATA_ERROR;
            hufGroup = bd->groups+selectors[selector++];
            base = hufGroup->base-1;
            limit = hufGroup->limit-1;
        }
        /* Read next Huffman-coded symbol. */
        /* Note: It is far cheaper to read maxLen bits and
           back up than it is to read minLen bits and then an
           additional bit at a time, testing as we go.
           Because there is a trailing last block (with file
           CRC), there is no danger of the overread causing an
           unexpected EOF for a valid compressed file.  As a
           further optimization, we do the read inline
           (falling back to a call to get_bits if the buffer
           runs dry).  The following (up to got_huff_bits:) is
           equivalent to j = get_bits(bd, hufGroup->maxLen);
         */
        while (bd->inbufBitCount < hufGroup->maxLen) {
            if (bd->inbufPos == bd->inbufCount) {
                j = get_bits(bd, hufGroup->maxLen);
                goto got_huff_bits;
            }
            bd->inbufBits =
                (bd->inbufBits << 8)|bd->inbuf[bd->inbufPos++];
            bd->inbufBitCount += 8;
        };
        bd->inbufBitCount -= hufGroup->maxLen;
        j = (bd->inbufBits >> bd->inbufBitCount)&
            ((1 << hufGroup->maxLen)-1);
got_huff_bits:
        /* Figure how how many bits are in next symbol and
         * unget extras */
        i = hufGroup->minLen;
        while (j > limit[i])
            ++i;
        bd->inbufBitCount += (hufGroup->maxLen - i);
        /* Huffman decode value to get nextSym (with bounds checking) */
        if ((i > hufGroup->maxLen)
            || (((unsigned)(j = (j>>(hufGroup->maxLen-i))-base[i]))
                >= MAX_SYMBOLS))
            return RETVAL_DATA_ERROR;
        nextSym = hufGroup->permute[j];
        /* We have now decoded the symbol, which indicates
           either a new literal byte, or a repeated run of the
           most recent literal byte.  First, check if nextSym
           indicates a repeated run, and if so loop collecting
           how many times to repeat the last literal. */
        if (((unsigned)nextSym) <= SYMBOL_RUNB) { /* RUNA or RUNB */
            /* If this is the start of a new run, zero out
             * counter */
            if (!runPos) {
                runPos = 1;
                t = 0;
            }
            /* Neat trick that saves 1 symbol: instead of
               or-ing 0 or 1 at each bit position, add 1
               or 2 instead.  For example, 1011 is 1 << 0
               + 1 << 1 + 2 << 2.  1010 is 2 << 0 + 2 << 1
               + 1 << 2.  You can make any bit pattern
               that way using 1 less symbol than the basic
               or 0/1 method (except all bits 0, which
               would use no symbols, but a run of length 0
               doesn't mean anything in this context).
               Thus space is saved. */
            t += (runPos << nextSym);
            /* +runPos if RUNA; +2*runPos if RUNB */

            runPos <<= 1;
            continue;
        }
        /* When we hit the first non-run symbol after a run,
           we now know how many times to repeat the last
           literal, so append that many copies to our buffer
           of decoded symbols (dbuf) now.  (The last literal
           used is the one at the head of the mtfSymbol
           array.) */
        if (runPos) {
            runPos = 0;
            if (dbufCount+t >= dbufSize)
                return RETVAL_DATA_ERROR;

            uc = symToByte[mtfSymbol[0]];
            byteCount[uc] += t;
            while (t--)
                dbuf[dbufCount++] = uc;
        }
        /* Is this the terminating symbol? */
        if (nextSym > symTotal)
            break;
        /* At this point, nextSym indicates a new literal
           character.  Subtract one to get the position in the
           MTF array at which this literal is currently to be
           found.  (Note that the result can't be -1 or 0,
           because 0 and 1 are RUNA and RUNB.  But another
           instance of the first symbol in the mtf array,
           position 0, would have been handled as part of a
           run above.  Therefore 1 unused mtf position minus 2
           non-literal nextSym values equals -1.) */
        if (dbufCount >= dbufSize)
            return RETVAL_DATA_ERROR;
        i = nextSym - 1;
        uc = mtfSymbol[i];
        /* Adjust the MTF array.  Since we typically expect to
         *move only a small number of symbols, and are bound
         *by 256 in any case, using memmove here would
         *typically be bigger and slower due to function call
         *overhead and other assorted setup costs. */
        do {
            mtfSymbol[i] = mtfSymbol[i-1];
        } while (--i);
        mtfSymbol[0] = uc;
        uc = symToByte[uc];
        /* We have our literal byte.  Save it into dbuf. */
        byteCount[uc]++;
        dbuf[dbufCount++] = (unsigned int)uc;
    }
    /* At this point, we've read all the Huffman-coded symbols
       (and repeated runs) for this block from the input stream,
       and decoded them into the intermediate buffer.  There are
       dbufCount many decoded bytes in dbuf[].  Now undo the
       Burrows-Wheeler transform on dbuf.  See
       http://dogma.net/markn/articles/bwt/bwt.htm
     */
    /* Turn byteCount into cumulative occurrence counts of 0 to n-1. */
    j = 0;
    for (i = 0; i < 256; i++) {
        k = j+byteCount[i];
        byteCount[i] = j;
        j = k;
    }
    /* Figure out what order dbuf would be in if we sorted it. */
    for (i = 0; i < dbufCount; i++) {
        uc = (unsigned char)(dbuf[i] & 0xff);
        dbuf[byteCount[uc]] |= (i << 8);
        byteCount[uc]++;
    }
    /* Decode first byte by hand to initialize "previous" byte.
       Note that it doesn't get output, and if the first three
       characters are identical it doesn't qualify as a run (hence
       writeRunCountdown = 5). */
    if (dbufCount) {
        if (origPtr >= dbufCount)
            return RETVAL_DATA_ERROR;
        bd->writePos = dbuf[origPtr];
        bd->writeCurrent = (unsigned char)(bd->writePos&0xff);
        bd->writePos >>= 8;
        bd->writeRunCountdown = 5;
    }
    bd->writeCount = dbufCount;

    return RETVAL_OK;
}

/* Undo burrows-wheeler transform on intermediate buffer to produce output.
   If start_bunzip was initialized with out_fd =-1, then up to len bytes of
   data are written to outbuf.  Return value is number of bytes written or
   error (all errors are negative numbers).  If out_fd!=-1, outbuf and len
   are ignored, data is written to out_fd and return is RETVAL_OK or error.
*/

static int read_bunzip(struct bunzip_data *bd, char *outbuf, int len)
{
    const unsigned int *dbuf;
    int pos, xcurrent, previous, gotcount;

    /* If last read was short due to end of file, return last block now */
    if (bd->writeCount < 0)
        return bd->writeCount;

    gotcount = 0;
    dbuf = bd->dbuf;
    pos = bd->writePos;
    xcurrent = bd->writeCurrent;

    /* We will always have pending decoded data to write into the output
       buffer unless this is the very first call (in which case we haven't
       Huffman-decoded a block into the intermediate buffer yet). */

    if (bd->writeCopies) {
        /* Inside the loop, writeCopies means extra copies (beyond 1) */
        --bd->writeCopies;
        /* Loop outputting bytes */
        for (;;) {
            /* If the output buffer is full, snapshot
             * state and return */
            if (gotcount >= len) {
                bd->writePos = pos;
                bd->writeCurrent = xcurrent;
                bd->writeCopies++;
                return len;
            }
            /* Write next byte into output buffer, updating CRC */
            outbuf[gotcount++] = xcurrent;
            bd->writeCRC = (((bd->writeCRC) << 8)
                ^bd->crc32Table[((bd->writeCRC) >> 24)
                ^xcurrent]);
            /* Loop now if we're outputting multiple
             * copies of this byte */
            if (bd->writeCopies) {
                --bd->writeCopies;
                continue;
            }
decode_next_byte:
            if (!bd->writeCount--)
                break;
            /* Follow sequence vector to undo
             * Burrows-Wheeler transform */
            previous = xcurrent;
            pos = dbuf[pos];
            xcurrent = pos&0xff;
            pos >>= 8;
            /* After 3 consecutive copies of the same
               byte, the 4th is a repeat count.  We count
               down from 4 instead *of counting up because
               testing for non-zero is faster */
            if (--bd->writeRunCountdown) {
                if (xcurrent != previous)
                    bd->writeRunCountdown = 4;
            } else {
                /* We have a repeated run, this byte
                 * indicates the count */
                bd->writeCopies = xcurrent;
                xcurrent = previous;
                bd->writeRunCountdown = 5;
                /* Sometimes there are just 3 bytes
                 * (run length 0) */
                if (!bd->writeCopies)
                    goto decode_next_byte;
                /* Subtract the 1 copy we'd output
                 * anyway to get extras */
                --bd->writeCopies;
            }
        }
        /* Decompression of this block completed successfully */
        bd->writeCRC = ~bd->writeCRC;
        bd->totalCRC = ((bd->totalCRC << 1) |
                (bd->totalCRC >> 31)) ^ bd->writeCRC;
        /* If this block had a CRC error, force file level CRC error. */
        if (bd->writeCRC != bd->headerCRC) {
            bd->totalCRC = bd->headerCRC+1;
            return RETVAL_LAST_BLOCK;
        }
    }

    /* Refill the intermediate buffer by Huffman-decoding next
     * block of input */
    /* (previous is just a convenient unused temp variable here) */
    previous = get_next_block(bd);
    if (previous) {
        bd->writeCount = previous;
        return (previous != RETVAL_LAST_BLOCK) ? previous : gotcount;
    }
    bd->writeCRC = 0xffffffffUL;
    pos = bd->writePos;
    xcurrent = bd->writeCurrent;
    goto decode_next_byte;
}

static long nofill(void *buf, unsigned long len)
{
    return -1;
}

/* Allocate the structure, read file header.  If in_fd ==-1, inbuf must contain
   a complete bunzip file (len bytes long).  If in_fd!=-1, inbuf and len are
   ignored, and data is read from file handle into temporary buffer. */
static int start_bunzip(struct bunzip_data **bdp, void *inbuf, long len,
                 long (*fill)(void*, unsigned long))
{
    struct bunzip_data *bd;
    unsigned int i, j, c;
    const unsigned int BZh0 =
        (((unsigned int)'B') << 24)+(((unsigned int)'Z') << 16)
        +(((unsigned int)'h') << 8)+(unsigned int)'0';

    /* Figure out how much data to allocate */
    i = sizeof(struct bunzip_data);

    /* Allocate bunzip_data.  Most fields initialize to zero. */
    bd = *bdp = malloc(i);
    if (!bd)
        return RETVAL_OUT_OF_MEMORY;
    memset(bd, 0, sizeof(struct bunzip_data));
    /* Setup input buffer */
    bd->inbuf = inbuf;
    bd->inbufCount = len;
    if (fill != NULL)
        bd->fill = fill;
    else
        bd->fill = nofill;

    /* Init the CRC32 table (big endian) */
    for (i = 0; i < 256; i++) {
        c = i << 24;
        for (j = 8; j; j--)
            c = c&0x80000000 ? (c << 1)^(CRC32_POLY_BE) : (c << 1);
        bd->crc32Table[i] = c;
    }

    /* Ensure that file starts with "BZh['1'-'9']." */
    i = get_bits(bd, 32);
    if (((unsigned int)(i-BZh0-1)) >= 9)
        return RETVAL_NOT_BZIP_DATA;

    /* Fourth byte (ascii '1'-'9'), indicates block size in units of 100k of
       uncompressed data.  Allocate intermediate buffer for block. */
    bd->dbufSize = 100000*(i-BZh0);

    bd->dbuf = large_malloc(bd->dbufSize * sizeof(int));
    if (!bd->dbuf)
        return RETVAL_OUT_OF_MEMORY;
    return RETVAL_OK;
}

/* Example usage: decompress src_fd to dst_fd.  (Stops at end of bzip2 data,
   not end of file.) */
int bunzip2(unsigned char *buf, long len,
            long (*fill)(void*, unsigned long),
            long (*flush)(void*, unsigned long),
            unsigned char *outbuf,
            long *pos,
            void(*error)(char *x))
{
    struct bunzip_data *bd;
    int i = -1;
    unsigned char *inbuf;

    if (flush)
        outbuf = malloc(BZIP2_IOBUF_SIZE);

    if (!outbuf) {
        error("Could not allocate output buffer");
        return RETVAL_OUT_OF_MEMORY;
    }
    if (buf)
        inbuf = buf;
    else
        inbuf = malloc(BZIP2_IOBUF_SIZE);
    if (!inbuf) {
        error("Could not allocate input buffer");
        i = RETVAL_OUT_OF_MEMORY;
        goto exit_0;
    }
    i = start_bunzip(&bd, inbuf, len, fill);
    if (!i) {
        for (;;) {
            i = read_bunzip(bd, outbuf, BZIP2_IOBUF_SIZE);
            if (i <= 0)
                break;
            if (!flush)
                outbuf += i;
            else
                if (i != flush(outbuf, i)) {
                    i = RETVAL_UNEXPECTED_OUTPUT_EOF;
                    break;
                }
        }
    }
    /* Check CRC and release memory */
    if (i == RETVAL_LAST_BLOCK) {
        if (bd->headerCRC != bd->totalCRC)
            error("Data integrity error when decompressing.");
        else
            i = RETVAL_OK;
    } else if (i == RETVAL_UNEXPECTED_OUTPUT_EOF) {
        error("Compressed file ends unexpectedly");
    }
    if (!bd)
        goto exit_1;
    if (bd->dbuf)
        large_free(bd->dbuf);
    if (pos)
        *pos = bd->inbufPos;
    free(bd);
exit_1:
    if (!buf)
        free(inbuf);
exit_0:
    if (flush)
        free(outbuf);
    return i;
}


// END BUNZIP2

// BEGIN LZ4


#define LZ4_DEFAULT_UNCOMPRESSED_CHUNK_SIZE (8 << 20)
#define ARCHIVE_MAGICNUMBER 0x184C2102

#define LZ4_MEMORY_USAGE 14

#define LZ4_MAX_INPUT_SIZE	0x7E000000 /* 2 113 929 216 bytes */
#define LZ4_COMPRESSBOUND(isize)	(\
    (unsigned int)(isize) > (unsigned int)LZ4_MAX_INPUT_SIZE \
    ? 0 \
    : (isize) + ((isize)/255) + 16)

#define LZ4_ACCELERATION_DEFAULT 1
#define LZ4_HASHLOG	 (LZ4_MEMORY_USAGE-2)
#define LZ4_HASHTABLESIZE (1 << LZ4_MEMORY_USAGE)
#define LZ4_HASH_SIZE_U32 (1 << LZ4_HASHLOG)

#define LZ4HC_MIN_CLEVEL			3
#define LZ4HC_DEFAULT_CLEVEL			9
#define LZ4HC_MAX_CLEVEL			16

#define LZ4HC_DICTIONARY_LOGSIZE 16
#define LZ4HC_MAXD (1<<LZ4HC_DICTIONARY_LOGSIZE)
#define LZ4HC_MAXD_MASK (LZ4HC_MAXD - 1)
#define LZ4HC_HASH_LOG (LZ4HC_DICTIONARY_LOGSIZE - 1)
#define LZ4HC_HASHTABLESIZE (1 << LZ4HC_HASH_LOG)
#define LZ4HC_HASH_MASK (LZ4HC_HASHTABLESIZE - 1)

/*-************************************************************************
 *	STREAMING CONSTANTS AND STRUCTURES
 **************************************************************************/
#define LZ4_STREAMSIZE_U64 ((1 << (LZ4_MEMORY_USAGE - 3)) + 4)
#define LZ4_STREAMSIZE	(LZ4_STREAMSIZE_U64 * sizeof(unsigned long long))

#define LZ4_STREAMHCSIZE        262192
#define LZ4_STREAMHCSIZE_SIZET (262192 / sizeof(size_t))

#define LZ4_STREAMDECODESIZE_U64	4
#define LZ4_STREAMDECODESIZE		 (LZ4_STREAMDECODESIZE_U64 * \
    sizeof(unsigned long long))

/*
 * LZ4_stream_t - information structure to track an LZ4 stream.
 */
typedef struct {
    uint32_t hashTable[LZ4_HASH_SIZE_U32];
    uint32_t currentOffset;
    uint32_t initCheck;
    const uint8_t *dictionary;
    uint8_t *bufferStart;
    uint32_t dictSize;
} LZ4_stream_t_internal;
typedef union {
    unsigned long long table[LZ4_STREAMSIZE_U64];
    LZ4_stream_t_internal internal_donotuse;
} LZ4_stream_t;

/*
 * LZ4_streamHC_t - information structure to track an LZ4HC stream.
 */
typedef struct {
    unsigned int	 hashTable[LZ4HC_HASHTABLESIZE];
    unsigned short	 chainTable[LZ4HC_MAXD];
    /* next block to continue on current prefix */
    const unsigned char *end;
    /* All index relative to this position */
    const unsigned char *base;
    /* alternate base for extDict */
    const unsigned char *dictBase;
    /* below that point, need extDict */
    unsigned int	 dictLimit;
    /* below that point, no more dict */
    unsigned int	 lowLimit;
    /* index from which to continue dict update */
    unsigned int	 nextToUpdate;
    unsigned int	 compressionLevel;
} LZ4HC_CCtx_internal;
typedef union {
    size_t table[LZ4_STREAMHCSIZE_SIZET];
    LZ4HC_CCtx_internal internal_donotuse;
} LZ4_streamHC_t;

/*
 * LZ4_streamDecode_t - information structure to track an
 *	LZ4 stream during decompression.
 *
 * init this structure using LZ4_setStreamDecode (or memset()) before first use
 */
typedef struct {
    const uint8_t *externalDict;
    size_t extDictSize;
    const uint8_t *prefixEnd;
    size_t prefixSize;
} LZ4_streamDecode_t_internal;
typedef union {
    unsigned long long table[LZ4_STREAMDECODESIZE_U64];
    LZ4_streamDecode_t_internal internal_donotuse;
} LZ4_streamDecode_t;

/*-************************************************************************
 *	SIZE OF STATE
 **************************************************************************/
#define LZ4_MEM_COMPRESS	LZ4_STREAMSIZE
#define LZ4HC_MEM_COMPRESS	LZ4_STREAMHCSIZE

/*-************************************************************************
 *	Compression Functions
 **************************************************************************/

/**
 * LZ4_compressBound() - Max. output size in worst case szenarios
 * @isize: Size of the input data
 *
 * Return: Max. size LZ4 may output in a "worst case" szenario
 * (data not compressible)
 */
static inline int LZ4_compressBound(size_t isize)
{
    return LZ4_COMPRESSBOUND(isize);
}

/**
 * LZ4_compress_default() - Compress data from source to dest
 * @source: source address of the original data
 * @dest: output buffer address of the compressed data
 * @inputSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
 * @maxOutputSize: full or partial size of buffer 'dest'
 *	which must be already allocated
 * @wrkmem: address of the working memory.
 *	This requires 'workmem' of LZ4_MEM_COMPRESS.
 *
 * Compresses 'sourceSize' bytes from buffer 'source'
 * into already allocated 'dest' buffer of size 'maxOutputSize'.
 * Compression is guaranteed to succeed if
 * 'maxOutputSize' >= LZ4_compressBound(inputSize).
 * It also runs faster, so it's a recommended setting.
 * If the function cannot compress 'source' into a more limited 'dest' budget,
 * compression stops *immediately*, and the function result is zero.
 * As a consequence, 'dest' content is not valid.
 *
 * Return: Number of bytes written into buffer 'dest'
 *	(necessarily <= maxOutputSize) or 0 if compression fails
 */
int LZ4_compress_default(const char *source, char *dest, int inputSize,
    int maxOutputSize, void *wrkmem);

/**
 * LZ4_compress_fast() - As LZ4_compress_default providing an acceleration param
 * @source: source address of the original data
 * @dest: output buffer address of the compressed data
 * @inputSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
 * @maxOutputSize: full or partial size of buffer 'dest'
 *	which must be already allocated
 * @acceleration: acceleration factor
 * @wrkmem: address of the working memory.
 *	This requires 'workmem' of LZ4_MEM_COMPRESS.
 *
 * Same as LZ4_compress_default(), but allows to select an "acceleration"
 * factor. The larger the acceleration value, the faster the algorithm,
 * but also the lesser the compression. It's a trade-off. It can be fine tuned,
 * with each successive value providing roughly +~3% to speed.
 * An acceleration value of "1" is the same as regular LZ4_compress_default()
 * Values <= 0 will be replaced by LZ4_ACCELERATION_DEFAULT, which is 1.
 *
 * Return: Number of bytes written into buffer 'dest'
 *	(necessarily <= maxOutputSize) or 0 if compression fails
 */
int LZ4_compress_fast(const char *source, char *dest, int inputSize,
    int maxOutputSize, int acceleration, void *wrkmem);

/**
 * LZ4_compress_destSize() - Compress as much data as possible
 *	from source to dest
 * @source: source address of the original data
 * @dest: output buffer address of the compressed data
 * @sourceSizePtr: will be modified to indicate how many bytes where read
 *	from 'source' to fill 'dest'. New value is necessarily <= old value.
 * @targetDestSize: Size of buffer 'dest' which must be already allocated
 * @wrkmem: address of the working memory.
 *	This requires 'workmem' of LZ4_MEM_COMPRESS.
 *
 * Reverse the logic, by compressing as much data as possible
 * from 'source' buffer into already allocated buffer 'dest'
 * of size 'targetDestSize'.
 * This function either compresses the entire 'source' content into 'dest'
 * if it's large enough, or fill 'dest' buffer completely with as much data as
 * possible from 'source'.
 *
 * Return: Number of bytes written into 'dest' (necessarily <= targetDestSize)
 *	or 0 if compression fails
 */
int LZ4_compress_destSize(const char *source, char *dest, int *sourceSizePtr,
    int targetDestSize, void *wrkmem);

/*-************************************************************************
 *	Decompression Functions
 **************************************************************************/

/**
 * LZ4_decompress_fast() - Decompresses data from 'source' into 'dest'
 * @source: source address of the compressed data
 * @dest: output buffer address of the uncompressed data
 *	which must be already allocated with 'originalSize' bytes
 * @originalSize: is the original and therefore uncompressed size
 *
 * Decompresses data from 'source' into 'dest'.
 * This function fully respect memory boundaries for properly formed
 * compressed data.
 * It is a bit faster than LZ4_decompress_safe().
 * However, it does not provide any protection against intentionally
 * modified data stream (malicious input).
 * Use this function in trusted environment only
 * (data to decode comes from a trusted source).
 *
 * Return: number of bytes read from the source buffer
 *	or a negative result if decompression fails.
 */
int LZ4_decompress_fast(const char *source, char *dest, int originalSize);

/**
 * LZ4_decompress_safe() - Decompression protected against buffer overflow
 * @source: source address of the compressed data
 * @dest: output buffer address of the uncompressed data
 *	which must be already allocated
 * @compressedSize: is the precise full size of the compressed block
 * @maxDecompressedSize: is the size of 'dest' buffer
 *
 * Decompresses data from 'source' into 'dest'.
 * If the source stream is detected malformed, the function will
 * stop decoding and return a negative result.
 * This function is protected against buffer overflow exploits,
 * including malicious data packets. It never writes outside output buffer,
 * nor reads outside input buffer.
 *
 * Return: number of bytes decompressed into destination buffer
 *	(necessarily <= maxDecompressedSize)
 *	or a negative result in case of error
 */
int LZ4_decompress_safe(const char *source, char *dest, int compressedSize,
    int maxDecompressedSize);

/**
 * LZ4_decompress_safe_partial() - Decompress a block of size 'compressedSize'
 *	at position 'source' into buffer 'dest'
 * @source: source address of the compressed data
 * @dest: output buffer address of the decompressed data which must be
 *	already allocated
 * @compressedSize: is the precise full size of the compressed block.
 * @targetOutputSize: the decompression operation will try
 *	to stop as soon as 'targetOutputSize' has been reached
 * @maxDecompressedSize: is the size of destination buffer
 *
 * This function decompresses a compressed block of size 'compressedSize'
 * at position 'source' into destination buffer 'dest'
 * of size 'maxDecompressedSize'.
 * The function tries to stop decompressing operation as soon as
 * 'targetOutputSize' has been reached, reducing decompression time.
 * This function never writes outside of output buffer,
 * and never reads outside of input buffer.
 * It is therefore protected against malicious data packets.
 *
 * Return: the number of bytes decoded in the destination buffer
 *	(necessarily <= maxDecompressedSize)
 *	or a negative result in case of error
 *
 */
int LZ4_decompress_safe_partial(const char *source, char *dest,
    int compressedSize, int targetOutputSize, int maxDecompressedSize);

/*-************************************************************************
 *	LZ4 HC Compression
 **************************************************************************/

/**
 * LZ4_compress_HC() - Compress data from `src` into `dst`, using HC algorithm
 * @src: source address of the original data
 * @dst: output buffer address of the compressed data
 * @srcSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
 * @dstCapacity: full or partial size of buffer 'dst',
 *	which must be already allocated
 * @compressionLevel: Recommended values are between 4 and 9, although any
 *	value between 1 and LZ4HC_MAX_CLEVEL will work.
 *	Values >LZ4HC_MAX_CLEVEL behave the same as 16.
 * @wrkmem: address of the working memory.
 *	This requires 'wrkmem' of size LZ4HC_MEM_COMPRESS.
 *
 * Compress data from 'src' into 'dst', using the more powerful
 * but slower "HC" algorithm. Compression is guaranteed to succeed if
 * `dstCapacity >= LZ4_compressBound(srcSize)
 *
 * Return : the number of bytes written into 'dst' or 0 if compression fails.
 */
int LZ4_compress_HC(const char *src, char *dst, int srcSize, int dstCapacity,
    int compressionLevel, void *wrkmem);

/**
 * LZ4_resetStreamHC() - Init an allocated 'LZ4_streamHC_t' structure
 * @streamHCPtr: pointer to the 'LZ4_streamHC_t' structure
 * @compressionLevel: Recommended values are between 4 and 9, although any
 *	value between 1 and LZ4HC_MAX_CLEVEL will work.
 *	Values >LZ4HC_MAX_CLEVEL behave the same as 16.
 *
 * An LZ4_streamHC_t structure can be allocated once
 * and re-used multiple times.
 * Use this function to init an allocated `LZ4_streamHC_t` structure
 * and start a new compression.
 */
void LZ4_resetStreamHC(LZ4_streamHC_t *streamHCPtr, int compressionLevel);

/**
 * LZ4_loadDictHC() - Load a static dictionary into LZ4_streamHC
 * @streamHCPtr: pointer to the LZ4HC_stream_t
 * @dictionary: dictionary to load
 * @dictSize: size of dictionary
 *
 * Use this function to load a static dictionary into LZ4HC_stream.
 * Any previous data will be forgotten, only 'dictionary'
 * will remain in memory.
 * Loading a size of 0 is allowed.
 *
 * Return : dictionary size, in bytes (necessarily <= 64 KB)
 */
int	LZ4_loadDictHC(LZ4_streamHC_t *streamHCPtr, const char *dictionary,
    int dictSize);

/**
 * LZ4_compress_HC_continue() - Compress 'src' using data from previously
 *	compressed blocks as a dictionary using the HC algorithm
 * @streamHCPtr: Pointer to the previous 'LZ4_streamHC_t' structure
 * @src: source address of the original data
 * @dst: output buffer address of the compressed data,
 *	which must be already allocated
 * @srcSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
 * @maxDstSize: full or partial size of buffer 'dest'
 *	which must be already allocated
 *
 * These functions compress data in successive blocks of any size, using
 * previous blocks as dictionary. One key assumption is that previous
 * blocks (up to 64 KB) remain read-accessible while
 * compressing next blocks. There is an exception for ring buffers,
 * which can be smaller than 64 KB.
 * Ring buffers scenario is automatically detected and handled by
 * LZ4_compress_HC_continue().
 * Before starting compression, state must be properly initialized,
 * using LZ4_resetStreamHC().
 * A first "fictional block" can then be designated as
 * initial dictionary, using LZ4_loadDictHC() (Optional).
 * Then, use LZ4_compress_HC_continue()
 * to compress each successive block. Previous memory blocks
 * (including initial dictionary when present) must remain accessible
 * and unmodified during compression.
 * 'dst' buffer should be sized to handle worst case scenarios, using
 *  LZ4_compressBound(), to ensure operation success.
 *  If, for any reason, previous data blocks can't be preserved unmodified
 *  in memory during next compression block,
 *  you must save it to a safer memory space, using LZ4_saveDictHC().
 * Return value of LZ4_saveDictHC() is the size of dictionary
 * effectively saved into 'safeBuffer'.
 *
 * Return: Number of bytes written into buffer 'dst'  or 0 if compression fails
 */
int LZ4_compress_HC_continue(LZ4_streamHC_t *streamHCPtr, const char *src,
    char *dst, int srcSize, int maxDstSize);

/**
 * LZ4_saveDictHC() - Save static dictionary from LZ4HC_stream
 * @streamHCPtr: pointer to the 'LZ4HC_stream_t' structure
 * @safeBuffer: buffer to save dictionary to, must be already allocated
 * @maxDictSize: size of 'safeBuffer'
 *
 * If previously compressed data block is not guaranteed
 * to remain available at its memory location,
 * save it into a safer place (char *safeBuffer).
 * Note : you don't need to call LZ4_loadDictHC() afterwards,
 * dictionary is immediately usable, you can therefore call
 * LZ4_compress_HC_continue().
 *
 * Return : saved dictionary size in bytes (necessarily <= maxDictSize),
 *	or 0 if error.
 */
int LZ4_saveDictHC(LZ4_streamHC_t *streamHCPtr, char *safeBuffer,
    int maxDictSize);

/*-*********************************************
 *	Streaming Compression Functions
 ***********************************************/

/**
 * LZ4_resetStream() - Init an allocated 'LZ4_stream_t' structure
 * @LZ4_stream: pointer to the 'LZ4_stream_t' structure
 *
 * An LZ4_stream_t structure can be allocated once
 * and re-used multiple times.
 * Use this function to init an allocated `LZ4_stream_t` structure
 * and start a new compression.
 */
void LZ4_resetStream(LZ4_stream_t *LZ4_stream);

/**
 * LZ4_loadDict() - Load a static dictionary into LZ4_stream
 * @streamPtr: pointer to the LZ4_stream_t
 * @dictionary: dictionary to load
 * @dictSize: size of dictionary
 *
 * Use this function to load a static dictionary into LZ4_stream.
 * Any previous data will be forgotten, only 'dictionary'
 * will remain in memory.
 * Loading a size of 0 is allowed.
 *
 * Return : dictionary size, in bytes (necessarily <= 64 KB)
 */
int LZ4_loadDict(LZ4_stream_t *streamPtr, const char *dictionary,
    int dictSize);

/**
 * LZ4_saveDict() - Save static dictionary from LZ4_stream
 * @streamPtr: pointer to the 'LZ4_stream_t' structure
 * @safeBuffer: buffer to save dictionary to, must be already allocated
 * @dictSize: size of 'safeBuffer'
 *
 * If previously compressed data block is not guaranteed
 * to remain available at its memory location,
 * save it into a safer place (char *safeBuffer).
 * Note : you don't need to call LZ4_loadDict() afterwards,
 * dictionary is immediately usable, you can therefore call
 * LZ4_compress_fast_continue().
 *
 * Return : saved dictionary size in bytes (necessarily <= dictSize),
 *	or 0 if error.
 */
int LZ4_saveDict(LZ4_stream_t *streamPtr, char *safeBuffer, int dictSize);

/**
 * LZ4_compress_fast_continue() - Compress 'src' using data from previously
 *	compressed blocks as a dictionary
 * @streamPtr: Pointer to the previous 'LZ4_stream_t' structure
 * @src: source address of the original data
 * @dst: output buffer address of the compressed data,
 *	which must be already allocated
 * @srcSize: size of the input data. Max supported value is LZ4_MAX_INPUT_SIZE
 * @maxDstSize: full or partial size of buffer 'dest'
 *	which must be already allocated
 * @acceleration: acceleration factor
 *
 * Compress buffer content 'src', using data from previously compressed blocks
 * as dictionary to improve compression ratio.
 * Important : Previous data blocks are assumed to still
 * be present and unmodified !
 * If maxDstSize >= LZ4_compressBound(srcSize),
 * compression is guaranteed to succeed, and runs faster.
 *
 * Return: Number of bytes written into buffer 'dst'  or 0 if compression fails
 */
int LZ4_compress_fast_continue(LZ4_stream_t *streamPtr, const char *src,
    char *dst, int srcSize, int maxDstSize, int acceleration);

/**
 * LZ4_setStreamDecode() - Instruct where to find dictionary
 * @LZ4_streamDecode: the 'LZ4_streamDecode_t' structure
 * @dictionary: dictionary to use
 * @dictSize: size of dictionary
 *
 * Use this function to instruct where to find the dictionary.
 *	Setting a size of 0 is allowed (same effect as reset).
 *
 * Return: 1 if OK, 0 if error
 */
int LZ4_setStreamDecode(LZ4_streamDecode_t *LZ4_streamDecode,
    const char *dictionary, int dictSize);

/**
 * LZ4_decompress_safe_continue() - Decompress blocks in streaming mode
 * @LZ4_streamDecode: the 'LZ4_streamDecode_t' structure
 * @source: source address of the compressed data
 * @dest: output buffer address of the uncompressed data
 *	which must be already allocated
 * @compressedSize: is the precise full size of the compressed block
 * @maxDecompressedSize: is the size of 'dest' buffer
 *
 * This decoding function allows decompression of multiple blocks
 * in "streaming" mode.
 * Previously decoded blocks *must* remain available at the memory position
 * where they were decoded (up to 64 KB)
 * In the case of a ring buffers, decoding buffer must be either :
 *    - Exactly same size as encoding buffer, with same update rule
 *      (block boundaries at same positions) In which case,
 *      the decoding & encoding ring buffer can have any size,
 *      including very small ones ( < 64 KB).
 *    - Larger than encoding buffer, by a minimum of maxBlockSize more bytes.
 *      maxBlockSize is implementation dependent.
 *      It's the maximum size you intend to compress into a single block.
 *      In which case, encoding and decoding buffers do not need
 *      to be synchronized, and encoding ring buffer can have any size,
 *      including small ones ( < 64 KB).
 *    - _At least_ 64 KB + 8 bytes + maxBlockSize.
 *      In which case, encoding and decoding buffers do not need to be
 *      synchronized, and encoding ring buffer can have any size,
 *      including larger than decoding buffer. W
 * Whenever these conditions are not possible, save the last 64KB of decoded
 * data into a safe buffer, and indicate where it is saved
 * using LZ4_setStreamDecode()
 *
 * Return: number of bytes decompressed into destination buffer
 *	(necessarily <= maxDecompressedSize)
 *	or a negative result in case of error
 */
int LZ4_decompress_safe_continue(LZ4_streamDecode_t *LZ4_streamDecode,
    const char *source, char *dest, int compressedSize,
    int maxDecompressedSize);

/**
 * LZ4_decompress_fast_continue() - Decompress blocks in streaming mode
 * @LZ4_streamDecode: the 'LZ4_streamDecode_t' structure
 * @source: source address of the compressed data
 * @dest: output buffer address of the uncompressed data
 *	which must be already allocated with 'originalSize' bytes
 * @originalSize: is the original and therefore uncompressed size
 *
 * This decoding function allows decompression of multiple blocks
 * in "streaming" mode.
 * Previously decoded blocks *must* remain available at the memory position
 * where they were decoded (up to 64 KB)
 * In the case of a ring buffers, decoding buffer must be either :
 *    - Exactly same size as encoding buffer, with same update rule
 *      (block boundaries at same positions) In which case,
 *      the decoding & encoding ring buffer can have any size,
 *      including very small ones ( < 64 KB).
 *    - Larger than encoding buffer, by a minimum of maxBlockSize more bytes.
 *      maxBlockSize is implementation dependent.
 *      It's the maximum size you intend to compress into a single block.
 *      In which case, encoding and decoding buffers do not need
 *      to be synchronized, and encoding ring buffer can have any size,
 *      including small ones ( < 64 KB).
 *    - _At least_ 64 KB + 8 bytes + maxBlockSize.
 *      In which case, encoding and decoding buffers do not need to be
 *      synchronized, and encoding ring buffer can have any size,
 *      including larger than decoding buffer. W
 * Whenever these conditions are not possible, save the last 64KB of decoded
 * data into a safe buffer, and indicate where it is saved
 * using LZ4_setStreamDecode()
 *
 * Return: number of bytes decompressed into destination buffer
 *	(necessarily <= maxDecompressedSize)
 *	or a negative result in case of error
 */
int LZ4_decompress_fast_continue(LZ4_streamDecode_t *LZ4_streamDecode,
    const char *source, char *dest, int originalSize);

/**
 * LZ4_decompress_safe_usingDict() - Same as LZ4_setStreamDecode()
 *	followed by LZ4_decompress_safe_continue()
 * @source: source address of the compressed data
 * @dest: output buffer address of the uncompressed data
 *	which must be already allocated
 * @compressedSize: is the precise full size of the compressed block
 * @maxDecompressedSize: is the size of 'dest' buffer
 * @dictStart: pointer to the start of the dictionary in memory
 * @dictSize: size of dictionary
 *
 * This decoding function works the same as
 * a combination of LZ4_setStreamDecode() followed by
 * LZ4_decompress_safe_continue()
 * It is stand-alone, and doesn't need an LZ4_streamDecode_t structure.
 *
 * Return: number of bytes decompressed into destination buffer
 *	(necessarily <= maxDecompressedSize)
 *	or a negative result in case of error
 */
int LZ4_decompress_safe_usingDict(const char *source, char *dest,
    int compressedSize, int maxDecompressedSize, const char *dictStart,
    int dictSize);

/**
 * LZ4_decompress_fast_usingDict() - Same as LZ4_setStreamDecode()
 *	followed by LZ4_decompress_fast_continue()
 * @source: source address of the compressed data
 * @dest: output buffer address of the uncompressed data
 *	which must be already allocated with 'originalSize' bytes
 * @originalSize: is the original and therefore uncompressed size
 * @dictStart: pointer to the start of the dictionary in memory
 * @dictSize: size of dictionary
 *
 * This decoding function works the same as
 * a combination of LZ4_setStreamDecode() followed by
 * LZ4_decompress_fast_continue()
 * It is stand-alone, and doesn't need an LZ4_streamDecode_t structure.
 *
 * Return: number of bytes decompressed into destination buffer
 *	(necessarily <= maxDecompressedSize)
 *	or a negative result in case of error
 */
int LZ4_decompress_fast_usingDict(const char *source, char *dest,
    int originalSize, const char *dictStart, int dictSize);

#define FORCE_INLINE __always_inline

/*-************************************
 *	Basic Types
 **************************************/
#include <linux/types.h>

typedef	uint8_t BYTE;
typedef uint16_t U16;
typedef uint32_t U32;
typedef	int32_t S32;
typedef uint64_t U64;
typedef uintptr_t uptrval;

/*-************************************
 *	Architecture specifics
 **************************************/
#if defined(CONFIG_64BIT)
#define LZ4_ARCH64 1
#else
#define LZ4_ARCH64 0
#endif

#if defined(__LITTLE_ENDIAN)
#define LZ4_LITTLE_ENDIAN 1
#else
#define LZ4_LITTLE_ENDIAN 0
#endif

/*-************************************
 *	Constants
 **************************************/
#define MINMATCH 4

#define WILDCOPYLENGTH 8
#define LASTLITERALS 5
#define MFLIMIT (WILDCOPYLENGTH + MINMATCH)
/*
 * ensure it's possible to write 2 x wildcopyLength
 * without overflowing output buffer
 */
#define MATCH_SAFEGUARD_DISTANCE  ((2 * WILDCOPYLENGTH) - MINMATCH)

/* Increase this value ==> compression run slower on incompressible data */
#define LZ4_SKIPTRIGGER 6

#define HASH_UNIT sizeof(size_t)

#define KB (1 << 10)
#define MB (1 << 20)
#define GB (1U << 30)

#define MAXD_LOG 16
#define MAX_DISTANCE ((1 << MAXD_LOG) - 1)
#define STEPSIZE sizeof(size_t)

#define ML_BITS	4
#define ML_MASK	((1U << ML_BITS) - 1)
#define RUN_BITS (8 - ML_BITS)
#define RUN_MASK ((1U << RUN_BITS) - 1)

/*-************************************
 *	Reading and writing into memory
 **************************************/
static FORCE_INLINE U16 LZ4_read16(const void *ptr)
{
    return get_unaligned((const U16 *)ptr);
}

static FORCE_INLINE U32 LZ4_read32(const void *ptr)
{
    return get_unaligned((const U32 *)ptr);
}

static FORCE_INLINE size_t LZ4_read_ARCH(const void *ptr)
{
    return get_unaligned((const size_t *)ptr);
}

static FORCE_INLINE void LZ4_write16(void *memPtr, U16 value)
{
    put_unaligned(value, (U16 *)memPtr);
}

static FORCE_INLINE void LZ4_write32(void *memPtr, U32 value)
{
    put_unaligned_le32(value, (U32 *)memPtr);
}

static FORCE_INLINE U16 LZ4_readLE16(const void *memPtr)
{
    return get_unaligned_le16(memPtr);
}

static FORCE_INLINE void LZ4_writeLE16(void *memPtr, U16 value)
{
    return put_unaligned_le16(value, memPtr);
}

#define LZ4_memmove(dst, src, size) __builtin_memmove(dst, src, size)

static inline uint32_t __get_unaligned_le32(const uint8_t *p)
{
    return p[0] | p[1] << 8 | p[2] << 16 | p[3] << 24;
}


static inline uint64_t __get_unaligned_le64(const uint8_t *p)
{
    return (uint64_t)__get_unaligned_le32(p + 4) << 32 |
           __get_unaligned_le32(p);
}

static inline uint64_t get_unaligned_le64(const void *p)
{
    return __get_unaligned_le64((const uint8_t *)p);
}

static inline void __put_unaligned_le16(uint16_t val, uint8_t *p)
{
    *p++ = val;
    *p++ = val >> 8;
}

static inline void __put_unaligned_le32(uint32_t val, uint8_t *p)
{
    __put_unaligned_le16(val >> 16, p + 2);
    __put_unaligned_le16(val, p);
}

static inline void __put_unaligned_le64(uint64_t val, uint8_t *p)
{
    __put_unaligned_le32(val >> 32, p + 4);
    __put_unaligned_le32(val, p);
}

static inline void put_unaligned_le64(uint64_t val, void *p)
{
    __put_unaligned_le64(val, p);
}

static FORCE_INLINE void LZ4_copy8(void *dst, const void *src)
{
    U64 a = get_unaligned_le64((const U64 *)src);

    put_unaligned_le64(a, (U64 *)dst);
}

/*
 * customized variant of memcpy,
 * which can overwrite up to 7 bytes beyond dstEnd
 */
static FORCE_INLINE void LZ4_wildCopy(void *dstPtr,
    const void *srcPtr, void *dstEnd)
{
    BYTE *d = (BYTE *)dstPtr;
    const BYTE *s = (const BYTE *)srcPtr;
    BYTE *const e = (BYTE *)dstEnd;

    do {
        LZ4_copy8(d, s);
        d += 8;
        s += 8;
    } while (d < e);
}

static FORCE_INLINE unsigned int LZ4_NbCommonBytes(register size_t val)
{
#if LZ4_LITTLE_ENDIAN
    return __ffs(val) >> 3;
#else
    return (BITS_PER_LONG - 1 - __fls(val)) >> 3;
#endif
}

static FORCE_INLINE unsigned int LZ4_count(
    const BYTE *pIn,
    const BYTE *pMatch,
    const BYTE *pInLimit)
{
    const BYTE *const pStart = pIn;

    while (likely(pIn < pInLimit - (STEPSIZE - 1))) {
        size_t const diff = LZ4_read_ARCH(pMatch) ^ LZ4_read_ARCH(pIn);

        if (!diff) {
            pIn += STEPSIZE;
            pMatch += STEPSIZE;
            continue;
        }

        pIn += LZ4_NbCommonBytes(diff);

        return (unsigned int)(pIn - pStart);
    }

#if LZ4_ARCH64
    if ((pIn < (pInLimit - 3))
        && (LZ4_read32(pMatch) == LZ4_read32(pIn))) {
        pIn += 4;
        pMatch += 4;
    }
#endif

    if ((pIn < (pInLimit - 1))
        && (LZ4_read16(pMatch) == LZ4_read16(pIn))) {
        pIn += 2;
        pMatch += 2;
    }

    if ((pIn < pInLimit) && (*pMatch == *pIn))
        pIn++;

    return (unsigned int)(pIn - pStart);
}

typedef enum { noLimit = 0, limitedOutput = 1 } limitedOutput_directive;
typedef enum { byPtr, byU32, byU16 } tableType_t;

typedef enum { noDict = 0, withPrefix64k, usingExtDict } dict_directive;
typedef enum { noDictIssue = 0, dictSmall } dictIssue_directive;

typedef enum { endOnOutputSize = 0, endOnInputSize = 1 } endCondition_directive;
typedef enum { decode_full_block = 0, partial_decode = 1 } earlyEnd_directive;

#define LZ4_STATIC_ASSERT(c)	(c)



static int LZ4_decompress_generic(
     const char * const src,
     char * const dst,
     int srcSize,
        /*
         * If endOnInput == endOnInputSize,
         * this value is `dstCapacity`
         */
     int outputSize,
     /* endOnOutputSize, endOnInputSize */
     endCondition_directive endOnInput,
     /* full, partial */
     earlyEnd_directive partialDecoding,
     /* noDict, withPrefix64k, usingExtDict */
     dict_directive dict,
     /* always <= dst, == dst when no prefix */
     const BYTE * const lowPrefix,
     /* only if dict == usingExtDict */
     const BYTE * const dictStart,
     /* note : = 0 if noDict */
     const size_t dictSize
     )
{
    const BYTE *ip = (const BYTE *) src;
    const BYTE * const iend = ip + srcSize;

    BYTE *op = (BYTE *) dst;
    BYTE * const oend = op + outputSize;
    BYTE *cpy;

    const BYTE * const dictEnd = (const BYTE *)dictStart + dictSize;
    static const unsigned int inc32table[8] = {0, 1, 2, 1, 0, 4, 4, 4};
    static const int dec64table[8] = {0, 0, 0, -1, -4, 1, 2, 3};

    const int safeDecode = (endOnInput == endOnInputSize);
    const int checkOffset = ((safeDecode) && (dictSize < (int)(64 * KB)));

    /* Set up the "end" pointers for the shortcut. */
    const BYTE *const shortiend = iend -
        (endOnInput ? 14 : 8) /*maxLL*/ - 2 /*offset*/;
    const BYTE *const shortoend = oend -
        (endOnInput ? 14 : 8) /*maxLL*/ - 18 /*maxML*/;

    //DEBUGLOG(5, "%s (srcSize:%i, dstSize:%i)", __func__,
     //    srcSize, outputSize);

    /* Special cases */
    assert(lowPrefix <= op);
    assert(src != NULL);

    /* Empty output buffer */
    if ((endOnInput) && (unlikely(outputSize == 0)))
        return ((srcSize == 1) && (*ip == 0)) ? 0 : -1;

    if ((!endOnInput) && (unlikely(outputSize == 0)))
        return (*ip == 0 ? 1 : -1);

    if ((endOnInput) && unlikely(srcSize == 0))
        return -1;

    /* Main Loop : decode sequences */
    while (1) {
        size_t length;
        const BYTE *match;
        size_t offset;

        /* get literal length */
        unsigned int const token = *ip++;
        length = token>>ML_BITS;

        /* ip < iend before the increment */
        assert(!endOnInput || ip <= iend);

        /*
         * A two-stage shortcut for the most common case:
         * 1) If the literal length is 0..14, and there is enough
         * space, enter the shortcut and copy 16 bytes on behalf
         * of the literals (in the fast mode, only 8 bytes can be
         * safely copied this way).
         * 2) Further if the match length is 4..18, copy 18 bytes
         * in a similar manner; but we ensure that there's enough
         * space in the output for those 18 bytes earlier, upon
         * entering the shortcut (in other words, there is a
         * combined check for both stages).
         */
        if ((endOnInput ? length != RUN_MASK : length <= 8)
           /*
            * strictly "less than" on input, to re-enter
            * the loop with at least one byte
            */
           && likely((endOnInput ? ip < shortiend : 1) &
                 (op <= shortoend))) {
            /* Copy the literals */
            memcpy(op, ip, endOnInput ? 16 : 8);
            op += length; ip += length;

            /*
             * The second stage:
             * prepare for match copying, decode full info.
             * If it doesn't work out, the info won't be wasted.
             */
            length = token & ML_MASK; /* match length */
            offset = LZ4_readLE16(ip);
            ip += 2;
            match = op - offset;
            assert(match <= op); /* check overflow */

            /* Do not deal with overlapping matches. */
            if ((length != ML_MASK) &&
                (offset >= 8) &&
                (dict == withPrefix64k || match >= lowPrefix)) {
                /* Copy the match. */
                memcpy(op + 0, match + 0, 8);
                memcpy(op + 8, match + 8, 8);
                memcpy(op + 16, match + 16, 2);
                op += length + MINMATCH;
                /* Both stages worked, load the next token. */
                continue;
            }

            /*
             * The second stage didn't work out, but the info
             * is ready. Propel it right to the point of match
             * copying.
             */
            goto _copy_match;
        }

        /* decode literal length */
        if (length == RUN_MASK) {
            unsigned int s;

            if (unlikely(endOnInput ? ip >= iend - RUN_MASK : 0)) {
                /* overflow detection */
                goto _output_error;
            }
            do {
                s = *ip++;
                length += s;
            } while (likely(endOnInput
                ? ip < iend - RUN_MASK
                : 1) & (s == 255));

            if ((safeDecode)
                && unlikely((uptrval)(op) +
                    length < (uptrval)(op))) {
                /* overflow detection */
                goto _output_error;
            }
            if ((safeDecode)
                && unlikely((uptrval)(ip) +
                    length < (uptrval)(ip))) {
                /* overflow detection */
                goto _output_error;
            }
        }

        /* copy literals */
        cpy = op + length;
        //LZ4_STATIC_ASSERT(MFLIMIT >= WILDCOPYLENGTH);

        if (((endOnInput) && ((cpy > oend - MFLIMIT)
            || (ip + length > iend - (2 + 1 + LASTLITERALS))))
            || ((!endOnInput) && (cpy > oend - WILDCOPYLENGTH))) {
            if (partialDecoding) {
                if (cpy > oend) {
                    /*
                     * Partial decoding :
                     * stop in the middle of literal segment
                     */
                    cpy = oend;
                    length = oend - op;
                }
                if ((endOnInput)
                    && (ip + length > iend)) {
                    /*
                     * Error :
                     * read attempt beyond
                     * end of input buffer
                     */
                    goto _output_error;
                }
            } else {
                if ((!endOnInput)
                    && (cpy != oend)) {
                    /*
                     * Error :
                     * block decoding must
                     * stop exactly there
                     */
                    goto _output_error;
                }
                if ((endOnInput)
                    && ((ip + length != iend)
                    || (cpy > oend))) {
                    /*
                     * Error :
                     * input must be consumed
                     */
                    goto _output_error;
                }
            }

            /*
             * supports overlapping memory regions; only matters
             * for in-place decompression scenarios
             */
            LZ4_memmove(op, ip, length);
            ip += length;
            op += length;

            /* Necessarily EOF when !partialDecoding.
             * When partialDecoding, it is EOF if we've either
             * filled the output buffer or
             * can't proceed with reading an offset for following match.
             */
            if (!partialDecoding || (cpy == oend) || (ip >= (iend - 2)))
                break;
        } else {
            /* may overwrite up to WILDCOPYLENGTH beyond cpy */
            LZ4_wildCopy(op, ip, cpy);
            ip += length;
            op = cpy;
        }

        /* get offset */
        offset = LZ4_readLE16(ip);
        ip += 2;
        match = op - offset;

        /* get matchlength */
        length = token & ML_MASK;

_copy_match:
        if ((checkOffset) && (unlikely(match + dictSize < lowPrefix))) {
            /* Error : offset outside buffers */
            goto _output_error;
        }

        /* costs ~1%; silence an msan warning when offset == 0 */
        /*
         * note : when partialDecoding, there is no guarantee that
         * at least 4 bytes remain available in output buffer
         */
        if (!partialDecoding) {
            assert(oend > op);
            assert(oend - op >= 4);

            LZ4_write32(op, (U32)offset);
        }

        if (length == ML_MASK) {
            unsigned int s;

            do {
                s = *ip++;

                if ((endOnInput) && (ip > iend - LASTLITERALS))
                    goto _output_error;

                length += s;
            } while (s == 255);

            if ((safeDecode)
                && unlikely(
                    (uptrval)(op) + length < (uptrval)op)) {
                /* overflow detection */
                goto _output_error;
            }
        }

        length += MINMATCH;

        /* match starting within external dictionary */
        if ((dict == usingExtDict) && (match < lowPrefix)) {
            if (unlikely(op + length > oend - LASTLITERALS)) {
                /* doesn't respect parsing restriction */
                if (!partialDecoding)
                    goto _output_error;
                length = min(length, (size_t)(oend - op));
            }

            if (length <= (size_t)(lowPrefix - match)) {
                /*
                 * match fits entirely within external
                 * dictionary : just copy
                 */
                memmove(op, dictEnd - (lowPrefix - match),
                    length);
                op += length;
            } else {
                /*
                 * match stretches into both external
                 * dictionary and current block
                 */
                size_t const copySize = (size_t)(lowPrefix - match);
                size_t const restSize = length - copySize;

                memcpy(op, dictEnd - copySize, copySize);
                op += copySize;
                if (restSize > (size_t)(op - lowPrefix)) {
                    /* overlap copy */
                    BYTE * const endOfMatch = op + restSize;
                    const BYTE *copyFrom = lowPrefix;

                    while (op < endOfMatch)
                        *op++ = *copyFrom++;
                } else {
                    memcpy(op, lowPrefix, restSize);
                    op += restSize;
                }
            }
            continue;
        }

        /* copy match within block */
        cpy = op + length;

        /*
         * partialDecoding :
         * may not respect endBlock parsing restrictions
         */
        assert(op <= oend);
        if (partialDecoding &&
            (cpy > oend - MATCH_SAFEGUARD_DISTANCE)) {
            size_t const mlen = min(length, (size_t)(oend - op));
            const BYTE * const matchEnd = match + mlen;
            BYTE * const copyEnd = op + mlen;

            if (matchEnd > op) {
                /* overlap copy */
                while (op < copyEnd)
                    *op++ = *match++;
            } else {
                memcpy(op, match, mlen);
            }
            op = copyEnd;
            if (op == oend)
                break;
            continue;
        }

        if (unlikely(offset < 8)) {
            op[0] = match[0];
            op[1] = match[1];
            op[2] = match[2];
            op[3] = match[3];
            match += inc32table[offset];
            memcpy(op + 4, match, 4);
            match -= dec64table[offset];
        } else {
            LZ4_copy8(op, match);
            match += 8;
        }

        op += 8;

        if (unlikely(cpy > oend - MATCH_SAFEGUARD_DISTANCE)) {
            BYTE * const oCopyLimit = oend - (WILDCOPYLENGTH - 1);

            if (cpy > oend - LASTLITERALS) {
                /*
                 * Error : last LASTLITERALS bytes
                 * must be literals (uncompressed)
                 */
                goto _output_error;
            }

            if (op < oCopyLimit) {
                LZ4_wildCopy(op, match, oCopyLimit);
                match += oCopyLimit - op;
                op = oCopyLimit;
            }
            while (op < cpy)
                *op++ = *match++;
        } else {
            LZ4_copy8(op, match);
            if (length > 16)
                LZ4_wildCopy(op + 8, match + 8, cpy);
        }
        op = cpy; /* wildcopy correction */
    }

    /* end of decoding */
    if (endOnInput) {
        /* Nb of output bytes decoded */
        return (int) (((char *)op) - dst);
    } else {
        /* Nb of input bytes read */
        return (int) (((const char *)ip) - src);
    }

    /* Overflow error detected */
_output_error:
    return (int) (-(((const char *)ip) - src)) - 1;
}


int LZ4_decompress_fast(const char *source, char *dest, int originalSize)
{
    return LZ4_decompress_generic(source, dest, 0, originalSize,
                      endOnOutputSize, decode_full_block,
                      withPrefix64k,
                      (BYTE *)dest - 64 * KB, NULL, 0);
}

int unlz4(uint8_t *input, long in_len,
                long (*fill)(void *, unsigned long),
                long (*flush)(void *, unsigned long),
                uint8_t *output, long *posp,
                void (*error) (char *x))
{
    int ret = -1;
    size_t chunksize = 0;
    size_t uncomp_chunksize = LZ4_DEFAULT_UNCOMPRESSED_CHUNK_SIZE;
    u8 *inp;
    u8 *inp_start;
    u8 *outp;
    long size = in_len;

    size_t out_len = get_unaligned_le32(input + in_len);
    size_t dest_len;


    if (output) {
        outp = output;
    } else if (!flush) {
        error("NULL output pointer and no flush function provided");
        goto exit_0;
    } else {
        outp = large_malloc(uncomp_chunksize);
        if (!outp) {
            error("Could not allocate output buffer");
            goto exit_0;
        }
    }

    if (input && fill) {
        error("Both input pointer and fill function provided,");
        goto exit_1;
    } else if (input) {
        inp = input;
    } else if (!fill) {
        error("NULL input pointer and missing fill function");
        goto exit_1;
    } else {
        inp = large_malloc(LZ4_compressBound(uncomp_chunksize));
        if (!inp) {
            error("Could not allocate input buffer");
            goto exit_1;
        }
    }
    inp_start = inp;

    if (posp)
        *posp = 0;

    if (fill) {
        size = fill(inp, 4);
        if (size < 4) {
            error("data corrupted");
            goto exit_2;
        }
    }

    chunksize = get_unaligned_le32(inp);
    if (chunksize == ARCHIVE_MAGICNUMBER) {
        if (!fill) {
            inp += 4;
            size -= 4;
        }
    } else {
        error("invalid header");
        goto exit_2;
    }

    if (posp)
        *posp += 4;

    for (;;) {

        if (fill) {
            size = fill(inp, 4);
            if (size == 0)
                break;
            if (size < 4) {
                error("data corrupted");
                goto exit_2;
            }
        } else if (size < 4) {
            /* empty or end-of-file */
            goto exit_3;
        }

        chunksize = get_unaligned_le32(inp);
        if (chunksize == ARCHIVE_MAGICNUMBER) {
            if (!fill) {
                inp += 4;
                size -= 4;
            }
            if (posp)
                *posp += 4;
            continue;
        }

        if (!fill && chunksize == 0) {
            /* empty or end-of-file */
            goto exit_3;
        }

        if (posp)
            *posp += 4;

        if (!fill) {
            inp += 4;
            size -= 4;
        } else {
            if (chunksize > LZ4_compressBound(uncomp_chunksize)) {
                error("chunk length is longer than allocated");
                goto exit_2;
            }
            size = fill(inp, chunksize);
            if (size < chunksize) {
                error("data corrupted");
                goto exit_2;
            }
        }

        if (out_len >= uncomp_chunksize) {
            dest_len = uncomp_chunksize;
            out_len -= dest_len;
        } else
            dest_len = out_len;

        ret = LZ4_decompress_fast(inp, outp, dest_len);
        chunksize = ret;

        if (ret < 0) {
            error("Decoding failed");
            goto exit_2;
        }

        ret = -1;
        if (flush && flush(outp, dest_len) != dest_len)
            goto exit_2;
        if (output)
            outp += dest_len;
        if (posp)
            *posp += chunksize;

        if (!fill) {
            size -= chunksize;

            if (size == 0)
                break;
            else if (size < 0) {
                error("data corrupted");
                goto exit_2;
            }
            inp += chunksize;
        }
    }

exit_3:
    ret = 0;
exit_2:
    if (!input)
        large_free(inp_start);
exit_1:
    if (!output)
        large_free(outp);
exit_0:
    return ret;
}
// END LZ4

// BEGIN LZMA

#define	MIN(a, b) (((a) < (b)) ? (a) : (b))

static long long read_int(unsigned char *ptr, int size)
{
    int i;
    long long ret = 0;

    for (i = 0; i < size; i++)
        ret = (ret << 8) | ptr[size-i-1];
    return ret;
}

#define ENDIAN_CONVERT(x) \
  x = (typeof(x))read_int((unsigned char *)&x, sizeof(x))


/* Small range coder implementation for lzma.
 *Copyright (C) 2006  Aurelien Jacobs < aurel@gnuage.org >
 *
 *Based on LzmaDecode.c from the LZMA SDK 4.22 (http://www.7-zip.org/)
 *Copyright (c) 1999-2005  Igor Pavlov
 */

#define LZMA_IOBUF_SIZE	0x10000

struct rc {
    long (*fill)(void*, unsigned long);
    uint8_t *ptr;
    uint8_t *buffer;
    uint8_t *buffer_end;
    long buffer_size;
    uint32_t code;
    uint32_t range;
    uint32_t bound;
    void (*error)(char *);
};


#define RC_TOP_BITS 24
#define RC_MOVE_BITS 5
#define RC_MODEL_TOTAL_BITS 11


static long nofill_lzma(void *buffer, unsigned long len)
{
    return -1;
}

/* Called twice: once at startup and once in rc_normalize() */
static void rc_read(struct rc *rc)
{
    rc->buffer_size = rc->fill((char *)rc->buffer, LZMA_IOBUF_SIZE);
    if (rc->buffer_size <= 0)
        rc->error("unexpected EOF");
    rc->ptr = rc->buffer;
    rc->buffer_end = rc->buffer + rc->buffer_size;
}

/* Called once */
static inline void rc_init(struct rc *rc,
                       long (*fill)(void*, unsigned long),
                       char *buffer, long buffer_size)
{
    if (fill)
        rc->fill = fill;
    else
        rc->fill = nofill_lzma;
    rc->buffer = (uint8_t *)buffer;
    rc->buffer_size = buffer_size;
    rc->buffer_end = rc->buffer + rc->buffer_size;
    rc->ptr = rc->buffer;

    rc->code = 0;
    rc->range = 0xFFFFFFFF;
}

static inline void rc_init_code(struct rc *rc)
{
    int i;

    for (i = 0; i < 5; i++) {
        if (rc->ptr >= rc->buffer_end)
            rc_read(rc);
        rc->code = (rc->code << 8) | *rc->ptr++;
    }
}


/* Called twice, but one callsite is in inline'd rc_is_bit_0_helper() */
static void rc_do_normalize(struct rc *rc)
{
    if (rc->ptr >= rc->buffer_end)
        rc_read(rc);
    rc->range <<= 8;
    rc->code = (rc->code << 8) | *rc->ptr++;
}
static inline void rc_normalize_lzma(struct rc *rc)
{
    if (rc->range < (1 << RC_TOP_BITS))
        rc_do_normalize(rc);
}

/* Called 9 times */
/* Why rc_is_bit_0_helper exists?
 *Because we want to always expose (rc->code < rc->bound) to optimizer
 */
static inline uint32_t rc_is_bit_0_helper(struct rc *rc, uint16_t *p)
{
    rc_normalize_lzma(rc);
    rc->bound = *p * (rc->range >> RC_MODEL_TOTAL_BITS);
    return rc->bound;
}
static inline int  rc_is_bit_0(struct rc *rc, uint16_t *p)
{
    uint32_t t = rc_is_bit_0_helper(rc, p);
    return rc->code < t;
}

/* Called ~10 times, but very small, thus inlined */
static inline void rc_update_bit_0(struct rc *rc, uint16_t *p)
{
    rc->range = rc->bound;
    *p += ((1 << RC_MODEL_TOTAL_BITS) - *p) >> RC_MOVE_BITS;
}
static inline void rc_update_bit_1(struct rc *rc, uint16_t *p)
{
    rc->range -= rc->bound;
    rc->code -= rc->bound;
    *p -= *p >> RC_MOVE_BITS;
}

/* Called 4 times in unlzma loop */
static int rc_get_bit(struct rc *rc, uint16_t *p, int *symbol)
{
    if (rc_is_bit_0(rc, p)) {
        rc_update_bit_0(rc, p);
        *symbol *= 2;
        return 0;
    } else {
        rc_update_bit_1(rc, p);
        *symbol = *symbol * 2 + 1;
        return 1;
    }
}

/* Called once */
static inline int rc_direct_bit(struct rc *rc)
{
    rc_normalize_lzma(rc);
    rc->range >>= 1;
    if (rc->code >= rc->range) {
        rc->code -= rc->range;
        return 1;
    }
    return 0;
}

/* Called twice */
static inline void rc_bit_tree_decode(struct rc *rc, uint16_t *p, int num_levels, int *symbol)
{
    int i = num_levels;

    *symbol = 1;
    while (i--)
        rc_get_bit(rc, p + *symbol, symbol);
    *symbol -= 1 << num_levels;
}


/*
 * Small lzma deflate implementation.
 * Copyright (C) 2006  Aurelien Jacobs < aurel@gnuage.org >
 *
 * Based on LzmaDecode.c from the LZMA SDK 4.22 (http://www.7-zip.org/)
 * Copyright (C) 1999-2005  Igor Pavlov
 */


struct lzma_header {
    uint8_t pos;
    uint32_t dict_size;
    uint64_t dst_size;
} __attribute__ ((packed)) ;


#define LZMA_BASE_SIZE 1846
#define LZMA_LIT_SIZE 768

#define LZMA_NUM_POS_BITS_MAX 4

#define LZMA_LEN_NUM_LOW_BITS 3
#define LZMA_LEN_NUM_MID_BITS 3
#define LZMA_LEN_NUM_HIGH_BITS 8

#define LZMA_LEN_CHOICE 0
#define LZMA_LEN_CHOICE_2 (LZMA_LEN_CHOICE + 1)
#define LZMA_LEN_LOW (LZMA_LEN_CHOICE_2 + 1)
#define LZMA_LEN_MID (LZMA_LEN_LOW \
              + (1 << (LZMA_NUM_POS_BITS_MAX + LZMA_LEN_NUM_LOW_BITS)))
#define LZMA_LEN_HIGH (LZMA_LEN_MID \
               +(1 << (LZMA_NUM_POS_BITS_MAX + LZMA_LEN_NUM_MID_BITS)))
#define LZMA_NUM_LEN_PROBS (LZMA_LEN_HIGH + (1 << LZMA_LEN_NUM_HIGH_BITS))

#define LZMA_NUM_STATES 12
#define LZMA_NUM_LIT_STATES 7

#define LZMA_START_POS_MODEL_INDEX 4
#define LZMA_END_POS_MODEL_INDEX 14
#define LZMA_NUM_FULL_DISTANCES (1 << (LZMA_END_POS_MODEL_INDEX >> 1))

#define LZMA_NUM_POS_SLOT_BITS 6
#define LZMA_NUM_LEN_TO_POS_STATES 4

#define LZMA_NUM_ALIGN_BITS 4

#define LZMA_MATCH_MIN_LEN 2

#define LZMA_IS_MATCH 0
#define LZMA_IS_REP (LZMA_IS_MATCH + (LZMA_NUM_STATES << LZMA_NUM_POS_BITS_MAX))
#define LZMA_IS_REP_G0 (LZMA_IS_REP + LZMA_NUM_STATES)
#define LZMA_IS_REP_G1 (LZMA_IS_REP_G0 + LZMA_NUM_STATES)
#define LZMA_IS_REP_G2 (LZMA_IS_REP_G1 + LZMA_NUM_STATES)
#define LZMA_IS_REP_0_LONG (LZMA_IS_REP_G2 + LZMA_NUM_STATES)
#define LZMA_POS_SLOT (LZMA_IS_REP_0_LONG \
               + (LZMA_NUM_STATES << LZMA_NUM_POS_BITS_MAX))
#define LZMA_SPEC_POS (LZMA_POS_SLOT \
               +(LZMA_NUM_LEN_TO_POS_STATES << LZMA_NUM_POS_SLOT_BITS))
#define LZMA_ALIGN (LZMA_SPEC_POS \
            + LZMA_NUM_FULL_DISTANCES - LZMA_END_POS_MODEL_INDEX)
#define LZMA_LEN_CODER (LZMA_ALIGN + (1 << LZMA_NUM_ALIGN_BITS))
#define LZMA_REP_LEN_CODER (LZMA_LEN_CODER + LZMA_NUM_LEN_PROBS)
#define LZMA_LITERAL (LZMA_REP_LEN_CODER + LZMA_NUM_LEN_PROBS)


struct writer {
    uint8_t *buffer;
    uint8_t previous_byte;
    size_t buffer_pos;
    int bufsize;
    size_t global_pos;
    long (*flush)(void*, unsigned long);
    struct lzma_header *header;
};

struct cstate {
    int state;
    uint32_t rep0, rep1, rep2, rep3;
};

static inline size_t get_pos(struct writer *wr)
{
    return
        wr->global_pos + wr->buffer_pos;
}

static inline uint8_t peek_old_byte(struct writer *wr,
                        uint32_t offs)
{
    if (!wr->flush) {
        int32_t pos;
        while (offs > wr->header->dict_size)
            offs -= wr->header->dict_size;
        pos = wr->buffer_pos - offs;
        return wr->buffer[pos];
    } else {
        uint32_t pos = wr->buffer_pos - offs;
        while (pos >= wr->header->dict_size)
            pos += wr->header->dict_size;
        return wr->buffer[pos];
    }

}

static inline int write_byte(struct writer *wr, uint8_t byte)
{
    wr->buffer[wr->buffer_pos++] = wr->previous_byte = byte;
    if (wr->flush && wr->buffer_pos == wr->header->dict_size) {
        wr->buffer_pos = 0;
        wr->global_pos += wr->header->dict_size;
        if (wr->flush((char *)wr->buffer, wr->header->dict_size)
                != wr->header->dict_size)
            return -1;
    }
    return 0;
}


static inline int copy_byte(struct writer *wr, uint32_t offs)
{
    return write_byte(wr, peek_old_byte(wr, offs));
}

static inline int copy_bytes(struct writer *wr,
                     uint32_t rep0, int len)
{
    do {
        if (copy_byte(wr, rep0))
            return -1;
        len--;
    } while (len != 0 && wr->buffer_pos < wr->header->dst_size);

    return len;
}

static inline int process_bit0(struct writer *wr, struct rc *rc,
                     struct cstate *cst, uint16_t *p,
                     int pos_state, uint16_t *prob,
                     int lc, uint32_t literal_pos_mask) {
    int mi = 1;
    rc_update_bit_0(rc, prob);
    prob = (p + LZMA_LITERAL +
        (LZMA_LIT_SIZE
         * (((get_pos(wr) & literal_pos_mask) << lc)
            + (wr->previous_byte >> (8 - lc))))
        );

    if (cst->state >= LZMA_NUM_LIT_STATES) {
        int match_byte = peek_old_byte(wr, cst->rep0);
        do {
            int bit;
            uint16_t *prob_lit;

            match_byte <<= 1;
            bit = match_byte & 0x100;
            prob_lit = prob + 0x100 + bit + mi;
            if (rc_get_bit(rc, prob_lit, &mi)) {
                if (!bit)
                    break;
            } else {
                if (bit)
                    break;
            }
        } while (mi < 0x100);
    }
    while (mi < 0x100) {
        uint16_t *prob_lit = prob + mi;
        rc_get_bit(rc, prob_lit, &mi);
    }
    if (cst->state < 4)
        cst->state = 0;
    else if (cst->state < 10)
        cst->state -= 3;
    else
        cst->state -= 6;

    return write_byte(wr, mi);
}

static inline int process_bit1(struct writer *wr, struct rc *rc,
                        struct cstate *cst, uint16_t *p,
                        int pos_state, uint16_t *prob) {
  int offset;
    uint16_t *prob_len;
    int num_bits;
    int len;

    rc_update_bit_1(rc, prob);
    prob = p + LZMA_IS_REP + cst->state;
    if (rc_is_bit_0(rc, prob)) {
        rc_update_bit_0(rc, prob);
        cst->rep3 = cst->rep2;
        cst->rep2 = cst->rep1;
        cst->rep1 = cst->rep0;
        cst->state = cst->state < LZMA_NUM_LIT_STATES ? 0 : 3;
        prob = p + LZMA_LEN_CODER;
    } else {
        rc_update_bit_1(rc, prob);
        prob = p + LZMA_IS_REP_G0 + cst->state;
        if (rc_is_bit_0(rc, prob)) {
            rc_update_bit_0(rc, prob);
            prob = (p + LZMA_IS_REP_0_LONG
                + (cst->state <<
                   LZMA_NUM_POS_BITS_MAX) +
                pos_state);
            if (rc_is_bit_0(rc, prob)) {
                rc_update_bit_0(rc, prob);

                cst->state = cst->state < LZMA_NUM_LIT_STATES ?
                    9 : 11;
                return copy_byte(wr, cst->rep0);
            } else {
                rc_update_bit_1(rc, prob);
            }
        } else {
            uint32_t distance;

            rc_update_bit_1(rc, prob);
            prob = p + LZMA_IS_REP_G1 + cst->state;
            if (rc_is_bit_0(rc, prob)) {
                rc_update_bit_0(rc, prob);
                distance = cst->rep1;
            } else {
                rc_update_bit_1(rc, prob);
                prob = p + LZMA_IS_REP_G2 + cst->state;
                if (rc_is_bit_0(rc, prob)) {
                    rc_update_bit_0(rc, prob);
                    distance = cst->rep2;
                } else {
                    rc_update_bit_1(rc, prob);
                    distance = cst->rep3;
                    cst->rep3 = cst->rep2;
                }
                cst->rep2 = cst->rep1;
            }
            cst->rep1 = cst->rep0;
            cst->rep0 = distance;
        }
        cst->state = cst->state < LZMA_NUM_LIT_STATES ? 8 : 11;
        prob = p + LZMA_REP_LEN_CODER;
    }

    prob_len = prob + LZMA_LEN_CHOICE;
    if (rc_is_bit_0(rc, prob_len)) {
        rc_update_bit_0(rc, prob_len);
        prob_len = (prob + LZMA_LEN_LOW
                + (pos_state <<
                   LZMA_LEN_NUM_LOW_BITS));
        offset = 0;
        num_bits = LZMA_LEN_NUM_LOW_BITS;
    } else {
        rc_update_bit_1(rc, prob_len);
        prob_len = prob + LZMA_LEN_CHOICE_2;
        if (rc_is_bit_0(rc, prob_len)) {
            rc_update_bit_0(rc, prob_len);
            prob_len = (prob + LZMA_LEN_MID
                    + (pos_state <<
                       LZMA_LEN_NUM_MID_BITS));
            offset = 1 << LZMA_LEN_NUM_LOW_BITS;
            num_bits = LZMA_LEN_NUM_MID_BITS;
        } else {
            rc_update_bit_1(rc, prob_len);
            prob_len = prob + LZMA_LEN_HIGH;
            offset = ((1 << LZMA_LEN_NUM_LOW_BITS)
                  + (1 << LZMA_LEN_NUM_MID_BITS));
            num_bits = LZMA_LEN_NUM_HIGH_BITS;
        }
    }

    rc_bit_tree_decode(rc, prob_len, num_bits, &len);
    len += offset;

    if (cst->state < 4) {
        int pos_slot;

        cst->state += LZMA_NUM_LIT_STATES;
        prob =
            p + LZMA_POS_SLOT +
            ((len <
              LZMA_NUM_LEN_TO_POS_STATES ? len :
              LZMA_NUM_LEN_TO_POS_STATES - 1)
             << LZMA_NUM_POS_SLOT_BITS);
        rc_bit_tree_decode(rc, prob,
                   LZMA_NUM_POS_SLOT_BITS,
                   &pos_slot);
        if (pos_slot >= LZMA_START_POS_MODEL_INDEX) {
            int i, mi;
            num_bits = (pos_slot >> 1) - 1;
            cst->rep0 = 2 | (pos_slot & 1);
            if (pos_slot < LZMA_END_POS_MODEL_INDEX) {
                cst->rep0 <<= num_bits;
                prob = p + LZMA_SPEC_POS +
                    cst->rep0 - pos_slot - 1;
            } else {
                num_bits -= LZMA_NUM_ALIGN_BITS;
                while (num_bits--)
                    cst->rep0 = (cst->rep0 << 1) |
                        rc_direct_bit(rc);
                prob = p + LZMA_ALIGN;
                cst->rep0 <<= LZMA_NUM_ALIGN_BITS;
                num_bits = LZMA_NUM_ALIGN_BITS;
            }
            i = 1;
            mi = 1;
            while (num_bits--) {
                if (rc_get_bit(rc, prob + mi, &mi))
                    cst->rep0 |= i;
                i <<= 1;
            }
        } else
            cst->rep0 = pos_slot;
        if (++(cst->rep0) == 0)
            return 0;
        if (cst->rep0 > wr->header->dict_size
                || cst->rep0 > get_pos(wr))
            return -1;
    }

    len += LZMA_MATCH_MIN_LEN;

    return copy_bytes(wr, cst->rep0, len);
}

int unlzma(unsigned char *buf, long in_len,
                  long (*fill)(void*, unsigned long),
                  long (*flush)(void*, unsigned long),
                  unsigned char *output,
                  long *posp,
                  void(*error)(char *x)
    )
{
    struct lzma_header header;
    int lc, pb, lp;
    uint32_t pos_state_mask;
    uint32_t literal_pos_mask;
    uint16_t *p;
    int num_probs;
    struct rc rc;
    int i, mi;
    struct writer wr;
    struct cstate cst;
    unsigned char *inbuf;
    int ret = -1;

    rc.error = error;

    if (buf)
        inbuf = buf;
    else
        inbuf = malloc(LZMA_IOBUF_SIZE);
    if (!inbuf) {
        error("Could not allocate input buffer");
        goto exit_0;
    }

    cst.state = 0;
    cst.rep0 = cst.rep1 = cst.rep2 = cst.rep3 = 1;

    wr.header = &header;
    wr.flush = flush;
    wr.global_pos = 0;
    wr.previous_byte = 0;
    wr.buffer_pos = 0;

    rc_init(&rc, fill, inbuf, in_len);

    for (i = 0; i < sizeof(header); i++) {
        if (rc.ptr >= rc.buffer_end)
            rc_read(&rc);
        ((unsigned char *)&header)[i] = *rc.ptr++;
    }

    if (header.pos >= (9 * 5 * 5)) {
        error("bad header");
        goto exit_1;
    }

    mi = 0;
    lc = header.pos;
    while (lc >= 9) {
        mi++;
        lc -= 9;
    }
    pb = 0;
    lp = mi;
    while (lp >= 5) {
        pb++;
        lp -= 5;
    }
    pos_state_mask = (1 << pb) - 1;
    literal_pos_mask = (1 << lp) - 1;

    ENDIAN_CONVERT(header.dict_size);
    ENDIAN_CONVERT(header.dst_size);

    if (header.dict_size == 0)
        header.dict_size = 1;

    if (output)
        wr.buffer = output;
    else {
        wr.bufsize = MIN(header.dst_size, header.dict_size);
        wr.buffer = large_malloc(wr.bufsize);
    }
    if (wr.buffer == NULL)
        goto exit_1;

    num_probs = LZMA_BASE_SIZE + (LZMA_LIT_SIZE << (lc + lp));
    p = (uint16_t *) large_malloc(num_probs * sizeof(*p));
    if (p == NULL)
        goto exit_2;
    num_probs = LZMA_LITERAL + (LZMA_LIT_SIZE << (lc + lp));
    for (i = 0; i < num_probs; i++)
        p[i] = (1 << RC_MODEL_TOTAL_BITS) >> 1;

    rc_init_code(&rc);

    while (get_pos(&wr) < header.dst_size) {
        int pos_state =	get_pos(&wr) & pos_state_mask;
        uint16_t *prob = p + LZMA_IS_MATCH +
            (cst.state << LZMA_NUM_POS_BITS_MAX) + pos_state;
        if (rc_is_bit_0(&rc, prob)) {
            if (process_bit0(&wr, &rc, &cst, p, pos_state, prob,
                    lc, literal_pos_mask)) {
                error("LZMA data is corrupt");
                goto exit_3;
            }
        } else {
            if (process_bit1(&wr, &rc, &cst, p, pos_state, prob)) {
                error("LZMA data is corrupt");
                goto exit_3;
            }
            if (cst.rep0 == 0)
                break;
        }
        if (rc.buffer_size <= 0)
            goto exit_3;
    }

    if (posp)
        *posp = rc.ptr-rc.buffer;
    if (!wr.flush || wr.flush(wr.buffer, wr.buffer_pos) == wr.buffer_pos)
        ret = 0;
exit_3:
    large_free(p);
exit_2:
    if (!output)
        large_free(wr.buffer);
exit_1:
    if (!buf)
        free(inbuf);
exit_0:
    return ret;
}

// END LZMA

// BEGIN GUNZIP

struct internal_state;

typedef struct z_stream_s {
    const unsigned char *next_in;   /* next input byte */
    unsigned long avail_in;  /* number of bytes available at next_in */
    unsigned long    total_in;  /* total nb of input bytes read so far */

    unsigned char    *next_out;  /* next output byte should be put there */
    unsigned long avail_out; /* remaining free space at next_out */
    unsigned long    total_out; /* total nb of bytes output so far */

    char     *msg;      /* last error message, NULL if no error */
    struct internal_state *state; /* not visible by applications */

    void     *workspace; /* memory allocated for this stream */

    int     data_type;  /* best guess about the data type: ascii or binary */
    unsigned long   adler;      /* adler32 value of the uncompressed data */
    unsigned long   reserved;   /* reserved for future use */
} z_stream;

typedef z_stream *z_streamp;

/*
   The application must update next_in and avail_in when avail_in has
   dropped to zero. It must update next_out and avail_out when avail_out
   has dropped to zero. The application must initialize zalloc, zfree and
   opaque before calling the init function. All other fields are set by the
   compression library and must not be updated by the application.

   The opaque value provided by the application will be passed as the first
   parameter for calls of zalloc and zfree. This can be useful for custom
   memory management. The compression library attaches no meaning to the
   opaque value.

   zalloc must return NULL if there is not enough memory for the object.
   If zlib is used in a multi-threaded application, zalloc and zfree must be
   thread safe.

   On 16-bit systems, the functions zalloc and zfree must be able to allocate
   exactly 65536 bytes, but will not be required to allocate more than this
   if the symbol MAXSEG_64K is defined (see zconf.h). WARNING: On MSDOS,
   pointers returned by zalloc for objects of exactly 65536 bytes *must*
   have their offset normalized to zero. The default allocation function
   provided by this library ensures this (see zutil.c). To reduce memory
   requirements and avoid any allocation of 64K objects, at the expense of
   compression ratio, compile the library with -DMAX_WBITS=14 (see zconf.h).

   The fields total_in and total_out can be used for statistics or
   progress reports. After compression, total_in holds the total size of
   the uncompressed data and may be saved for use in the decompressor
   (particularly if the decompressor wants to decompress everything in
   a single step).
*/

                        /* constants */

#define Z_NO_FLUSH      0
#define Z_PARTIAL_FLUSH 1 /* will be removed, use Z_SYNC_FLUSH instead */
#define Z_PACKET_FLUSH  2
#define Z_SYNC_FLUSH    3
#define Z_FULL_FLUSH    4
#define Z_FINISH        5
#define Z_BLOCK         6 /* Only for inflate at present */
/* Allowed flush values; see deflate() and inflate() below for details */

#define Z_OK            0
#define Z_STREAM_END    1
#define Z_NEED_DICT     2
#define Z_ERRNO        (-1)
#define Z_STREAM_ERROR (-2)
#define Z_DATA_ERROR   (-3)
#define Z_MEM_ERROR    (-4)
#define Z_BUF_ERROR    (-5)
#define Z_VERSION_ERROR (-6)
/* Return codes for the compression/decompression functions. Negative
 * values are errors, positive values are used for special but normal events.
 */

#define Z_NO_COMPRESSION         0
#define Z_BEST_SPEED             1
#define Z_BEST_COMPRESSION       9
#define Z_DEFAULT_COMPRESSION  (-1)
/* compression levels */

#define Z_FILTERED            1
#define Z_HUFFMAN_ONLY        2
#define Z_DEFAULT_STRATEGY    0
/* compression strategy; see deflateInit2() below for details */

#define Z_BINARY   0
#define Z_ASCII    1
#define Z_UNKNOWN  2
/* Possible values of the data_type field */

#define Z_DEFLATED   8
/* The deflate compression method (the only one supported in this version) */

                        /* basic functions */

extern int zlib_deflate_workspacesize (int windowBits, int memLevel);
/*
   Returns the number of bytes that needs to be allocated for a per-
   stream workspace with the specified parameters.  A pointer to this
   number of bytes should be returned in stream->workspace before
   you call zlib_deflateInit() or zlib_deflateInit2().  If you call
   zlib_deflateInit(), specify windowBits = MAX_WBITS and memLevel =
   MAX_MEM_LEVEL here.  If you call zlib_deflateInit2(), the windowBits
   and memLevel parameters passed to zlib_deflateInit2() must not
   exceed those passed here.
*/

extern int zlib_deflate_dfltcc_enabled (void);
/*
   Returns 1 if Deflate-Conversion facility is installed and enabled,
   otherwise 0.
*/

/*
extern int deflateInit (z_streamp strm, int level);

     Initializes the internal stream state for compression. The fields
   zalloc, zfree and opaque must be initialized before by the caller.
   If zalloc and zfree are set to NULL, deflateInit updates them to
   use default allocation functions.

     The compression level must be Z_DEFAULT_COMPRESSION, or between 0 and 9:
   1 gives best speed, 9 gives best compression, 0 gives no compression at
   all (the input data is simply copied a block at a time).
   Z_DEFAULT_COMPRESSION requests a default compromise between speed and
   compression (currently equivalent to level 6).

     deflateInit returns Z_OK if success, Z_MEM_ERROR if there was not
   enough memory, Z_STREAM_ERROR if level is not a valid compression level,
   Z_VERSION_ERROR if the zlib library version (zlib_version) is incompatible
   with the version assumed by the caller (ZLIB_VERSION).
   msg is set to null if there is no error message.  deflateInit does not
   perform any compression: this will be done by deflate().
*/


extern int zlib_deflate (z_streamp strm, int flush);
/*
    deflate compresses as much data as possible, and stops when the input
  buffer becomes empty or the output buffer becomes full. It may introduce some
  output latency (reading input without producing any output) except when
  forced to flush.

    The detailed semantics are as follows. deflate performs one or both of the
  following actions:

  - Compress more input starting at next_in and update next_in and avail_in
    accordingly. If not all input can be processed (because there is not
    enough room in the output buffer), next_in and avail_in are updated and
    processing will resume at this point for the next call of deflate().

  - Provide more output starting at next_out and update next_out and avail_out
    accordingly. This action is forced if the parameter flush is non zero.
    Forcing flush frequently degrades the compression ratio, so this parameter
    should be set only when necessary (in interactive applications).
    Some output may be provided even if flush is not set.

  Before the call of deflate(), the application should ensure that at least
  one of the actions is possible, by providing more input and/or consuming
  more output, and updating avail_in or avail_out accordingly; avail_out
  should never be zero before the call. The application can consume the
  compressed output when it wants, for example when the output buffer is full
  (avail_out == 0), or after each call of deflate(). If deflate returns Z_OK
  and with zero avail_out, it must be called again after making room in the
  output buffer because there might be more output pending.

    If the parameter flush is set to Z_SYNC_FLUSH, all pending output is
  flushed to the output buffer and the output is aligned on a byte boundary, so
  that the decompressor can get all input data available so far. (In particular
  avail_in is zero after the call if enough output space has been provided
  before the call.)  Flushing may degrade compression for some compression
  algorithms and so it should be used only when necessary.

    If flush is set to Z_FULL_FLUSH, all output is flushed as with
  Z_SYNC_FLUSH, and the compression state is reset so that decompression can
  restart from this point if previous compressed data has been damaged or if
  random access is desired. Using Z_FULL_FLUSH too often can seriously degrade
  the compression.

    If deflate returns with avail_out == 0, this function must be called again
  with the same value of the flush parameter and more output space (updated
  avail_out), until the flush is complete (deflate returns with non-zero
  avail_out).

    If the parameter flush is set to Z_FINISH, pending input is processed,
  pending output is flushed and deflate returns with Z_STREAM_END if there
  was enough output space; if deflate returns with Z_OK, this function must be
  called again with Z_FINISH and more output space (updated avail_out) but no
  more input data, until it returns with Z_STREAM_END or an error. After
  deflate has returned Z_STREAM_END, the only possible operations on the
  stream are deflateReset or deflateEnd.

    Z_FINISH can be used immediately after deflateInit if all the compression
  is to be done in a single step. In this case, avail_out must be at least
  0.1% larger than avail_in plus 12 bytes.  If deflate does not return
  Z_STREAM_END, then it must be called again as described above.

    deflate() sets strm->adler to the adler32 checksum of all input read
  so far (that is, total_in bytes).

    deflate() may update data_type if it can make a good guess about
  the input data type (Z_ASCII or Z_BINARY). In doubt, the data is considered
  binary. This field is only for information purposes and does not affect
  the compression algorithm in any manner.

    deflate() returns Z_OK if some progress has been made (more input
  processed or more output produced), Z_STREAM_END if all input has been
  consumed and all output has been produced (only when flush is set to
  Z_FINISH), Z_STREAM_ERROR if the stream state was inconsistent (for example
  if next_in or next_out was NULL), Z_BUF_ERROR if no progress is possible
  (for example avail_in or avail_out was zero).
*/


extern int zlib_deflateEnd (z_streamp strm);
/*
     All dynamically allocated data structures for this stream are freed.
   This function discards any unprocessed input and does not flush any
   pending output.

     deflateEnd returns Z_OK if success, Z_STREAM_ERROR if the
   stream state was inconsistent, Z_DATA_ERROR if the stream was freed
   prematurely (some input or output was discarded). In the error case,
   msg may be set but then points to a static string (which must not be
   deallocated).
*/


extern int zlib_inflate_workspacesize (void);
/*
   Returns the number of bytes that needs to be allocated for a per-
   stream workspace.  A pointer to this number of bytes should be
   returned in stream->workspace before calling zlib_inflateInit().
*/

/*
extern int zlib_inflateInit (z_streamp strm);

     Initializes the internal stream state for decompression. The fields
   next_in, avail_in, and workspace must be initialized before by
   the caller. If next_in is not NULL and avail_in is large enough (the exact
   value depends on the compression method), inflateInit determines the
   compression method from the zlib header and allocates all data structures
   accordingly; otherwise the allocation will be deferred to the first call of
   inflate.  If zalloc and zfree are set to NULL, inflateInit updates them to
   use default allocation functions.

     inflateInit returns Z_OK if success, Z_MEM_ERROR if there was not enough
   memory, Z_VERSION_ERROR if the zlib library version is incompatible with the
   version assumed by the caller.  msg is set to null if there is no error
   message. inflateInit does not perform any decompression apart from reading
   the zlib header if present: this will be done by inflate().  (So next_in and
   avail_in may be modified, but next_out and avail_out are unchanged.)
*/


extern int zlib_inflate (z_streamp strm, int flush);
/*
    inflate decompresses as much data as possible, and stops when the input
  buffer becomes empty or the output buffer becomes full. It may introduce
  some output latency (reading input without producing any output) except when
  forced to flush.

  The detailed semantics are as follows. inflate performs one or both of the
  following actions:

  - Decompress more input starting at next_in and update next_in and avail_in
    accordingly. If not all input can be processed (because there is not
    enough room in the output buffer), next_in is updated and processing
    will resume at this point for the next call of inflate().

  - Provide more output starting at next_out and update next_out and avail_out
    accordingly.  inflate() provides as much output as possible, until there
    is no more input data or no more space in the output buffer (see below
    about the flush parameter).

  Before the call of inflate(), the application should ensure that at least
  one of the actions is possible, by providing more input and/or consuming
  more output, and updating the next_* and avail_* values accordingly.
  The application can consume the uncompressed output when it wants, for
  example when the output buffer is full (avail_out == 0), or after each
  call of inflate(). If inflate returns Z_OK and with zero avail_out, it
  must be called again after making room in the output buffer because there
  might be more output pending.

    The flush parameter of inflate() can be Z_NO_FLUSH, Z_SYNC_FLUSH,
  Z_FINISH, or Z_BLOCK. Z_SYNC_FLUSH requests that inflate() flush as much
  output as possible to the output buffer. Z_BLOCK requests that inflate() stop
  if and when it gets to the next deflate block boundary. When decoding the
  zlib or gzip format, this will cause inflate() to return immediately after
  the header and before the first block. When doing a raw inflate, inflate()
  will go ahead and process the first block, and will return when it gets to
  the end of that block, or when it runs out of data.

    The Z_BLOCK option assists in appending to or combining deflate streams.
  Also to assist in this, on return inflate() will set strm->data_type to the
  number of unused bits in the last byte taken from strm->next_in, plus 64
  if inflate() is currently decoding the last block in the deflate stream,
  plus 128 if inflate() returned immediately after decoding an end-of-block
  code or decoding the complete header up to just before the first byte of the
  deflate stream. The end-of-block will not be indicated until all of the
  uncompressed data from that block has been written to strm->next_out.  The
  number of unused bits may in general be greater than seven, except when
  bit 7 of data_type is set, in which case the number of unused bits will be
  less than eight.

    inflate() should normally be called until it returns Z_STREAM_END or an
  error. However if all decompression is to be performed in a single step
  (a single call of inflate), the parameter flush should be set to
  Z_FINISH. In this case all pending input is processed and all pending
  output is flushed; avail_out must be large enough to hold all the
  uncompressed data. (The size of the uncompressed data may have been saved
  by the compressor for this purpose.) The next operation on this stream must
  be inflateEnd to deallocate the decompression state. The use of Z_FINISH
  is never required, but can be used to inform inflate that a faster approach
  may be used for the single inflate() call.

     In this implementation, inflate() always flushes as much output as
  possible to the output buffer, and always uses the faster approach on the
  first call. So the only effect of the flush parameter in this implementation
  is on the return value of inflate(), as noted below, or when it returns early
  because Z_BLOCK is used.

     If a preset dictionary is needed after this call (see inflateSetDictionary
  below), inflate sets strm->adler to the adler32 checksum of the dictionary
  chosen by the compressor and returns Z_NEED_DICT; otherwise it sets
  strm->adler to the adler32 checksum of all output produced so far (that is,
  total_out bytes) and returns Z_OK, Z_STREAM_END or an error code as described
  below. At the end of the stream, inflate() checks that its computed adler32
  checksum is equal to that saved by the compressor and returns Z_STREAM_END
  only if the checksum is correct.

    inflate() will decompress and check either zlib-wrapped or gzip-wrapped
  deflate data.  The header type is detected automatically.  Any information
  contained in the gzip header is not retained, so applications that need that
  information should instead use raw inflate, see inflateInit2() below, or
  inflateBack() and perform their own processing of the gzip header and
  trailer.

    inflate() returns Z_OK if some progress has been made (more input processed
  or more output produced), Z_STREAM_END if the end of the compressed data has
  been reached and all uncompressed output has been produced, Z_NEED_DICT if a
  preset dictionary is needed at this point, Z_DATA_ERROR if the input data was
  corrupted (input stream not conforming to the zlib format or incorrect check
  value), Z_STREAM_ERROR if the stream structure was inconsistent (for example
  if next_in or next_out was NULL), Z_MEM_ERROR if there was not enough memory,
  Z_BUF_ERROR if no progress is possible or if there was not enough room in the
  output buffer when Z_FINISH is used. Note that Z_BUF_ERROR is not fatal, and
  inflate() can be called again with more input and more output space to
  continue decompressing. If Z_DATA_ERROR is returned, the application may then
  call inflateSync() to look for a good compression block if a partial recovery
  of the data is desired.
*/


extern int zlib_inflateEnd (z_streamp strm);
/*
     All dynamically allocated data structures for this stream are freed.
   This function discards any unprocessed input and does not flush any
   pending output.

     inflateEnd returns Z_OK if success, Z_STREAM_ERROR if the stream state
   was inconsistent. In the error case, msg may be set but then points to a
   static string (which must not be deallocated).
*/

                        /* Advanced functions */

/*
    The following functions are needed only in some special applications.
*/

/*
extern int deflateInit2 (z_streamp strm,
                                     int  level,
                                     int  method,
                                     int  windowBits,
                                     int  memLevel,
                                     int  strategy);

     This is another version of deflateInit with more compression options. The
   fields next_in, zalloc, zfree and opaque must be initialized before by
   the caller.

     The method parameter is the compression method. It must be Z_DEFLATED in
   this version of the library.

     The windowBits parameter is the base two logarithm of the window size
   (the size of the history buffer).  It should be in the range 8..15 for this
   version of the library. Larger values of this parameter result in better
   compression at the expense of memory usage. The default value is 15 if
   deflateInit is used instead.

     The memLevel parameter specifies how much memory should be allocated
   for the internal compression state. memLevel=1 uses minimum memory but
   is slow and reduces compression ratio; memLevel=9 uses maximum memory
   for optimal speed. The default value is 8. See zconf.h for total memory
   usage as a function of windowBits and memLevel.

     The strategy parameter is used to tune the compression algorithm. Use the
   value Z_DEFAULT_STRATEGY for normal data, Z_FILTERED for data produced by a
   filter (or predictor), or Z_HUFFMAN_ONLY to force Huffman encoding only (no
   string match).  Filtered data consists mostly of small values with a
   somewhat random distribution. In this case, the compression algorithm is
   tuned to compress them better. The effect of Z_FILTERED is to force more
   Huffman coding and less string matching; it is somewhat intermediate
   between Z_DEFAULT and Z_HUFFMAN_ONLY. The strategy parameter only affects
   the compression ratio but not the correctness of the compressed output even
   if it is not set appropriately.

      deflateInit2 returns Z_OK if success, Z_MEM_ERROR if there was not enough
   memory, Z_STREAM_ERROR if a parameter is invalid (such as an invalid
   method). msg is set to null if there is no error message.  deflateInit2 does
   not perform any compression: this will be done by deflate().
*/

extern int zlib_deflateReset (z_streamp strm);
/*
     This function is equivalent to deflateEnd followed by deflateInit,
   but does not free and reallocate all the internal compression state.
   The stream will keep the same compression level and any other attributes
   that may have been set by deflateInit2.

      deflateReset returns Z_OK if success, or Z_STREAM_ERROR if the source
   stream state was inconsistent (such as zalloc or state being NULL).
*/

static inline unsigned long deflateBound(unsigned long s)
{
    return s + ((s + 7) >> 3) + ((s + 63) >> 6) + 11;
}

/*
extern int inflateInit2 (z_streamp strm, int  windowBits);

     This is another version of inflateInit with an extra parameter. The
   fields next_in, avail_in, zalloc, zfree and opaque must be initialized
   before by the caller.

     The windowBits parameter is the base two logarithm of the maximum window
   size (the size of the history buffer).  It should be in the range 8..15 for
   this version of the library. The default value is 15 if inflateInit is used
   instead. windowBits must be greater than or equal to the windowBits value
   provided to deflateInit2() while compressing, or it must be equal to 15 if
   deflateInit2() was not used. If a compressed stream with a larger window
   size is given as input, inflate() will return with the error code
   Z_DATA_ERROR instead of trying to allocate a larger window.

     windowBits can also be -8..-15 for raw inflate. In this case, -windowBits
   determines the window size. inflate() will then process raw deflate data,
   not looking for a zlib or gzip header, not generating a check value, and not
   looking for any check values for comparison at the end of the stream. This
   is for use with other formats that use the deflate compressed data format
   such as zip.  Those formats provide their own check values. If a custom
   format is developed using the raw deflate format for compressed data, it is
   recommended that a check value such as an adler32 or a crc32 be applied to
   the uncompressed data as is done in the zlib, gzip, and zip formats.  For
   most applications, the zlib format should be used as is. Note that comments
   above on the use in deflateInit2() applies to the magnitude of windowBits.

     windowBits can also be greater than 15 for optional gzip decoding. Add
   32 to windowBits to enable zlib and gzip decoding with automatic header
   detection, or add 16 to decode only the gzip format (the zlib format will
   return a Z_DATA_ERROR).  If a gzip stream is being decoded, strm->adler is
   a crc32 instead of an adler32.

     inflateInit2 returns Z_OK if success, Z_MEM_ERROR if there was not enough
   memory, Z_STREAM_ERROR if a parameter is invalid (such as a null strm). msg
   is set to null if there is no error message.  inflateInit2 does not perform
   any decompression apart from reading the zlib header if present: this will
   be done by inflate(). (So next_in and avail_in may be modified, but next_out
   and avail_out are unchanged.)
*/

extern int zlib_inflateReset (z_streamp strm);
/*
     This function is equivalent to inflateEnd followed by inflateInit,
   but does not free and reallocate all the internal decompression state.
   The stream will keep attributes that may have been set by inflateInit2.

      inflateReset returns Z_OK if success, or Z_STREAM_ERROR if the source
   stream state was inconsistent (such as zalloc or state being NULL).
*/

extern int zlib_inflateIncomp (z_stream *strm);
/*
     This function adds the data at next_in (avail_in bytes) to the output
   history without performing any output.  There must be no pending output,
   and the decompressor must be expecting to see the start of a block.
   Calling this function is equivalent to decompressing a stored block
   containing the data at next_in (except that the data is not output).
*/

#define zlib_deflateInit(strm, level) \
    zlib_deflateInit2((strm), (level), Z_DEFLATED, MAX_WBITS, \
                  DEF_MEM_LEVEL, Z_DEFAULT_STRATEGY)
#define zlib_inflateInit(strm) \
    zlib_inflateInit2((strm), DEF_WBITS)

extern int zlib_deflateInit2(z_streamp strm, int  level, int  method,
                                      int windowBits, int memLevel,
                                      int strategy);
extern int zlib_inflateInit2(z_streamp strm, int  windowBits);

#if !defined(_Z_UTIL_H) && !defined(NO_DUMMY_DECL)
    struct internal_state {int dummy;}; /* hack for buggy compilers */
#endif

/* Utility function: initialize zlib, unpack binary blob, clean up zlib,
 * return len or negative error code. */
extern int zlib_inflate_blob(void *dst, unsigned dst_sz, const void *src, unsigned src_sz);



/* inftrees.h -- header to use inftrees.c
 * Copyright (C) 1995-2005 Mark Adler
 * For conditions of distribution and use, see copyright notice in zlib.h
 */

/* WARNING: this file should *not* be used by applications. It is
   part of the implementation of the compression library and is
   subject to change. Applications should only use zlib.h.
 */

/* Structure for decoding tables.  Each entry provides either the
   information needed to do the operation requested by the code that
   indexed that table entry, or it provides a pointer to another
   table that indexes more bits of the code.  op indicates whether
   the entry is a pointer to another table, a literal, a length or
   distance, an end-of-block, or an invalid code.  For a table
   pointer, the low four bits of op is the number of index bits of
   that table.  For a length or distance, the low four bits of op
   is the number of extra bits to get after the code.  bits is
   the number of bits in this code or part of the code to drop off
   of the bit buffer.  val is the actual byte to output in the case
   of a literal, the base length or distance, or the offset from
   the current table to the next table.  Each entry is four bytes. */
typedef struct {
    unsigned char op;           /* operation, extra bits, table bits */
    unsigned char bits;         /* bits in this part of the code */
    unsigned short val;         /* offset in table or code value */
} code;

/* op values as set by inflate_table():
    00000000 - literal
    0000tttt - table link, tttt != 0 is the number of table index bits
    0001eeee - length or distance, eeee is the number of extra bits
    01100000 - end of block
    01000000 - invalid code
 */

/* Maximum size of dynamic tree.  The maximum found in a long but non-
   exhaustive search was 1444 code structures (852 for length/literals
   and 592 for distances, the latter actually the result of an
   exhaustive search).  The true maximum is not known, but the value
   below is more than safe. */
#define ENOUGH 2048
#define MAXD 592

/* Type of code to build for inftable() */
typedef enum {
    CODES,
    LENS,
    DISTS
} codetype;

typedef enum {
    HEAD,       /* i: waiting for magic header */
    FLAGS,      /* i: waiting for method and flags (gzip) */
    TIME,       /* i: waiting for modification time (gzip) */
    OS,         /* i: waiting for extra flags and operating system (gzip) */
    EXLEN,      /* i: waiting for extra length (gzip) */
    EXTRA,      /* i: waiting for extra bytes (gzip) */
    NAME,       /* i: waiting for end of file name (gzip) */
    COMMENT,    /* i: waiting for end of comment (gzip) */
    HCRC,       /* i: waiting for header crc (gzip) */
    DICTID,     /* i: waiting for dictionary check value */
    DICT,       /* waiting for inflateSetDictionary() call */
        TYPE,       /* i: waiting for type bits, including last-flag bit */
        TYPEDO,     /* i: same, but skip check to exit inflate on new block */
        STORED,     /* i: waiting for stored size (length and complement) */
        COPY,       /* i/o: waiting for input or output to copy stored block */
        TABLE,      /* i: waiting for dynamic block table lengths */
        LENLENS,    /* i: waiting for code length code lengths */
        CODELENS,   /* i: waiting for length/lit and distance code lengths */
            LEN,        /* i: waiting for length/lit code */
            LENEXT,     /* i: waiting for length extra bits */
            DIST,       /* i: waiting for distance code */
            DISTEXT,    /* i: waiting for distance extra bits */
            MATCH,      /* o: waiting for output space to copy string */
            LIT,        /* o: waiting for output space to write literal */
    CHECK,      /* i: waiting for 32-bit check value */
    LENGTH,     /* i: waiting for 32-bit length (gzip) */
    DONE,       /* finished check, done -- remain here until reset */
    BAD,        /* got a data error -- remain here until reset */
    MEM,        /* got an inflate() memory error -- remain here until reset */
    SYNC        /* looking for synchronization bytes to restart inflate() */
} inflate_mode;

/*
    State transitions between above modes -

    (most modes can go to the BAD or MEM mode -- not shown for clarity)

    Process header:
        HEAD -> (gzip) or (zlib)
        (gzip) -> FLAGS -> TIME -> OS -> EXLEN -> EXTRA -> NAME
        NAME -> COMMENT -> HCRC -> TYPE
        (zlib) -> DICTID or TYPE
        DICTID -> DICT -> TYPE
    Read deflate blocks:
            TYPE -> STORED or TABLE or LEN or CHECK
            STORED -> COPY -> TYPE
            TABLE -> LENLENS -> CODELENS -> LEN
    Read deflate codes:
                LEN -> LENEXT or LIT or TYPE
                LENEXT -> DIST -> DISTEXT -> MATCH -> LEN
                LIT -> LEN
    Process trailer:
        CHECK -> LENGTH -> DONE
 */

/* state maintained between inflate() calls.  Approximately 7K bytes. */
struct inflate_state {
    inflate_mode mode;          /* current inflate mode */
    int last;                   /* true if processing last block */
    int wrap;                   /* bit 0 true for zlib, bit 1 true for gzip */
    int havedict;               /* true if dictionary provided */
    int flags;                  /* gzip header method and flags (0 if zlib) */
    unsigned dmax;              /* zlib header max distance (INFLATE_STRICT) */
    unsigned long check;        /* protected copy of check value */
    unsigned long total;        /* protected copy of output count */
 /*   gz_headerp head; */           /* where to save gzip header information */
        /* sliding window */
    unsigned wbits;             /* log base 2 of requested window size */
    unsigned wsize;             /* window size or zero if not using window */
    unsigned whave;             /* valid bytes in the window */
    unsigned write;             /* window write index */
    unsigned char *window;  /* allocated sliding window, if needed */
        /* bit accumulator */
    unsigned long hold;         /* input bit accumulator */
    unsigned bits;              /* number of bits in "in" */
        /* for string and stored block copying */
    unsigned length;            /* literal or length of data to copy */
    unsigned offset;            /* distance back to copy string from */
        /* for table and code decoding */
    unsigned extra;             /* extra bits needed */
        /* fixed and dynamic code tables */
    code const *lencode;    /* starting table for length/literal codes */
    code const *distcode;   /* starting table for distance codes */
    unsigned lenbits;           /* index bits for lencode */
    unsigned distbits;          /* index bits for distcode */
        /* dynamic table building */
    unsigned ncode;             /* number of code length code lengths */
    unsigned nlen;              /* number of length code lengths */
    unsigned ndist;             /* number of distance code lengths */
    unsigned have;              /* number of code lengths in lens[] */
    code *next;             /* next available space in codes[] */
    unsigned short lens[320];   /* temporary storage for code lengths */
    unsigned short work[288];   /* work area for code table building */
    code codes[ENOUGH];         /* space for code tables */
};

/* Reverse the bytes in a 32-bit value */
#define REVERSE(q) \
    ((((q) >> 24) & 0xff) + (((q) >> 8) & 0xff00) + \
     (((q) & 0xff00) << 8) + (((q) & 0xff) << 24))

#define MAXBITS 15

/*
   Build a set of tables to decode the provided canonical Huffman code.
   The code lengths are lens[0..codes-1].  The result starts at *table,
   whose indices are 0..2^bits-1.  work is a writable array of at least
   lens shorts, which is used as a work area.  type is the type of code
   to be generated, CODES, LENS, or DISTS.  On return, zero is success,
   -1 is an invalid code, and +1 means that ENOUGH isn't enough.  table
   on return points to the next available entry's address.  bits is the
   requested root table index bits, and on return it is the actual root
   table index bits.  It will differ if the request is greater than the
   longest code or if it is less than the shortest code.
 */
int zlib_inflate_table(codetype type, unsigned short *lens, unsigned codes,
            code **table, unsigned *bits, unsigned short *work)
{
    unsigned len;               /* a code's length in bits */
    unsigned sym;               /* index of code symbols */
    unsigned min, max;          /* minimum and maximum code lengths */
    unsigned root;              /* number of index bits for root table */
    unsigned curr;              /* number of index bits for current table */
    unsigned drop;              /* code bits to drop for sub-table */
    int left;                   /* number of prefix codes available */
    unsigned used;              /* code entries in table used */
    unsigned huff;              /* Huffman code */
    unsigned incr;              /* for incrementing code, index */
    unsigned fill;              /* index for replicating entries */
    unsigned low;               /* low bits for current root entry */
    unsigned mask;              /* mask for low root bits */
    code this;                  /* table entry for duplication */
    code *next;             /* next available space in table */
    const unsigned short *base;     /* base value table to use */
    const unsigned short *extra;    /* extra bits table to use */
    int end;                    /* use base and extra for symbol > end */
    unsigned short count[MAXBITS+1];    /* number of codes of each length */
    unsigned short offs[MAXBITS+1];     /* offsets in table for each length */
    static const unsigned short lbase[31] = { /* Length codes 257..285 base */
        3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31,
        35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258, 0, 0};
    static const unsigned short lext[31] = { /* Length codes 257..285 extra */
        16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18,
        19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 16, 201, 196};
    static const unsigned short dbase[32] = { /* Distance codes 0..29 base */
        1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193,
        257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145,
        8193, 12289, 16385, 24577, 0, 0};
    static const unsigned short dext[32] = { /* Distance codes 0..29 extra */
        16, 16, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22,
        23, 23, 24, 24, 25, 25, 26, 26, 27, 27,
        28, 28, 29, 29, 64, 64};

    /*
       Process a set of code lengths to create a canonical Huffman code.  The
       code lengths are lens[0..codes-1].  Each length corresponds to the
       symbols 0..codes-1.  The Huffman code is generated by first sorting the
       symbols by length from short to long, and retaining the symbol order
       for codes with equal lengths.  Then the code starts with all zero bits
       for the first code of the shortest length, and the codes are integer
       increments for the same length, and zeros are appended as the length
       increases.  For the deflate format, these bits are stored backwards
       from their more natural integer increment ordering, and so when the
       decoding tables are built in the large loop below, the integer codes
       are incremented backwards.

       This routine assumes, but does not check, that all of the entries in
       lens[] are in the range 0..MAXBITS.  The caller must assure this.
       1..MAXBITS is interpreted as that code length.  zero means that that
       symbol does not occur in this code.

       The codes are sorted by computing a count of codes for each length,
       creating from that a table of starting indices for each length in the
       sorted table, and then entering the symbols in order in the sorted
       table.  The sorted table is work[], with that space being provided by
       the caller.

       The length counts are used for other purposes as well, i.e. finding
       the minimum and maximum length codes, determining if there are any
       codes at all, checking for a valid set of lengths, and looking ahead
       at length counts to determine sub-table sizes when building the
       decoding tables.
     */

    /* accumulate lengths for codes (assumes lens[] all in 0..MAXBITS) */
    for (len = 0; len <= MAXBITS; len++)
        count[len] = 0;
    for (sym = 0; sym < codes; sym++)
        count[lens[sym]]++;

    /* bound code lengths, force root to be within code lengths */
    root = *bits;
    for (max = MAXBITS; max >= 1; max--)
        if (count[max] != 0) break;
    if (root > max) root = max;
    if (max == 0) {                     /* no symbols to code at all */
        this.op = (unsigned char)64;    /* invalid code marker */
        this.bits = (unsigned char)1;
        this.val = (unsigned short)0;
        *(*table)++ = this;             /* make a table to force an error */
        *(*table)++ = this;
        *bits = 1;
        return 0;     /* no symbols, but wait for decoding to report error */
    }
    for (min = 1; min < MAXBITS; min++)
        if (count[min] != 0) break;
    if (root < min) root = min;

    /* check for an over-subscribed or incomplete set of lengths */
    left = 1;
    for (len = 1; len <= MAXBITS; len++) {
        left <<= 1;
        left -= count[len];
        if (left < 0) return -1;        /* over-subscribed */
    }
    if (left > 0 && (type == CODES || max != 1))
        return -1;                      /* incomplete set */

    /* generate offsets into symbol table for each length for sorting */
    offs[1] = 0;
    for (len = 1; len < MAXBITS; len++)
        offs[len + 1] = offs[len] + count[len];

    /* sort symbols by length, by symbol order within each length */
    for (sym = 0; sym < codes; sym++)
        if (lens[sym] != 0) work[offs[lens[sym]]++] = (unsigned short)sym;

    /*
       Create and fill in decoding tables.  In this loop, the table being
       filled is at next and has curr index bits.  The code being used is huff
       with length len.  That code is converted to an index by dropping drop
       bits off of the bottom.  For codes where len is less than drop + curr,
       those top drop + curr - len bits are incremented through all values to
       fill the table with replicated entries.

       root is the number of index bits for the root table.  When len exceeds
       root, sub-tables are created pointed to by the root entry with an index
       of the low root bits of huff.  This is saved in low to check for when a
       new sub-table should be started.  drop is zero when the root table is
       being filled, and drop is root when sub-tables are being filled.

       When a new sub-table is needed, it is necessary to look ahead in the
       code lengths to determine what size sub-table is needed.  The length
       counts are used for this, and so count[] is decremented as codes are
       entered in the tables.

       used keeps track of how many table entries have been allocated from the
       provided *table space.  It is checked when a LENS table is being made
       against the space in *table, ENOUGH, minus the maximum space needed by
       the worst case distance code, MAXD.  This should never happen, but the
       sufficiency of ENOUGH has not been proven exhaustively, hence the check.
       This assumes that when type == LENS, bits == 9.

       sym increments through all symbols, and the loop terminates when
       all codes of length max, i.e. all codes, have been processed.  This
       routine permits incomplete codes, so another loop after this one fills
       in the rest of the decoding tables with invalid code markers.
     */

    /* set up for code type */
    switch (type) {
    case CODES:
        base = extra = work;    /* dummy value--not used */
        end = 19;
        break;
    case LENS:
        base = lbase;
        base -= 257;
        extra = lext;
        extra -= 257;
        end = 256;
        break;
    default:            /* DISTS */
        base = dbase;
        extra = dext;
        end = -1;
    }

    /* initialize state for loop */
    huff = 0;                   /* starting code */
    sym = 0;                    /* starting code symbol */
    len = min;                  /* starting code length */
    next = *table;              /* current table to fill in */
    curr = root;                /* current table index bits */
    drop = 0;                   /* current bits to drop from code for index */
    low = (unsigned)(-1);       /* trigger new sub-table when len > root */
    used = 1U << root;          /* use root table entries */
    mask = used - 1;            /* mask for comparing low */

    /* check available table space */
    if (type == LENS && used >= ENOUGH - MAXD)
        return 1;

    /* process all codes and make table entries */
    for (;;) {
        /* create table entry */
        this.bits = (unsigned char)(len - drop);
        if ((int)(work[sym]) < end) {
            this.op = (unsigned char)0;
            this.val = work[sym];
        }
        else if ((int)(work[sym]) > end) {
            this.op = (unsigned char)(extra[work[sym]]);
            this.val = base[work[sym]];
        }
        else {
            this.op = (unsigned char)(32 + 64);         /* end of block */
            this.val = 0;
        }

        /* replicate for those indices with low len bits equal to huff */
        incr = 1U << (len - drop);
        fill = 1U << curr;
        min = fill;                 /* save offset to next table */
        do {
            fill -= incr;
            next[(huff >> drop) + fill] = this;
        } while (fill != 0);

        /* backwards increment the len-bit code huff */
        incr = 1U << (len - 1);
        while (huff & incr)
            incr >>= 1;
        if (incr != 0) {
            huff &= incr - 1;
            huff += incr;
        }
        else
            huff = 0;

        /* go to next symbol, update count, len */
        sym++;
        if (--(count[len]) == 0) {
            if (len == max) break;
            len = lens[work[sym]];
        }

        /* create new sub-table if needed */
        if (len > root && (huff & mask) != low) {
            /* if first time, transition to sub-tables */
            if (drop == 0)
                drop = root;

            /* increment past last table */
            next += min;            /* here min is 1 << curr */

            /* determine length of next table */
            curr = len - drop;
            left = (int)(1 << curr);
            while (curr + drop < max) {
                left -= count[curr + drop];
                if (left <= 0) break;
                curr++;
                left <<= 1;
            }

            /* check for enough space */
            used += 1U << curr;
            if (type == LENS && used >= ENOUGH - MAXD)
                return 1;

            /* point entry in root table to sub-table */
            low = huff & mask;
            (*table)[low].op = (unsigned char)curr;
            (*table)[low].bits = (unsigned char)root;
            (*table)[low].val = (unsigned short)(next - *table);
        }
    }

    /*
       Fill in rest of table for incomplete codes.  This loop is similar to the
       loop above in incrementing huff for table indices.  It is assumed that
       len is equal to curr + drop, so there is no loop needed to increment
       through high index bits.  When the current sub-table is filled, the loop
       drops back to the root table to fill in any remaining entries there.
     */
    this.op = (unsigned char)64;                /* invalid code marker */
    this.bits = (unsigned char)(len - drop);
    this.val = (unsigned short)0;
    while (huff != 0) {
        /* when done with sub-table, drop back to root table */
        if (drop != 0 && (huff & mask) != low) {
            drop = 0;
            len = root;
            next = *table;
            this.bits = (unsigned char)len;
        }

        /* put invalid code marker in table */
        next[huff >> drop] = this;

        /* backwards increment the len-bit code huff */
        incr = 1U << (len - 1);
        while (huff & incr)
            incr >>= 1;
        if (incr != 0) {
            huff &= incr - 1;
            huff += incr;
        }
        else
            huff = 0;
    }

    /* set return parameters */
    *table += used;
    *bits = root;
    return 0;
}

union uu {
    unsigned short us;
    unsigned char b[2];
};

/* Endian independent version */
static inline unsigned short
get_unaligned16(const unsigned short *p)
{
    union uu  mm;
    unsigned char *b = (unsigned char *)p;

    mm.b[0] = b[0];
    mm.b[1] = b[1];
    return mm.us;
}

/*
   Decode literal, length, and distance codes and write out the resulting
   literal and match bytes until either not enough input or output is
   available, an end-of-block is encountered, or a data error is encountered.
   When large enough input and output buffers are supplied to inflate(), for
   example, a 16K input buffer and a 64K output buffer, more than 95% of the
   inflate execution time is spent in this routine.

   Entry assumptions:

        state->mode == LEN
        strm->avail_in >= 6
        strm->avail_out >= 258
        start >= strm->avail_out
        state->bits < 8

   On return, state->mode is one of:

        LEN -- ran out of enough output space or enough available input
        TYPE -- reached end of block code, inflate() to interpret next block
        BAD -- error in block data

   Notes:

    - The maximum input bits used by a length/distance pair is 15 bits for the
      length code, 5 bits for the length extra, 15 bits for the distance code,
      and 13 bits for the distance extra.  This totals 48 bits, or six bytes.
      Therefore if strm->avail_in >= 6, then there is enough input to avoid
      checking for available input while decoding.

    - The maximum bytes that a single length/distance pair can output is 258
      bytes, which is the maximum length that can be coded.  inflate_fast()
      requires strm->avail_out >= 258 for each loop to avoid checking for
      output space.

    - @start:	inflate()'s starting value for strm->avail_out
 */
void inflate_fast(z_streamp strm, unsigned start)
{
    struct inflate_state *state;
    const unsigned char *in;    /* local strm->next_in */
    const unsigned char *last;  /* while in < last, enough input available */
    unsigned char *out;         /* local strm->next_out */
    unsigned char *beg;         /* inflate()'s initial strm->next_out */
    unsigned char *end;         /* while out < end, enough space available */
#ifdef INFLATE_STRICT
    unsigned dmax;              /* maximum distance from zlib header */
#endif
    unsigned wsize;             /* window size or zero if not using window */
    unsigned whave;             /* valid bytes in the window */
    unsigned write;             /* window write index */
    unsigned char *window;      /* allocated sliding window, if wsize != 0 */
    unsigned long hold;         /* local strm->hold */
    unsigned bits;              /* local strm->bits */
    code const *lcode;          /* local strm->lencode */
    code const *dcode;          /* local strm->distcode */
    unsigned lmask;             /* mask for first level of length codes */
    unsigned dmask;             /* mask for first level of distance codes */
    code this;                  /* retrieved table entry */
    unsigned op;                /* code bits, operation, extra bits, or */
                                /*  window position, window bytes to copy */
    unsigned len;               /* match length, unused bytes */
    unsigned dist;              /* match distance */
    unsigned char *from;        /* where to copy match from */

    /* copy state to local variables */
    state = (struct inflate_state *)strm->state;
    in = strm->next_in;
    last = in + (strm->avail_in - 5);
    out = strm->next_out;
    beg = out - (start - strm->avail_out);
    end = out + (strm->avail_out - 257);
#ifdef INFLATE_STRICT
    dmax = state->dmax;
#endif
    wsize = state->wsize;
    whave = state->whave;
    write = state->write;
    window = state->window;
    hold = state->hold;
    bits = state->bits;
    lcode = state->lencode;
    dcode = state->distcode;
    lmask = (1U << state->lenbits) - 1;
    dmask = (1U << state->distbits) - 1;

    /* decode literals and length/distances until end-of-block or not enough
       input data or output space */
    do {
        if (bits < 15) {
            hold += (unsigned long)(*in++) << bits;
            bits += 8;
            hold += (unsigned long)(*in++) << bits;
            bits += 8;
        }
        this = lcode[hold & lmask];
      dolen:
        op = (unsigned)(this.bits);
        hold >>= op;
        bits -= op;
        op = (unsigned)(this.op);
        if (op == 0) {                          /* literal */
            *out++ = (unsigned char)(this.val);
        }
        else if (op & 16) {                     /* length base */
            len = (unsigned)(this.val);
            op &= 15;                           /* number of extra bits */
            if (op) {
                if (bits < op) {
                    hold += (unsigned long)(*in++) << bits;
                    bits += 8;
                }
                len += (unsigned)hold & ((1U << op) - 1);
                hold >>= op;
                bits -= op;
            }
            if (bits < 15) {
                hold += (unsigned long)(*in++) << bits;
                bits += 8;
                hold += (unsigned long)(*in++) << bits;
                bits += 8;
            }
            this = dcode[hold & dmask];
          dodist:
            op = (unsigned)(this.bits);
            hold >>= op;
            bits -= op;
            op = (unsigned)(this.op);
            if (op & 16) {                      /* distance base */
                dist = (unsigned)(this.val);
                op &= 15;                       /* number of extra bits */
                if (bits < op) {
                    hold += (unsigned long)(*in++) << bits;
                    bits += 8;
                    if (bits < op) {
                        hold += (unsigned long)(*in++) << bits;
                        bits += 8;
                    }
                }
                dist += (unsigned)hold & ((1U << op) - 1);
#ifdef INFLATE_STRICT
                if (dist > dmax) {
                    strm->msg = (char *)"invalid distance too far back";
                    state->mode = BAD;
                    break;
                }
#endif
                hold >>= op;
                bits -= op;
                op = (unsigned)(out - beg);     /* max distance in output */
                if (dist > op) {                /* see if copy from window */
                    op = dist - op;             /* distance back in window */
                    if (op > whave) {
                        strm->msg = (char *)"invalid distance too far back";
                        state->mode = BAD;
                        break;
                    }
                    from = window;
                    if (write == 0) {           /* very common case */
                        from += wsize - op;
                        if (op < len) {         /* some from window */
                            len -= op;
                            do {
                                *out++ = *from++;
                            } while (--op);
                            from = out - dist;  /* rest from output */
                        }
                    }
                    else if (write < op) {      /* wrap around window */
                        from += wsize + write - op;
                        op -= write;
                        if (op < len) {         /* some from end of window */
                            len -= op;
                            do {
                                *out++ = *from++;
                            } while (--op);
                            from = window;
                            if (write < len) {  /* some from start of window */
                                op = write;
                                len -= op;
                                do {
                                    *out++ = *from++;
                                } while (--op);
                                from = out - dist;      /* rest from output */
                            }
                        }
                    }
                    else {                      /* contiguous in window */
                        from += write - op;
                        if (op < len) {         /* some from window */
                            len -= op;
                            do {
                                *out++ = *from++;
                            } while (--op);
                            from = out - dist;  /* rest from output */
                        }
                    }
                    while (len > 2) {
                        *out++ = *from++;
                        *out++ = *from++;
                        *out++ = *from++;
                        len -= 3;
                    }
                    if (len) {
                        *out++ = *from++;
                        if (len > 1)
                            *out++ = *from++;
                    }
                }
                else {
            unsigned short *sout;
            unsigned long loops;

                    from = out - dist;          /* copy direct from output */
            /* minimum length is three */
            /* Align out addr */
            if (!((long)(out - 1) & 1)) {
            *out++ = *from++;
            len--;
            }
            sout = (unsigned short *)(out);
            if (dist > 2) {
            unsigned short *sfrom;

            sfrom = (unsigned short *)(from);
            loops = len >> 1;
            do {
//                if (IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))
//                *sout++ = *sfrom++;
//                else
                *sout++ = get_unaligned16(sfrom++);
            } while (--loops);
            out = (unsigned char *)sout;
            from = (unsigned char *)sfrom;
            } else { /* dist == 1 or dist == 2 */
            unsigned short pat16;

            pat16 = *(sout-1);
            if (dist == 1) {
                union uu mm;
                /* copy one char pattern to both bytes */
                mm.us = pat16;
                mm.b[0] = mm.b[1];
                pat16 = mm.us;
            }
            loops = len >> 1;
            do
                *sout++ = pat16;
            while (--loops);
            out = (unsigned char *)sout;
            }
            if (len & 1)
            *out++ = *from++;
                }
            }
            else if ((op & 64) == 0) {          /* 2nd level distance code */
                this = dcode[this.val + (hold & ((1U << op) - 1))];
                goto dodist;
            }
            else {
                strm->msg = (char *)"invalid distance code";
                state->mode = BAD;
                break;
            }
        }
        else if ((op & 64) == 0) {              /* 2nd level length code */
            this = lcode[this.val + (hold & ((1U << op) - 1))];
            goto dolen;
        }
        else if (op & 32) {                     /* end-of-block */
            state->mode = TYPE;
            break;
        }
        else {
            strm->msg = (char *)"invalid literal/length code";
            state->mode = BAD;
            break;
        }
    } while (in < last && out < end);

    /* return unused bytes (on entry, bits < 8, so in won't go too far back) */
    len = bits >> 3;
    in -= len;
    bits -= len << 3;
    hold &= (1U << bits) - 1;

    /* update state and return */
    strm->next_in = in;
    strm->next_out = out;
    strm->avail_in = (unsigned)(in < last ? 5 + (last - in) : 5 - (in - last));
    strm->avail_out = (unsigned)(out < end ?
                                 257 + (end - out) : 257 - (out - end));
    state->hold = hold;
    state->bits = bits;
    return;
}

#ifdef CONFIG_ZLIB_DFLTCC
#  include "../zlib_dfltcc/dfltcc.h"
#else
#define INFLATE_RESET_HOOK(strm) do {} while (0)
#define INFLATE_TYPEDO_HOOK(strm, flush) do {} while (0)
#define INFLATE_NEED_UPDATEWINDOW(strm) 1
#define INFLATE_NEED_CHECKSUM(strm) 1
#endif


#ifndef MAX_WBITS
#  define MAX_WBITS   15 /* 32K LZ77 window */
#endif


struct inflate_workspace {
    struct inflate_state inflate_state;
#ifdef CONFIG_ZLIB_DFLTCC
    struct dfltcc_state dfltcc_state;
    unsigned char working_window[(1 << MAX_WBITS) + PAGE_SIZE];
#else
    unsigned char working_window[(1 << MAX_WBITS)];
#endif
};

#ifdef CONFIG_ZLIB_DFLTCC
/* dfltcc_state must be doubleword aligned for DFLTCC call */
static_assert(offsetof(struct inflate_workspace, dfltcc_state) % 8 == 0);
#endif

#define WS(strm) ((struct inflate_workspace *)(strm->workspace))


int zlib_inflate_workspacesize(void)
{
    return sizeof(struct inflate_workspace);
}

int zlib_inflateReset(z_streamp strm)
{
    struct inflate_state *state;

    if (strm == NULL || strm->state == NULL) return Z_STREAM_ERROR;
    state = (struct inflate_state *)strm->state;
    strm->total_in = strm->total_out = state->total = 0;
    strm->msg = NULL;
    strm->adler = 1;        /* to support ill-conceived Java test suite */
    state->mode = HEAD;
    state->last = 0;
    state->havedict = 0;
    state->dmax = 32768U;
    state->hold = 0;
    state->bits = 0;
    state->lencode = state->distcode = state->next = state->codes;

    /* Initialise Window */
    state->wsize = 1U << state->wbits;
    state->write = 0;
    state->whave = 0;

    INFLATE_RESET_HOOK(strm);
    return Z_OK;
}

int zlib_inflateInit2(z_streamp strm, int windowBits)
{
    struct inflate_state *state;

    if (strm == NULL) return Z_STREAM_ERROR;
    strm->msg = NULL;                 /* in case we return an error */

    state = &WS(strm)->inflate_state;
    strm->state = (struct internal_state *)state;

    if (windowBits < 0) {
        state->wrap = 0;
        windowBits = -windowBits;
    }
    else {
        state->wrap = (windowBits >> 4) + 1;
    }
    if (windowBits < 8 || windowBits > 15) {
        return Z_STREAM_ERROR;
    }
    state->wbits = (unsigned)windowBits;
#ifdef CONFIG_ZLIB_DFLTCC
    /*
     * DFLTCC requires the window to be page aligned.
     * Thus, we overallocate and take the aligned portion of the buffer.
     */
    state->window = PTR_ALIGN(&WS(strm)->working_window[0], PAGE_SIZE);
#else
    state->window = &WS(strm)->working_window[0];
#endif

    return zlib_inflateReset(strm);
}

/*
   Return state with length and distance decoding tables and index sizes set to
   fixed code decoding.  This returns fixed tables from inffixed.h.
 */

static const code lenfix[512] = {
        {96,7,0},{0,8,80},{0,8,16},{20,8,115},{18,7,31},{0,8,112},{0,8,48},
        {0,9,192},{16,7,10},{0,8,96},{0,8,32},{0,9,160},{0,8,0},{0,8,128},
        {0,8,64},{0,9,224},{16,7,6},{0,8,88},{0,8,24},{0,9,144},{19,7,59},
        {0,8,120},{0,8,56},{0,9,208},{17,7,17},{0,8,104},{0,8,40},{0,9,176},
        {0,8,8},{0,8,136},{0,8,72},{0,9,240},{16,7,4},{0,8,84},{0,8,20},
        {21,8,227},{19,7,43},{0,8,116},{0,8,52},{0,9,200},{17,7,13},{0,8,100},
        {0,8,36},{0,9,168},{0,8,4},{0,8,132},{0,8,68},{0,9,232},{16,7,8},
        {0,8,92},{0,8,28},{0,9,152},{20,7,83},{0,8,124},{0,8,60},{0,9,216},
        {18,7,23},{0,8,108},{0,8,44},{0,9,184},{0,8,12},{0,8,140},{0,8,76},
        {0,9,248},{16,7,3},{0,8,82},{0,8,18},{21,8,163},{19,7,35},{0,8,114},
        {0,8,50},{0,9,196},{17,7,11},{0,8,98},{0,8,34},{0,9,164},{0,8,2},
        {0,8,130},{0,8,66},{0,9,228},{16,7,7},{0,8,90},{0,8,26},{0,9,148},
        {20,7,67},{0,8,122},{0,8,58},{0,9,212},{18,7,19},{0,8,106},{0,8,42},
        {0,9,180},{0,8,10},{0,8,138},{0,8,74},{0,9,244},{16,7,5},{0,8,86},
        {0,8,22},{64,8,0},{19,7,51},{0,8,118},{0,8,54},{0,9,204},{17,7,15},
        {0,8,102},{0,8,38},{0,9,172},{0,8,6},{0,8,134},{0,8,70},{0,9,236},
        {16,7,9},{0,8,94},{0,8,30},{0,9,156},{20,7,99},{0,8,126},{0,8,62},
        {0,9,220},{18,7,27},{0,8,110},{0,8,46},{0,9,188},{0,8,14},{0,8,142},
        {0,8,78},{0,9,252},{96,7,0},{0,8,81},{0,8,17},{21,8,131},{18,7,31},
        {0,8,113},{0,8,49},{0,9,194},{16,7,10},{0,8,97},{0,8,33},{0,9,162},
        {0,8,1},{0,8,129},{0,8,65},{0,9,226},{16,7,6},{0,8,89},{0,8,25},
        {0,9,146},{19,7,59},{0,8,121},{0,8,57},{0,9,210},{17,7,17},{0,8,105},
        {0,8,41},{0,9,178},{0,8,9},{0,8,137},{0,8,73},{0,9,242},{16,7,4},
        {0,8,85},{0,8,21},{16,8,258},{19,7,43},{0,8,117},{0,8,53},{0,9,202},
        {17,7,13},{0,8,101},{0,8,37},{0,9,170},{0,8,5},{0,8,133},{0,8,69},
        {0,9,234},{16,7,8},{0,8,93},{0,8,29},{0,9,154},{20,7,83},{0,8,125},
        {0,8,61},{0,9,218},{18,7,23},{0,8,109},{0,8,45},{0,9,186},{0,8,13},
        {0,8,141},{0,8,77},{0,9,250},{16,7,3},{0,8,83},{0,8,19},{21,8,195},
        {19,7,35},{0,8,115},{0,8,51},{0,9,198},{17,7,11},{0,8,99},{0,8,35},
        {0,9,166},{0,8,3},{0,8,131},{0,8,67},{0,9,230},{16,7,7},{0,8,91},
        {0,8,27},{0,9,150},{20,7,67},{0,8,123},{0,8,59},{0,9,214},{18,7,19},
        {0,8,107},{0,8,43},{0,9,182},{0,8,11},{0,8,139},{0,8,75},{0,9,246},
        {16,7,5},{0,8,87},{0,8,23},{64,8,0},{19,7,51},{0,8,119},{0,8,55},
        {0,9,206},{17,7,15},{0,8,103},{0,8,39},{0,9,174},{0,8,7},{0,8,135},
        {0,8,71},{0,9,238},{16,7,9},{0,8,95},{0,8,31},{0,9,158},{20,7,99},
        {0,8,127},{0,8,63},{0,9,222},{18,7,27},{0,8,111},{0,8,47},{0,9,190},
        {0,8,15},{0,8,143},{0,8,79},{0,9,254},{96,7,0},{0,8,80},{0,8,16},
        {20,8,115},{18,7,31},{0,8,112},{0,8,48},{0,9,193},{16,7,10},{0,8,96},
        {0,8,32},{0,9,161},{0,8,0},{0,8,128},{0,8,64},{0,9,225},{16,7,6},
        {0,8,88},{0,8,24},{0,9,145},{19,7,59},{0,8,120},{0,8,56},{0,9,209},
        {17,7,17},{0,8,104},{0,8,40},{0,9,177},{0,8,8},{0,8,136},{0,8,72},
        {0,9,241},{16,7,4},{0,8,84},{0,8,20},{21,8,227},{19,7,43},{0,8,116},
        {0,8,52},{0,9,201},{17,7,13},{0,8,100},{0,8,36},{0,9,169},{0,8,4},
        {0,8,132},{0,8,68},{0,9,233},{16,7,8},{0,8,92},{0,8,28},{0,9,153},
        {20,7,83},{0,8,124},{0,8,60},{0,9,217},{18,7,23},{0,8,108},{0,8,44},
        {0,9,185},{0,8,12},{0,8,140},{0,8,76},{0,9,249},{16,7,3},{0,8,82},
        {0,8,18},{21,8,163},{19,7,35},{0,8,114},{0,8,50},{0,9,197},{17,7,11},
        {0,8,98},{0,8,34},{0,9,165},{0,8,2},{0,8,130},{0,8,66},{0,9,229},
        {16,7,7},{0,8,90},{0,8,26},{0,9,149},{20,7,67},{0,8,122},{0,8,58},
        {0,9,213},{18,7,19},{0,8,106},{0,8,42},{0,9,181},{0,8,10},{0,8,138},
        {0,8,74},{0,9,245},{16,7,5},{0,8,86},{0,8,22},{64,8,0},{19,7,51},
        {0,8,118},{0,8,54},{0,9,205},{17,7,15},{0,8,102},{0,8,38},{0,9,173},
        {0,8,6},{0,8,134},{0,8,70},{0,9,237},{16,7,9},{0,8,94},{0,8,30},
        {0,9,157},{20,7,99},{0,8,126},{0,8,62},{0,9,221},{18,7,27},{0,8,110},
        {0,8,46},{0,9,189},{0,8,14},{0,8,142},{0,8,78},{0,9,253},{96,7,0},
        {0,8,81},{0,8,17},{21,8,131},{18,7,31},{0,8,113},{0,8,49},{0,9,195},
        {16,7,10},{0,8,97},{0,8,33},{0,9,163},{0,8,1},{0,8,129},{0,8,65},
        {0,9,227},{16,7,6},{0,8,89},{0,8,25},{0,9,147},{19,7,59},{0,8,121},
        {0,8,57},{0,9,211},{17,7,17},{0,8,105},{0,8,41},{0,9,179},{0,8,9},
        {0,8,137},{0,8,73},{0,9,243},{16,7,4},{0,8,85},{0,8,21},{16,8,258},
        {19,7,43},{0,8,117},{0,8,53},{0,9,203},{17,7,13},{0,8,101},{0,8,37},
        {0,9,171},{0,8,5},{0,8,133},{0,8,69},{0,9,235},{16,7,8},{0,8,93},
        {0,8,29},{0,9,155},{20,7,83},{0,8,125},{0,8,61},{0,9,219},{18,7,23},
        {0,8,109},{0,8,45},{0,9,187},{0,8,13},{0,8,141},{0,8,77},{0,9,251},
        {16,7,3},{0,8,83},{0,8,19},{21,8,195},{19,7,35},{0,8,115},{0,8,51},
        {0,9,199},{17,7,11},{0,8,99},{0,8,35},{0,9,167},{0,8,3},{0,8,131},
        {0,8,67},{0,9,231},{16,7,7},{0,8,91},{0,8,27},{0,9,151},{20,7,67},
        {0,8,123},{0,8,59},{0,9,215},{18,7,19},{0,8,107},{0,8,43},{0,9,183},
        {0,8,11},{0,8,139},{0,8,75},{0,9,247},{16,7,5},{0,8,87},{0,8,23},
        {64,8,0},{19,7,51},{0,8,119},{0,8,55},{0,9,207},{17,7,15},{0,8,103},
        {0,8,39},{0,9,175},{0,8,7},{0,8,135},{0,8,71},{0,9,239},{16,7,9},
        {0,8,95},{0,8,31},{0,9,159},{20,7,99},{0,8,127},{0,8,63},{0,9,223},
        {18,7,27},{0,8,111},{0,8,47},{0,9,191},{0,8,15},{0,8,143},{0,8,79},
        {0,9,255}
    };

    static const code distfix[32] = {
        {16,5,1},{23,5,257},{19,5,17},{27,5,4097},{17,5,5},{25,5,1025},
        {21,5,65},{29,5,16385},{16,5,3},{24,5,513},{20,5,33},{28,5,8193},
        {18,5,9},{26,5,2049},{22,5,129},{64,5,0},{16,5,2},{23,5,385},
        {19,5,25},{27,5,6145},{17,5,7},{25,5,1537},{21,5,97},{29,5,24577},
        {16,5,4},{24,5,769},{20,5,49},{28,5,12289},{18,5,13},{26,5,3073},
        {22,5,193},{64,5,0}
    };


static void zlib_fixedtables(struct inflate_state *state)
{
    state->lencode = lenfix;
    state->lenbits = 9;
    state->distcode = distfix;
    state->distbits = 5;
}


/*
   Update the window with the last wsize (normally 32K) bytes written before
   returning. This is only called when a window is already in use, or when
   output has been written during this inflate call, but the end of the deflate
   stream has not been reached yet. It is also called to window dictionary data
   when a dictionary is loaded.

   Providing output buffers larger than 32K to inflate() should provide a speed
   advantage, since only the last 32K of output is copied to the sliding window
   upon return from inflate(), and since all distances after the first 32K of
   output will fall in the output data, making match copies simpler and faster.
   The advantage may be dependent on the size of the processor's data caches.
 */
static void zlib_updatewindow(z_streamp strm, unsigned out)
{
    struct inflate_state *state;
    unsigned copy, dist;

    state = (struct inflate_state *)strm->state;

    /* copy state->wsize or less output bytes into the circular window */
    copy = out - strm->avail_out;
    if (copy >= state->wsize) {
        memcpy(state->window, strm->next_out - state->wsize, state->wsize);
        state->write = 0;
        state->whave = state->wsize;
    }
    else {
        dist = state->wsize - state->write;
        if (dist > copy) dist = copy;
        memcpy(state->window + state->write, strm->next_out - copy, dist);
        copy -= dist;
        if (copy) {
            memcpy(state->window, strm->next_out - copy, copy);
            state->write = copy;
            state->whave = state->wsize;
        }
        else {
            state->write += dist;
            if (state->write == state->wsize) state->write = 0;
            if (state->whave < state->wsize) state->whave += dist;
        }
    }
}


/*
 * At the end of a Deflate-compressed PPP packet, we expect to have seen
 * a `stored' block type value but not the (zero) length bytes.
 */
/*
   Returns true if inflate is currently at the end of a block generated by
   Z_SYNC_FLUSH or Z_FULL_FLUSH. This function is used by one PPP
   implementation to provide an additional safety check. PPP uses
   Z_SYNC_FLUSH but removes the length bytes of the resulting empty stored
   block. When decompressing, PPP checks that at the end of input packet,
   inflate is waiting for these length bytes.
 */
static int zlib_inflateSyncPacket(z_streamp strm)
{
    struct inflate_state *state;

    if (strm == NULL || strm->state == NULL) return Z_STREAM_ERROR;
    state = (struct inflate_state *)strm->state;

    if (state->mode == STORED && state->bits == 0) {
    state->mode = TYPE;
        return Z_OK;
    }
    return Z_DATA_ERROR;
}

/* Macros for inflate(): */

/* check function to use adler32() for zlib or crc32() for gzip */
#define UPDATE(check, buf, len) zlib_adler32(check, buf, len)

/* Load registers with state in inflate() for speed */
#define LOAD() \
    do { \
        put = strm->next_out; \
        left = strm->avail_out; \
        next = strm->next_in; \
        have = strm->avail_in; \
        hold = state->hold; \
        bits = state->bits; \
    } while (0)

/* Restore state from registers in inflate() */
#define RESTORE() \
    do { \
        strm->next_out = put; \
        strm->avail_out = left; \
        strm->next_in = next; \
        strm->avail_in = have; \
        state->hold = hold; \
        state->bits = bits; \
    } while (0)

/* Clear the input bit accumulator */
#define INITBITS() \
    do { \
        hold = 0; \
        bits = 0; \
    } while (0)

/* Get a byte of input into the bit accumulator, or return from inflate()
   if there is no input available. */
#define PULLBYTE() \
    do { \
        if (have == 0) goto inf_leave; \
        have--; \
        hold += (unsigned long)(*next++) << bits; \
        bits += 8; \
    } while (0)

/* Assure that there are at least n bits in the bit accumulator.  If there is
   not enough available input to do that, then return from inflate(). */
#define NEEDBITS(n) \
    do { \
        while (bits < (unsigned)(n)) \
            PULLBYTE(); \
    } while (0)

/* Return the low n bits of the bit accumulator (n < 16) */
#define BITS(n) \
    ((unsigned)hold & ((1U << (n)) - 1))

/* Remove n bits from the bit accumulator */
#define DROPBITS(n) \
    do { \
        hold >>= (n); \
        bits -= (unsigned)(n); \
    } while (0)

/* Remove zero to seven bits as needed to go to a byte boundary */
#define BYTEBITS() \
    do { \
        hold >>= bits & 7; \
        bits -= bits & 7; \
    } while (0)

/*
   inflate() uses a state machine to process as much input data and generate as
   much output data as possible before returning.  The state machine is
   structured roughly as follows:

    for (;;) switch (state) {
    ...
    case STATEn:
        if (not enough input data or output space to make progress)
            return;
        ... make progress ...
        state = STATEm;
        break;
    ...
    }

   so when inflate() is called again, the same case is attempted again, and
   if the appropriate resources are provided, the machine proceeds to the
   next state.  The NEEDBITS() macro is usually the way the state evaluates
   whether it can proceed or should return.  NEEDBITS() does the return if
   the requested bits are not available.  The typical use of the BITS macros
   is:

        NEEDBITS(n);
        ... do something with BITS(n) ...
        DROPBITS(n);

   where NEEDBITS(n) either returns from inflate() if there isn't enough
   input left to load n bits into the accumulator, or it continues.  BITS(n)
   gives the low n bits in the accumulator.  When done, DROPBITS(n) drops
   the low n bits off the accumulator.  INITBITS() clears the accumulator
   and sets the number of available bits to zero.  BYTEBITS() discards just
   enough bits to put the accumulator on a byte boundary.  After BYTEBITS()
   and a NEEDBITS(8), then BITS(8) would return the next byte in the stream.

   NEEDBITS(n) uses PULLBYTE() to get an available byte of input, or to return
   if there is no input available.  The decoding of variable length codes uses
   PULLBYTE() directly in order to pull just enough bytes to decode the next
   code, and no more.

   Some states loop until they get enough input, making sure that enough
   state information is maintained to continue the loop where it left off
   if NEEDBITS() returns in the loop.  For example, want, need, and keep
   would all have to actually be part of the saved state in case NEEDBITS()
   returns:

    case STATEw:
        while (want < need) {
            NEEDBITS(n);
            keep[want++] = BITS(n);
            DROPBITS(n);
        }
        state = STATEx;
    case STATEx:

   As shown above, if the next state is also the next case, then the break
   is omitted.

   A state may also return if there is not enough output space available to
   complete that state.  Those states are copying stored data, writing a
   literal byte, and copying a matching string.

   When returning, a "goto inf_leave" is used to update the total counters,
   update the check value, and determine whether any progress has been made
   during that inflate() call in order to return the proper return code.
   Progress is defined as a change in either strm->avail_in or strm->avail_out.
   When there is a window, goto inf_leave will update the window with the last
   output written.  If a goto inf_leave occurs in the middle of decompression
   and there is no window currently, goto inf_leave will create one and copy
   output to the window for the next call of inflate().

   In this implementation, the flush parameter of inflate() only affects the
   return code (per zlib.h).  inflate() always writes as much as possible to
   strm->next_out, given the space available and the provided input--the effect
   documented in zlib.h of Z_SYNC_FLUSH.  Furthermore, inflate() always defers
   the allocation of and copying into a sliding window until necessary, which
   provides the effect documented in zlib.h for Z_FINISH when the entire input
   stream available.  So the only thing the flush parameter actually does is:
   when flush is set to Z_FINISH, inflate() cannot return Z_OK.  Instead it
   will return Z_BUF_ERROR if it has not reached the end of the stream.
 */

#define BASE 65521L /* largest prime smaller than 65536 */
#define NMAX 5552
/* NMAX is the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1 */

#define DO1(buf,i)  {s1 += buf[i]; s2 += s1;}
#define DO2(buf,i)  DO1(buf,i); DO1(buf,i+1);
#define DO4(buf,i)  DO2(buf,i); DO2(buf,i+2);
#define DO8(buf,i)  DO4(buf,i); DO4(buf,i+4);
#define DO16(buf)   DO8(buf,0); DO8(buf,8);

static inline unsigned long zlib_adler32(unsigned long adler,
                 const unsigned char *buf,
                 unsigned int len)
{
    unsigned long s1 = adler & 0xffff;
    unsigned long s2 = (adler >> 16) & 0xffff;
    int k;

    if (buf == NULL) return 1L;

    while (len > 0) {
        k = len < NMAX ? len : NMAX;
        len -= k;
        while (k >= 16) {
            DO16(buf);
        buf += 16;
            k -= 16;
        }
        if (k != 0) do {
            s1 += *buf++;
        s2 += s1;
        } while (--k);
        s1 %= BASE;
        s2 %= BASE;
    }
    return (s2 << 16) | s1;
}

int zlib_inflate(z_streamp strm, int flush)
{
    struct inflate_state *state;
    const unsigned char *next;  /* next input */
    unsigned char *put;         /* next output */
    unsigned have, left;        /* available input and output */
    unsigned long hold;         /* bit buffer */
    unsigned bits;              /* bits in bit buffer */
    unsigned in, out;           /* save starting available input and output */
    unsigned copy;              /* number of stored or match bytes to copy */
    unsigned char *from;        /* where to copy match bytes from */
    code this;                  /* current decoding table entry */
    code last;                  /* parent table entry */
    unsigned len;               /* length to copy for repeats, bits to drop */
    int ret;                    /* return code */
    static const unsigned short order[19] = /* permutation of code lengths */
        {16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15};

    /* Do not check for strm->next_out == NULL here as ppc zImage
       inflates to strm->next_out = 0 */

    if (strm == NULL || strm->state == NULL ||
        (strm->next_in == NULL && strm->avail_in != 0))
        return Z_STREAM_ERROR;

    state = (struct inflate_state *)strm->state;

    if (state->mode == TYPE) state->mode = TYPEDO;      /* skip check */
    LOAD();
    in = have;
    out = left;
    ret = Z_OK;
    for (;;)
        switch (state->mode) {
        case HEAD:
            if (state->wrap == 0) {
                state->mode = TYPEDO;
                break;
            }
            NEEDBITS(16);
            if (
                ((BITS(8) << 8) + (hold >> 8)) % 31) {
                strm->msg = (char *)"incorrect header check";
                state->mode = BAD;
                break;
            }
            if (BITS(4) != Z_DEFLATED) {
                strm->msg = (char *)"unknown compression method";
                state->mode = BAD;
                break;
            }
            DROPBITS(4);
            len = BITS(4) + 8;
            if (len > state->wbits) {
                strm->msg = (char *)"invalid window size";
                state->mode = BAD;
                break;
            }
            state->dmax = 1U << len;
            strm->adler = state->check = zlib_adler32(0L, NULL, 0);
            state->mode = hold & 0x200 ? DICTID : TYPE;
            INITBITS();
            break;
        case DICTID:
            NEEDBITS(32);
            strm->adler = state->check = REVERSE(hold);
            INITBITS();
            state->mode = DICT;
         __attribute__((fallthrough));
        case DICT:
            if (state->havedict == 0) {
                RESTORE();
                return Z_NEED_DICT;
            }
            strm->adler = state->check = zlib_adler32(0L, NULL, 0);
            state->mode = TYPE;
         __attribute__((fallthrough));
        case TYPE:
            if (flush == Z_BLOCK) goto inf_leave;
         __attribute__((fallthrough));
        case TYPEDO:
            INFLATE_TYPEDO_HOOK(strm, flush);
            if (state->last) {
                BYTEBITS();
                state->mode = CHECK;
                break;
            }
            NEEDBITS(3);
            state->last = BITS(1);
            DROPBITS(1);
            switch (BITS(2)) {
            case 0:                             /* stored block */
                state->mode = STORED;
                break;
            case 1:                             /* fixed block */
                zlib_fixedtables(state);
                state->mode = LEN;              /* decode codes */
                break;
            case 2:                             /* dynamic block */
                state->mode = TABLE;
                break;
            case 3:
                strm->msg = (char *)"invalid block type";
                state->mode = BAD;
            }
            DROPBITS(2);
            break;
        case STORED:
            BYTEBITS();                         /* go to byte boundary */
            NEEDBITS(32);
            if ((hold & 0xffff) != ((hold >> 16) ^ 0xffff)) {
                strm->msg = (char *)"invalid stored block lengths";
                state->mode = BAD;
                break;
            }
            state->length = (unsigned)hold & 0xffff;
            INITBITS();
            state->mode = COPY;
         __attribute__((fallthrough));
        case COPY:
            copy = state->length;
            if (copy) {
                if (copy > have) copy = have;
                if (copy > left) copy = left;
                if (copy == 0) goto inf_leave;
                memcpy(put, next, copy);
                have -= copy;
                next += copy;
                left -= copy;
                put += copy;
                state->length -= copy;
                break;
            }
            state->mode = TYPE;
            break;
        case TABLE:
            NEEDBITS(14);
            state->nlen = BITS(5) + 257;
            DROPBITS(5);
            state->ndist = BITS(5) + 1;
            DROPBITS(5);
            state->ncode = BITS(4) + 4;
            DROPBITS(4);
#ifndef PKZIP_BUG_WORKAROUND
            if (state->nlen > 286 || state->ndist > 30) {
                strm->msg = (char *)"too many length or distance symbols";
                state->mode = BAD;
                break;
            }
#endif
            state->have = 0;
            state->mode = LENLENS;
         __attribute__((fallthrough));
        case LENLENS:
            while (state->have < state->ncode) {
                NEEDBITS(3);
                state->lens[order[state->have++]] = (unsigned short)BITS(3);
                DROPBITS(3);
            }
            while (state->have < 19)
                state->lens[order[state->have++]] = 0;
            state->next = state->codes;
            state->lencode = (code const *)(state->next);
            state->lenbits = 7;
            ret = zlib_inflate_table(CODES, state->lens, 19, &(state->next),
                                &(state->lenbits), state->work);
            if (ret) {
                strm->msg = (char *)"invalid code lengths set";
                state->mode = BAD;
                break;
            }
            state->have = 0;
            state->mode = CODELENS;
        __attribute__((fallthrough));
        case CODELENS:
            while (state->have < state->nlen + state->ndist) {
                for (;;) {
                    this = state->lencode[BITS(state->lenbits)];
                    if ((unsigned)(this.bits) <= bits) break;
                    PULLBYTE();
                }
                if (this.val < 16) {
                    NEEDBITS(this.bits);
                    DROPBITS(this.bits);
                    state->lens[state->have++] = this.val;
                }
                else {
                    if (this.val == 16) {
                        NEEDBITS(this.bits + 2);
                        DROPBITS(this.bits);
                        if (state->have == 0) {
                            strm->msg = (char *)"invalid bit length repeat";
                            state->mode = BAD;
                            break;
                        }
                        len = state->lens[state->have - 1];
                        copy = 3 + BITS(2);
                        DROPBITS(2);
                    }
                    else if (this.val == 17) {
                        NEEDBITS(this.bits + 3);
                        DROPBITS(this.bits);
                        len = 0;
                        copy = 3 + BITS(3);
                        DROPBITS(3);
                    }
                    else {
                        NEEDBITS(this.bits + 7);
                        DROPBITS(this.bits);
                        len = 0;
                        copy = 11 + BITS(7);
                        DROPBITS(7);
                    }
                    if (state->have + copy > state->nlen + state->ndist) {
                        strm->msg = (char *)"invalid bit length repeat";
                        state->mode = BAD;
                        break;
                    }
                    while (copy--)
                        state->lens[state->have++] = (unsigned short)len;
                }
            }

            /* handle error breaks in while */
            if (state->mode == BAD) break;

            /* build code tables */
            state->next = state->codes;
            state->lencode = (code const *)(state->next);
            state->lenbits = 9;
            ret = zlib_inflate_table(LENS, state->lens, state->nlen, &(state->next),
                                &(state->lenbits), state->work);
            if (ret) {
                strm->msg = (char *)"invalid literal/lengths set";
                state->mode = BAD;
                break;
            }
            state->distcode = (code const *)(state->next);
            state->distbits = 6;
            ret = zlib_inflate_table(DISTS, state->lens + state->nlen, state->ndist,
                            &(state->next), &(state->distbits), state->work);
            if (ret) {
                strm->msg = (char *)"invalid distances set";
                state->mode = BAD;
                break;
            }
            state->mode = LEN;
         __attribute__((fallthrough));
        case LEN:
            if (have >= 6 && left >= 258) {
                RESTORE();
                inflate_fast(strm, out);
                LOAD();
                break;
            }
            for (;;) {
                this = state->lencode[BITS(state->lenbits)];
                if ((unsigned)(this.bits) <= bits) break;
                PULLBYTE();
            }
            if (this.op && (this.op & 0xf0) == 0) {
                last = this;
                for (;;) {
                    this = state->lencode[last.val +
                            (BITS(last.bits + last.op) >> last.bits)];
                    if ((unsigned)(last.bits + this.bits) <= bits) break;
                    PULLBYTE();
                }
                DROPBITS(last.bits);
            }
            DROPBITS(this.bits);
            state->length = (unsigned)this.val;
            if ((int)(this.op) == 0) {
                state->mode = LIT;
                break;
            }
            if (this.op & 32) {
                state->mode = TYPE;
                break;
            }
            if (this.op & 64) {
                strm->msg = (char *)"invalid literal/length code";
                state->mode = BAD;
                break;
            }
            state->extra = (unsigned)(this.op) & 15;
            state->mode = LENEXT;
         __attribute__((fallthrough));
        case LENEXT:
            if (state->extra) {
                NEEDBITS(state->extra);
                state->length += BITS(state->extra);
                DROPBITS(state->extra);
            }
            state->mode = DIST;
         __attribute__((fallthrough));
        case DIST:
            for (;;) {
                this = state->distcode[BITS(state->distbits)];
                if ((unsigned)(this.bits) <= bits) break;
                PULLBYTE();
            }
            if ((this.op & 0xf0) == 0) {
                last = this;
                for (;;) {
                    this = state->distcode[last.val +
                            (BITS(last.bits + last.op) >> last.bits)];
                    if ((unsigned)(last.bits + this.bits) <= bits) break;
                    PULLBYTE();
                }
                DROPBITS(last.bits);
            }
            DROPBITS(this.bits);
            if (this.op & 64) {
                strm->msg = (char *)"invalid distance code";
                state->mode = BAD;
                break;
            }
            state->offset = (unsigned)this.val;
            state->extra = (unsigned)(this.op) & 15;
            state->mode = DISTEXT;
         __attribute__((fallthrough));
        case DISTEXT:
            if (state->extra) {
                NEEDBITS(state->extra);
                state->offset += BITS(state->extra);
                DROPBITS(state->extra);
            }
#ifdef INFLATE_STRICT
            if (state->offset > state->dmax) {
                strm->msg = (char *)"invalid distance too far back";
                state->mode = BAD;
                break;
            }
#endif
            if (state->offset > state->whave + out - left) {
                strm->msg = (char *)"invalid distance too far back";
                state->mode = BAD;
                break;
            }
            state->mode = MATCH;
         __attribute__((fallthrough));
        case MATCH:
            if (left == 0) goto inf_leave;
            copy = out - left;
            if (state->offset > copy) {         /* copy from window */
                copy = state->offset - copy;
                if (copy > state->write) {
                    copy -= state->write;
                    from = state->window + (state->wsize - copy);
                }
                else
                    from = state->window + (state->write - copy);
                if (copy > state->length) copy = state->length;
            }
            else {                              /* copy from output */
                from = put - state->offset;
                copy = state->length;
            }
            if (copy > left) copy = left;
            left -= copy;
            state->length -= copy;
            do {
                *put++ = *from++;
            } while (--copy);
            if (state->length == 0) state->mode = LEN;
            break;
        case LIT:
            if (left == 0) goto inf_leave;
            *put++ = (unsigned char)(state->length);
            left--;
            state->mode = LEN;
            break;
        case CHECK:
            if (state->wrap) {
                NEEDBITS(32);
                out -= left;
                strm->total_out += out;
                state->total += out;
                if (INFLATE_NEED_CHECKSUM(strm) && out)
                    strm->adler = state->check =
                        UPDATE(state->check, put - out, out);
                out = left;
                if ((
                     REVERSE(hold)) != state->check) {
                    strm->msg = (char *)"incorrect data check";
                    state->mode = BAD;
                    break;
                }
                INITBITS();
            }
            state->mode = DONE;
         __attribute__((fallthrough));
        case DONE:
            ret = Z_STREAM_END;
            goto inf_leave;
        case BAD:
            ret = Z_DATA_ERROR;
            goto inf_leave;
        case MEM:
            return Z_MEM_ERROR;
        case SYNC:
        default:
            return Z_STREAM_ERROR;
        }

    /*
       Return from inflate(), updating the total counts and the check value.
       If there was no progress during the inflate() call, return a buffer
       error.  Call zlib_updatewindow() to create and/or update the window state.
     */
  inf_leave:
    RESTORE();
    if (INFLATE_NEED_UPDATEWINDOW(strm) &&
            (state->wsize || (state->mode < CHECK && out != strm->avail_out)))
        zlib_updatewindow(strm, out);

    in -= strm->avail_in;
    out -= strm->avail_out;
    strm->total_in += in;
    strm->total_out += out;
    state->total += out;
    if (INFLATE_NEED_CHECKSUM(strm) && state->wrap && out)
        strm->adler = state->check =
            UPDATE(state->check, strm->next_out - out, out);

    strm->data_type = state->bits + (state->last ? 64 : 0) +
                      (state->mode == TYPE ? 128 : 0);

    if (flush == Z_PACKET_FLUSH && ret == Z_OK &&
            strm->avail_out != 0 && strm->avail_in == 0)
        return zlib_inflateSyncPacket(strm);

    if (((in == 0 && out == 0) || flush == Z_FINISH) && ret == Z_OK)
        ret = Z_BUF_ERROR;

    return ret;
}

int zlib_inflateEnd(z_streamp strm)
{
    if (strm == NULL || strm->state == NULL)
        return Z_STREAM_ERROR;
    return Z_OK;
}

/*
 * This subroutine adds the data at next_in/avail_in to the output history
 * without performing any output.  The output buffer must be "caught up";
 * i.e. no pending output but this should always be the case. The state must
 * be waiting on the start of a block (i.e. mode == TYPE or HEAD).  On exit,
 * the output will also be caught up, and the checksum will have been updated
 * if need be.
 */
int zlib_inflateIncomp(z_stream *z)
{
    struct inflate_state *state = (struct inflate_state *)z->state;
    unsigned char *saved_no = z->next_out;
    unsigned int saved_ao = z->avail_out;

    if (state->mode != TYPE && state->mode != HEAD)
    return Z_DATA_ERROR;

    /* Setup some variables to allow misuse of updateWindow */
    z->avail_out = 0;
    z->next_out = (unsigned char*)z->next_in + z->avail_in;

    zlib_updatewindow(z, z->avail_in);

    /* Restore saved variables */
    z->avail_out = saved_ao;
    z->next_out = saved_no;

    z->adler = state->check =
        UPDATE(state->check, z->next_in, z->avail_in);

    z->total_out += z->avail_in;
    z->total_in += z->avail_in;
    z->next_in += z->avail_in;
    state->total += z->avail_in;
    z->avail_in = 0;

    return Z_OK;
}


#define GZIP_IOBUF_SIZE (16*1024)

static long nofill_zlib(void *buffer, unsigned long len)
{
    return -1;
}

/* Included from initramfs et al code */
int __gunzip(unsigned char *buf, long len,
               long (*fill)(void*, unsigned long),
               long (*flush)(void*, unsigned long),
               unsigned char *out_buf, long out_len,
               long *pos,
               void(*error)(char *x)) {
    u8 *zbuf;
    struct z_stream_s *strm;
    int rc;

    rc = -1;
    if (flush) {
        out_len = 0x8000; /* 32 K */
        out_buf = malloc(out_len);
    } else {
        if (!out_len)
            out_len = ((size_t)~0) - (size_t)out_buf; /* no limit */
    }
    if (!out_buf) {
        error("Out of memory while allocating output buffer");
        goto gunzip_nomem1;
    }

    if (buf)
        zbuf = buf;
    else {
        zbuf = malloc(GZIP_IOBUF_SIZE);
        len = 0;
    }
    if (!zbuf) {
        error("Out of memory while allocating input buffer");
        goto gunzip_nomem2;
    }

    strm = malloc(sizeof(*strm));
    if (strm == NULL) {
        error("Out of memory while allocating z_stream");
        goto gunzip_nomem3;
    }

    strm->workspace = malloc(flush ? zlib_inflate_workspacesize() :
#ifdef CONFIG_ZLIB_DFLTCC
    /* Always allocate the full workspace for DFLTCC */
                 zlib_inflate_workspacesize());
#else
                 sizeof(struct inflate_state));
#endif
    if (strm->workspace == NULL) {
        error("Out of memory while allocating workspace");
        goto gunzip_nomem4;
    }

    if (!fill)
        fill = nofill_zlib;

    if (len == 0)
        len = fill(zbuf, GZIP_IOBUF_SIZE);

    /* verify the gzip header */
    if (len < 10 ||
       zbuf[0] != 0x1f || zbuf[1] != 0x8b || zbuf[2] != 0x08) {
        if (pos)
            *pos = 0;
        error("Not a gzip file");
        goto gunzip_5;
    }

    /* skip over gzip header (1f,8b,08... 10 bytes total +
     * possible asciz filename)
     */
    strm->next_in = zbuf + 10;
    strm->avail_in = len - 10;
    /* skip over asciz filename */
    if (zbuf[3] & 0x8) {
        do {
            /*
             * If the filename doesn't fit into the buffer,
             * the file is very probably corrupt. Don't try
             * to read more data.
             */
            if (strm->avail_in == 0) {
                error("header error");
                goto gunzip_5;
            }
            --strm->avail_in;
        } while (*strm->next_in++);
    }

    strm->next_out = out_buf;
    strm->avail_out = out_len;

    rc = zlib_inflateInit2(strm, -MAX_WBITS);

#ifdef CONFIG_ZLIB_DFLTCC
    /* Always keep the window for DFLTCC */
#else
    if (!flush) {
        WS(strm)->inflate_state.wsize = 0;
        WS(strm)->inflate_state.window = NULL;
    }
#endif

    while (rc == Z_OK) {
        if (strm->avail_in == 0) {
            /* TODO: handle case where both pos and fill are set */
            len = fill(zbuf, GZIP_IOBUF_SIZE);
            if (len < 0) {
                rc = -1;
                error("read error");
                break;
            }
            strm->next_in = zbuf;
            strm->avail_in = len;
        }
        rc = zlib_inflate(strm, 0);

        /* Write any data generated */
        if (flush && strm->next_out > out_buf) {
            long l = strm->next_out - out_buf;
            if (l != flush(out_buf, l)) {
                rc = -1;
                error("write error");
                break;
            }
            strm->next_out = out_buf;
            strm->avail_out = out_len;
        }

        /* after Z_FINISH, only Z_STREAM_END is "we unpacked it all" */
        if (rc == Z_STREAM_END) {
            rc = 0;
            break;
        } else if (rc != Z_OK) {
            error("uncompression error");
            rc = -1;
        }
    }

    zlib_inflateEnd(strm);
    if (pos)
        /* add + 8 to skip over trailer */
        *pos = strm->next_in - zbuf+8;

gunzip_5:
    free(strm->workspace);
gunzip_nomem4:
    free(strm);
gunzip_nomem3:
    if (!buf)
        free(zbuf);
gunzip_nomem2:
    if (flush)
        free(out_buf);
gunzip_nomem1:
    return rc; /* returns Z_OK (0) if successful */
}

int gunzip(unsigned char *buf, long len,
               long (*fill)(void*, unsigned long),
               long (*flush)(void*, unsigned long),
               unsigned char *out_buf,
               long *pos,
               void (*error)(char *x))
{
    return __gunzip(buf, len, fill, flush, out_buf, 0, pos, error);
}

//END GUNZIP
